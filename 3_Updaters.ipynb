{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3 Updaters.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN8UcLkRQnzcnKDjcL5py+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LondonNode/Pearl-tutorials/blob/main/3_Updaters.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Z6RJu7djrlh"
      },
      "outputs": [],
      "source": [
        "!pip install pearll"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a tutorial for the `updaters` module within Pearl. This module is responsible for updating the models approximating the policy or value functions. There are three base classes specifying the three types of updater:\n",
        "- `BaseActorUpdater`\n",
        "- `BaseCriticUpdater`\n",
        "- `BaseEvolutionUpdater`\n",
        "\n",
        "All updaters follow the same design philosophy, with an `__init__` method for initialization and a `__call__` method to run the updater which returns an `UpdaterLog` object."
      ],
      "metadata": {
        "id": "pFfgXj6-mvja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.common.type_aliases import UpdaterLog\n",
        "\n",
        "\n",
        "loss = 0\n",
        "entropy = 0\n",
        "divergence = 0\n",
        "log = UpdaterLog(\n",
        "  loss=loss, # loss function value\n",
        "  divergence=divergence, # divergence or distance measurement (e.g. KL divergence)\n",
        "  entropy=entropy, # entropy of a policy or other distribution parameter\n",
        ")\n",
        "\n",
        "print(log)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "587ZNx7Ap7sW",
        "outputId": "36c5c7c4-3def-42f4-a2f6-d7a07cbacc4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UpdaterLog(loss=0, divergence=0, entropy=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor Updaters\n",
        "\n",
        "The actor updaters are responsible for updating the actor models via backpropagation. The formulae for updating actors are all very unique, so separate implementations are required for each one. \n",
        "\n",
        "Currently implemented updaters:\n",
        "- `PolicyGradient`: REINFORCE algorithm (e.g. A2C)\n",
        "- `ProximalPolicyClip`: PPO policy loss\n",
        "- `DeterministicPolicyGradient`: DDPG policy loss\n",
        "- `SoftPolicyGradient`: SAC policy loss"
      ],
      "metadata": {
        "id": "RaRRoDKOnjOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models import ActorCritic, Actor\n",
        "from pearll.updaters import BaseActorUpdater\n",
        "from pearll.common.type_aliases import UpdaterLog\n",
        "\n",
        "import torch as T\n",
        "from typing import Type, Union\n",
        "\n",
        "\n",
        "class YourActorUpdater(BaseActorUpdater):\n",
        "  def __init__(\n",
        "      self,\n",
        "      optimizer_class: Type[T.optim.Optimizer] = T.optim.Adam, # alter optimizer class\n",
        "      max_grad: float = 0, # clip gradients\n",
        "  ) -> None:\n",
        "    super().__init__(optimizer_class, max_grad)\n",
        "\n",
        "  # __call__ is an abstract method in the base class\n",
        "  def __call__(\n",
        "      self,\n",
        "      model: Union[ActorCritic, Actor],\n",
        "  ) -> UpdaterLog:\n",
        "    # Useful inbuilt method to get the requried parameters to be updated.\n",
        "    actor_parameters = self._get_model_parameters(model=model)\n",
        "    optimizer = self.optimizer_class(actor_parameters)\n",
        "    loss = 0\n",
        "    # Useful inbuilt method to run an optimization step\n",
        "    self.run_optimizer(optimizer=optimizer, loss=loss, actor_parameters=actor_parameters)\n",
        "    return UpdaterLog(loss=loss)"
      ],
      "metadata": {
        "id": "EJiBfOXltI5_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Critic Updaters\n",
        "\n",
        "The critic updaters are responsible for updating the critic models via backpropagation. These implementations are more generalizable across algorithms compared to the actor updaters since the critics are often updated in the same ways either trying to learn a value or Q function. \n",
        "\n",
        "Currently implemented updaters:\n",
        "- `ValueRegression`\n",
        "- `ContinuousQRegression`\n",
        "- `DiscreteQRegression`"
      ],
      "metadata": {
        "id": "uTna9aMVNGQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The BaseCriticUpdater class has the same features as the BaseActorUpdater\n",
        "\n",
        "from pearll.models import ActorCritic, Critic\n",
        "from pearll.updaters import BaseCriticUpdater\n",
        "from pearll.common.type_aliases import UpdaterLog\n",
        "\n",
        "import torch as T\n",
        "from typing import Type, Union\n",
        "\n",
        "\n",
        "class YourCriticUpdater(BaseCriticUpdater):\n",
        "  def __init__(\n",
        "      self,\n",
        "      optimizer_class: Type[T.optim.Optimizer] = T.optim.Adam, # alter optimizer class\n",
        "      max_grad: float = 0, # clip gradients\n",
        "  ) -> None:\n",
        "    super().__init__(optimizer_class, max_grad)\n",
        "\n",
        "  def __call__(\n",
        "      self,\n",
        "      model: Union[ActorCritic, Critic],\n",
        "  ) -> UpdaterLog:\n",
        "    critic_parameters = self._get_model_parameters(model=model)\n",
        "    optimizer = self.optimizer_class(critic_parameters)\n",
        "    loss = 0\n",
        "    self.run_optimizer(optimizer=optimizer, loss=loss, critic_parameters=critic_parameters)\n",
        "    return UpdaterLog(loss=loss)"
      ],
      "metadata": {
        "id": "5JPp5tI4PXq-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The implemented updaters all share the same pattern, so let's review by going\n",
        "# through the ValueRegression updater\n",
        "\n",
        "from pearll.updaters.critics import ValueRegression\n",
        "from pearll.models import Critic\n",
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import ValueHead\n",
        "\n",
        "import torch as T\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "model = Critic(\n",
        "    encoder = IdentityEncoder(),\n",
        "    torso = MLP(layer_sizes=[5, 5]),\n",
        "    head = ValueHead(input_shape=5)\n",
        ")\n",
        "\n",
        "updater = ValueRegression(\n",
        "    loss_class = T.nn.MSELoss(), # alter the regression loss function\n",
        "    optimizer_class = T.optim.Adam,\n",
        "    max_grad=0.5,\n",
        ")\n",
        "\n",
        "updater(\n",
        "    model=model,\n",
        "    observations = np.random.rand(1, 5),\n",
        "    returns = T.rand(size=(1, 1)),\n",
        "    learning_rate = 0.001, # alter learning rate of optimizer\n",
        "    loss_coeff = 1, # weight of the critic loss in relation to the actor loss\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPC5LqKiQLbH",
        "outputId": "1cc79d69-fe10-4195-ac00-f6f04f1c612a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UpdaterLog(loss=0.5480731129646301, divergence=None, entropy=None)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolution Updaters\n",
        "\n",
        "The evolution updaters are responsible for updating either the actor or critic models via random search. These are somewhat generalizable implementations.\n",
        "\n",
        "Currently implemented updaters:\n",
        "- `NoisyGradientAscent`\n",
        "- `GeneticUpdater`"
      ],
      "metadata": {
        "id": "5o_0uMrGo__D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.updaters import BaseEvolutionUpdater\n",
        "from pearll.common.type_aliases import UpdaterLog\n",
        "\n",
        "\n",
        "class YourEvolutionUpdater(BaseEvolutionUpdater):\n",
        "  # The population_type parameter specifies whether to update the actor\n",
        "  # or critic populations in the ActorCritic model.\n",
        "  def __init__(self, model: ActorCritic, population_type: str = \"actor\") -> None:\n",
        "    super.__init__(model, population_type)\n",
        "\n",
        "  # Abstract method needs to be implemented\n",
        "  def __call__(self) -> UpdaterLog:\n",
        "    population = self.model.numpy_actors()\n",
        "    # Useful inbuilt method will update the actor or critic population to the\n",
        "    # state passed as input.\n",
        "    self.update_networks(population=population)"
      ],
      "metadata": {
        "id": "o9l6Vr5GnfY7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.updaters.evolution import NoisyGradientAscent\n",
        "from pearll.models import Dummy, ActorCritic\n",
        "from pearll.settings import PopulationSettings\n",
        "\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "space = Box(-100, 100, shape=(1,))\n",
        "settings = PopulationSettings(actor_population_size=10, actor_distribution=\"normal\")\n",
        "\n",
        "actor = Dummy(space)\n",
        "critic = Dummy(space)\n",
        "model = ActorCritic(actor, critic, settings)\n",
        "\n",
        "updater = NoisyGradientAscent(model=model, population_type=\"actor\")\n",
        "# The NoisyGradientAscent shifts the population in some optimization_direction\n",
        "# where step size is controlled by the learning_rate.\n",
        "log = updater(learning_rate=1, optimization_direction=np.array(1))\n",
        "print(log)\n",
        "print(\"divergence = KL divergence between old and new populations\")\n",
        "print(\"entropy = entropy of new population distribution\")\n",
        "# A useful state of the updater consists of the samples taken from the standard\n",
        "# normal distribution used to generate a new population\n",
        "samples = updater.normal_dist\n",
        "print(f\"\\nMap of samples from standard normal distribution (left) to the resultant model parameters (right): \\n{np.concatenate((samples, model.numpy_actors()), axis=-1)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m8pL0ii5CZn",
        "outputId": "1753a28c-bcfc-4b9e-a5ae-5fd8ea431238"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UpdaterLog(loss=None, divergence=tensor(0.5000), entropy=tensor(1.4189))\n",
            "divergence = KL divergence between old and new populations\n",
            "entropy = entropy of new population distribution\n",
            "\n",
            "Map of samples from standard normal distribution (left) to the resultant model parameters (right): \n",
            "[[ 2.32818566e-01 -2.82520924e+01]\n",
            " [-2.85612791e-01 -2.87705238e+01]\n",
            " [ 9.91035826e-02 -2.83858074e+01]\n",
            " [-8.65469413e-01 -2.93503804e+01]\n",
            " [-1.35573335e-02 -2.84984683e+01]\n",
            " [ 1.71631997e-01 -2.83132790e+01]\n",
            " [-1.12415523e+00 -2.96090662e+01]\n",
            " [-8.46786684e-01 -2.93316976e+01]\n",
            " [ 1.71824444e+00 -2.67666665e+01]\n",
            " [-1.73978490e+00 -3.02246959e+01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.updaters.evolution import GeneticUpdater\n",
        "from pearll.models import Dummy, ActorCritic\n",
        "from pearll.settings import PopulationSettings, Settings, MutationSettings\n",
        "from pearll.signal_processing import selection_operators, crossover_operators, mutation_operators\n",
        "\n",
        "from gym.spaces import Box\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "space = Box(-100, 100, shape=(1,))\n",
        "settings = PopulationSettings(actor_population_size=10, actor_distribution=\"uniform\")\n",
        "rewards = np.random.uniform(size=10)\n",
        "\n",
        "actor = Dummy(space)\n",
        "critic = Dummy(space)\n",
        "model = ActorCritic(actor, critic, settings)\n",
        "\n",
        "updater = GeneticUpdater(model=model, population_type=\"actor\")\n",
        "# The GeneticUpdater uses the genetic algorithm approach to update a population.\n",
        "# Selection operators, crossover operators and mutation operators are\n",
        "# implemented in the signal_processing module.\n",
        "# In this case divergence\n",
        "log = updater(\n",
        "    rewards=rewards, \n",
        "    selection_operator=selection_operators.roulette_selection,\n",
        "    crossover_operator=crossover_operators.one_point_crossover,\n",
        "    mutation_operator=mutation_operators.uniform_mutation\n",
        ")\n",
        "print(log)\n",
        "print(\"divergence = average distance between new and old population\")\n",
        "print(\"entropy = distance between max and min of new population\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0cAzhTd8-78",
        "outputId": "9a13623b-6b84-4b9c-8602-a43e5ff711ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UpdaterLog(loss=None, divergence=61.14169680740664, entropy=181.91257)\n",
            "divergence = average distance between new and old population\n",
            "entropy = distance between max and min of new population\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CFD3FajlKL2k"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}