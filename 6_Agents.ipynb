{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6. Agents.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqx3IeFQJcdZoasTSMWSIw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LondonNode/Pearl-tutorials/blob/main/6_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p59FKmlBcUxI"
      },
      "outputs": [],
      "source": [
        "!pip install pearll"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a tutorial for the `agents` module within Pearl. This is the main interface between the user and the algorithm. Pearl is designed to allow for three types of agent:\n",
        "\n",
        "1. Reinforcement learning (RL)\n",
        "2. Evolutionary Computation (EC)\n",
        "3. Hybrid algorithms (combination of RL and EC)\n",
        "\n",
        "All three types of agent can be derived from the `BaseAgent` object. The goal is to allow the user to focus on writing the code for the training step itself without having to think about any of the other infrastructure around the algorithm (e.g. collecting trajectories). As such, the only abstract method defined is the `_fit()` method where the update algorithms should be implemented.\n",
        "\n",
        "Key features include:\n",
        "\n",
        "| Features                 | Pearl   | \n",
        "|-------------------       |---------|\n",
        "| Modular Components       | ✅      |\n",
        "| Dataclass settings       | ✅      |\n",
        "| Tensorboard integration  | ✅      |\n",
        "| Single or multi-agent    | ✅      |\n",
        "| Callbacks                | ✅      |\n",
        "| Control log frequency    | ✅      |\n",
        "| Control train frequency  | ✅      |\n",
        "| Trajectory log file agent.log  | ✅      |\n",
        "\n",
        "Some agents are already implemented as examples and a [template](https://github.com/LondonNode/Pearl/blob/main/pearll/agents/templates.py) is included for creating your own agents."
      ],
      "metadata": {
        "id": "WXUa-YoQcYtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A demo script is also included to run differnt implemented agents in Pearl\n",
        "# from the command line.\n",
        "\n",
        "!python -m pearll.demo -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrK5HHY2gAhZ",
        "outputId": "53343145-a849-4e8a-b090-c7c3c304c34b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: demo.py [-h] [--agent AGENT]\n",
            "\n",
            "pearll demo with preloaded hyperparameters\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help     show this help message and exit\n",
            "  --agent AGENT  Agent to demo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "Pearl supports single and multi agent RL. Let's use the CartPole gym environment for its simplicity, for which we can implement the DQN off-policy algorithm. An implementation of DQN is done in Pearl already but we'll do a simplified version here to demonstrate the flow."
      ],
      "metadata": {
        "id": "iyxwKzKBdNdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.agents import BaseAgent\n",
        "from pearll.models import ActorCritic\n",
        "from pearll.updaters.critics import BaseCriticUpdater, DiscreteQRegression\n",
        "from pearll.buffers import BaseBuffer, ReplayBuffer\n",
        "from pearll.explorers import BaseExplorer\n",
        "from pearll.callbacks import BaseCallback\n",
        "from pearll.common.type_aliases import Log\n",
        "# The utils file in common contains some useful data manipulation functions.\n",
        "from pearll.common.utils import to_numpy\n",
        "# Pearl includes implementations of common estimators, TD(0), GAE, etc.\n",
        "from pearll.signal_processing import return_estimators\n",
        "from pearll.settings import (\n",
        "    BufferSettings,\n",
        "    ExplorerSettings,\n",
        "    LoggerSettings,\n",
        "    MiscellaneousSettings,\n",
        "    OptimizerSettings,\n",
        "    Settings,\n",
        ")\n",
        "\n",
        "from typing import Type, Optional, List\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch as T\n",
        "\n",
        "\n",
        "\n",
        "class DQN(BaseAgent):\n",
        "  # Note that many of the agent parameters are grouped into settings objects for\n",
        "  # a cleaner interface. The __init__() method's goal is to simply intialize all\n",
        "  # the other modules inputted to the class, should generally be quite simple to\n",
        "  # implement.\n",
        "  def __init__(\n",
        "    self,\n",
        "    env: gym.Env,\n",
        "    model: ActorCritic,\n",
        "    trajectory_discount: float = 0.99, # can add extra parameters not defined in the BaseAgent\n",
        "    updater_class: Type[BaseCriticUpdater] = DiscreteQRegression, # easy to swap for another critic updater\n",
        "    optimizer_settings: OptimizerSettings = OptimizerSettings(),\n",
        "    buffer_class: Type[BaseBuffer] = ReplayBuffer, # easy to swap for another buffer\n",
        "    buffer_settings: BufferSettings = BufferSettings(),\n",
        "    action_explorer_class: Type[BaseExplorer] = BaseExplorer, # easy to swap for another explorer\n",
        "    explorer_settings: ExplorerSettings = ExplorerSettings(start_steps=1000),\n",
        "    callbacks: Optional[List[Type[BaseCallback]]] = None, # easy to swap for another callback (in the next tutorial...)\n",
        "    callback_settings: Optional[List[Settings]] = None,\n",
        "    logger_settings: LoggerSettings = LoggerSettings(),\n",
        "    misc_settings: MiscellaneousSettings = MiscellaneousSettings(), # note seed is stored here!\n",
        "  ) -> None:\n",
        "    # The BaseAgent handles intialization of many of the modules, this can be\n",
        "    # done since the submodules within share the same initialization interface.\n",
        "    super().__init__(\n",
        "      env,\n",
        "      model,\n",
        "      action_explorer_class=action_explorer_class,\n",
        "      explorer_settings=explorer_settings,\n",
        "      buffer_class=buffer_class,\n",
        "      buffer_settings=buffer_settings,\n",
        "      logger_settings=logger_settings,\n",
        "      callbacks=callbacks,\n",
        "      callback_settings=callback_settings,\n",
        "      misc_settings=misc_settings,\n",
        "    )\n",
        "\n",
        "    self.q_regression = updater_class(\n",
        "        loss_class = optimizer_settings.loss_class,\n",
        "        optimizer_class = optimizer_settings.optimizer_class,\n",
        "        max_grad = optimizer_settings.max_grad\n",
        "    )\n",
        "\n",
        "    self.learning_rate = optimizer_settings.learning_rate\n",
        "    self.trajectory_discount = trajectory_discount\n",
        "\n",
        "  # Abstract method needs to be implemented. The _fit() method defines the\n",
        "  # actual training update step. This can be quite simple as well though\n",
        "  # thanks to the pre-implemented flexible components that handle much of the \n",
        "  # deep logic.\n",
        "  def _fit(\n",
        "        self, batch_size: int, actor_epochs: int = 1, critic_epochs: int = 1\n",
        "  ) -> Log:\n",
        "    critic_losses = np.zeros(shape=(critic_epochs))\n",
        "    for i in range(critic_epochs):\n",
        "      # Sample trajectories\n",
        "      trajectories = self.buffer.sample(batch_size=batch_size, flatten_env=False)\n",
        "\n",
        "      # Get target Q values for regression loss\n",
        "      with T.no_grad():\n",
        "        next_q_values = self.model.forward_target_critics(\n",
        "          trajectories.next_observations\n",
        "        )\n",
        "        next_q_values = to_numpy(next_q_values.max(dim=-1)[0])\n",
        "        next_q_values = next_q_values[..., np.newaxis]\n",
        "        target_q_values = return_estimators.TD_zero( # TD(0) is already implemented!\n",
        "          trajectories.rewards,\n",
        "          next_q_values,\n",
        "          trajectories.dones,\n",
        "          self.trajectory_discount,\n",
        "        )\n",
        "\n",
        "      # Run Q regression\n",
        "      updater_log = self.q_regression(\n",
        "        self.model,\n",
        "        trajectories.observations,\n",
        "        target_q_values,\n",
        "        trajectories.actions,\n",
        "        learning_rate=self.learning_rate,\n",
        "      )\n",
        "      critic_losses[i] = updater_log.loss\n",
        "\n",
        "    # Update target networks\n",
        "    self.model.assign_targets()\n",
        "\n",
        "    # Returns a Log object which contains useful training statistics to be\n",
        "    # logged in Tensorboad.\n",
        "    return Log(critic_loss=np.mean(critic_losses))"
      ],
      "metadata": {
        "id": "JfimHyC7cYLZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single agent training\n",
        "\n",
        "from pearll.models import Critic, EpsilonGreedyActor, ActorCritic\n",
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import DiscreteQHead\n",
        "\n",
        "\n",
        "encoder = IdentityEncoder()\n",
        "torso = MLP(layer_sizes=[4, 64, 32], activation_fn=T.nn.ReLU)\n",
        "head = DiscreteQHead(input_shape=32, output_shape=2)\n",
        "\n",
        "# Epsilon greedy policy included!\n",
        "actor = EpsilonGreedyActor(\n",
        "    critic_encoder=encoder, critic_torso=torso, critic_head=head\n",
        ")\n",
        "critic = Critic(encoder=encoder, torso=torso, head=head, create_target=True)\n",
        "\n",
        "agent = DQN(\n",
        "  env=gym.make(\"CartPole-v0\"),\n",
        "  model = ActorCritic(actor, critic),\n",
        "  logger_settings = LoggerSettings(log_frequency=(\"episode\", 1), verbose=True),\n",
        "  explorer_settings=ExplorerSettings(start_steps=1000),\n",
        ")\n",
        "\n",
        "# max episode reward = 200\n",
        "# Note that an agent.log file has also been saved which stores all the trajectories run.\n",
        "agent.fit(num_steps=20000, batch_size=32, critic_epochs=16, train_frequency=(\"episode\", 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_hCStaWd6Fm",
        "outputId": "1fa72828-5c4f-48ff-cc16-797a22f98902"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device cpu\n",
            "44: Log(reward=45.0, actor_loss=None, critic_loss=1.2274129167199135, divergence=None, entropy=None)\n",
            "69: Log(reward=25.0, actor_loss=None, critic_loss=1.0130657367408276, divergence=None, entropy=None)\n",
            "80: Log(reward=11.0, actor_loss=None, critic_loss=0.8367413617670536, divergence=None, entropy=None)\n",
            "105: Log(reward=25.0, actor_loss=None, critic_loss=0.6842276882380247, divergence=None, entropy=None)\n",
            "125: Log(reward=20.0, actor_loss=None, critic_loss=0.6481776218861341, divergence=None, entropy=None)\n",
            "141: Log(reward=16.0, actor_loss=None, critic_loss=0.6356014953926206, divergence=None, entropy=None)\n",
            "159: Log(reward=18.0, actor_loss=None, critic_loss=0.8716539908200502, divergence=None, entropy=None)\n",
            "173: Log(reward=14.0, actor_loss=None, critic_loss=1.1501947110518813, divergence=None, entropy=None)\n",
            "211: Log(reward=38.0, actor_loss=None, critic_loss=1.4279367625713348, divergence=None, entropy=None)\n",
            "252: Log(reward=41.0, actor_loss=None, critic_loss=1.6205486487597227, divergence=None, entropy=None)\n",
            "261: Log(reward=9.0, actor_loss=None, critic_loss=1.8463203981518745, divergence=None, entropy=None)\n",
            "280: Log(reward=19.0, actor_loss=None, critic_loss=3.1611114144325256, divergence=None, entropy=None)\n",
            "296: Log(reward=16.0, actor_loss=None, critic_loss=3.7674178555607796, divergence=None, entropy=None)\n",
            "314: Log(reward=18.0, actor_loss=None, critic_loss=3.7210289649665356, divergence=None, entropy=None)\n",
            "335: Log(reward=21.0, actor_loss=None, critic_loss=4.738821821287274, divergence=None, entropy=None)\n",
            "357: Log(reward=22.0, actor_loss=None, critic_loss=4.915557207539678, divergence=None, entropy=None)\n",
            "373: Log(reward=16.0, actor_loss=None, critic_loss=6.126064894720912, divergence=None, entropy=None)\n",
            "393: Log(reward=20.0, actor_loss=None, critic_loss=8.945874154567719, divergence=None, entropy=None)\n",
            "416: Log(reward=23.0, actor_loss=None, critic_loss=9.084479235112667, divergence=None, entropy=None)\n",
            "432: Log(reward=16.0, actor_loss=None, critic_loss=13.316812627017498, divergence=None, entropy=None)\n",
            "449: Log(reward=17.0, actor_loss=None, critic_loss=11.463591404259205, divergence=None, entropy=None)\n",
            "463: Log(reward=14.0, actor_loss=None, critic_loss=15.942279875278473, divergence=None, entropy=None)\n",
            "481: Log(reward=18.0, actor_loss=None, critic_loss=15.365437477827072, divergence=None, entropy=None)\n",
            "504: Log(reward=23.0, actor_loss=None, critic_loss=16.729868784546852, divergence=None, entropy=None)\n",
            "534: Log(reward=30.0, actor_loss=None, critic_loss=20.387827783823013, divergence=None, entropy=None)\n",
            "549: Log(reward=15.0, actor_loss=None, critic_loss=25.20965939760208, divergence=None, entropy=None)\n",
            "571: Log(reward=22.0, actor_loss=None, critic_loss=30.906023174524307, divergence=None, entropy=None)\n",
            "582: Log(reward=11.0, actor_loss=None, critic_loss=29.310860872268677, divergence=None, entropy=None)\n",
            "596: Log(reward=14.0, actor_loss=None, critic_loss=36.261956349015236, divergence=None, entropy=None)\n",
            "621: Log(reward=25.0, actor_loss=None, critic_loss=41.77965131402016, divergence=None, entropy=None)\n",
            "648: Log(reward=27.0, actor_loss=None, critic_loss=47.67110487818718, divergence=None, entropy=None)\n",
            "676: Log(reward=28.0, actor_loss=None, critic_loss=33.63309986889362, divergence=None, entropy=None)\n",
            "718: Log(reward=42.0, actor_loss=None, critic_loss=35.29112686216831, divergence=None, entropy=None)\n",
            "748: Log(reward=30.0, actor_loss=None, critic_loss=52.88035613298416, divergence=None, entropy=None)\n",
            "761: Log(reward=13.0, actor_loss=None, critic_loss=76.29339925944805, divergence=None, entropy=None)\n",
            "774: Log(reward=13.0, actor_loss=None, critic_loss=57.510393261909485, divergence=None, entropy=None)\n",
            "798: Log(reward=24.0, actor_loss=None, critic_loss=70.2003378868103, divergence=None, entropy=None)\n",
            "813: Log(reward=15.0, actor_loss=None, critic_loss=68.79212445020676, divergence=None, entropy=None)\n",
            "832: Log(reward=19.0, actor_loss=None, critic_loss=120.07074707746506, divergence=None, entropy=None)\n",
            "856: Log(reward=24.0, actor_loss=None, critic_loss=87.22396624088287, divergence=None, entropy=None)\n",
            "879: Log(reward=23.0, actor_loss=None, critic_loss=106.59405916929245, divergence=None, entropy=None)\n",
            "900: Log(reward=21.0, actor_loss=None, critic_loss=78.11992761492729, divergence=None, entropy=None)\n",
            "912: Log(reward=12.0, actor_loss=None, critic_loss=95.9546685218811, divergence=None, entropy=None)\n",
            "945: Log(reward=33.0, actor_loss=None, critic_loss=77.72411662340164, divergence=None, entropy=None)\n",
            "961: Log(reward=16.0, actor_loss=None, critic_loss=76.2769857943058, divergence=None, entropy=None)\n",
            "990: Log(reward=29.0, actor_loss=None, critic_loss=119.71723318099976, divergence=None, entropy=None)\n",
            "999: Log(reward=9.0, actor_loss=None, critic_loss=186.87689459323883, divergence=None, entropy=None)\n",
            "1017: Log(reward=18.0, actor_loss=None, critic_loss=155.5352439880371, divergence=None, entropy=None)\n",
            "1030: Log(reward=13.0, actor_loss=None, critic_loss=96.45241105556488, divergence=None, entropy=None)\n",
            "1053: Log(reward=23.0, actor_loss=None, critic_loss=169.2865464091301, divergence=None, entropy=None)\n",
            "1069: Log(reward=16.0, actor_loss=None, critic_loss=233.3739778995514, divergence=None, entropy=None)\n",
            "1087: Log(reward=18.0, actor_loss=None, critic_loss=293.72185802459717, divergence=None, entropy=None)\n",
            "1097: Log(reward=10.0, actor_loss=None, critic_loss=342.6595503091812, divergence=None, entropy=None)\n",
            "1111: Log(reward=14.0, actor_loss=None, critic_loss=267.070872426033, divergence=None, entropy=None)\n",
            "1126: Log(reward=15.0, actor_loss=None, critic_loss=291.8335700035095, divergence=None, entropy=None)\n",
            "1138: Log(reward=12.0, actor_loss=None, critic_loss=219.66482937335968, divergence=None, entropy=None)\n",
            "1153: Log(reward=15.0, actor_loss=None, critic_loss=425.60749673843384, divergence=None, entropy=None)\n",
            "1170: Log(reward=17.0, actor_loss=None, critic_loss=630.3919587135315, divergence=None, entropy=None)\n",
            "1188: Log(reward=18.0, actor_loss=None, critic_loss=673.9950144290924, divergence=None, entropy=None)\n",
            "1248: Log(reward=60.0, actor_loss=None, critic_loss=598.6003551483154, divergence=None, entropy=None)\n",
            "1278: Log(reward=30.0, actor_loss=None, critic_loss=609.5545744895935, divergence=None, entropy=None)\n",
            "1315: Log(reward=37.0, actor_loss=None, critic_loss=743.163754940033, divergence=None, entropy=None)\n",
            "1342: Log(reward=27.0, actor_loss=None, critic_loss=690.5680589675903, divergence=None, entropy=None)\n",
            "1364: Log(reward=22.0, actor_loss=None, critic_loss=844.4919693470001, divergence=None, entropy=None)\n",
            "1373: Log(reward=9.0, actor_loss=None, critic_loss=703.0316045284271, divergence=None, entropy=None)\n",
            "1388: Log(reward=15.0, actor_loss=None, critic_loss=726.3989758491516, divergence=None, entropy=None)\n",
            "1416: Log(reward=28.0, actor_loss=None, critic_loss=1499.781611442566, divergence=None, entropy=None)\n",
            "1429: Log(reward=13.0, actor_loss=None, critic_loss=808.1858263015747, divergence=None, entropy=None)\n",
            "1442: Log(reward=13.0, actor_loss=None, critic_loss=768.1167771816254, divergence=None, entropy=None)\n",
            "1465: Log(reward=23.0, actor_loss=None, critic_loss=1713.1087017059326, divergence=None, entropy=None)\n",
            "1480: Log(reward=15.0, actor_loss=None, critic_loss=1569.3234300613403, divergence=None, entropy=None)\n",
            "1499: Log(reward=19.0, actor_loss=None, critic_loss=2054.3403272628784, divergence=None, entropy=None)\n",
            "1510: Log(reward=11.0, actor_loss=None, critic_loss=1515.724135875702, divergence=None, entropy=None)\n",
            "1520: Log(reward=10.0, actor_loss=None, critic_loss=1881.7900042533875, divergence=None, entropy=None)\n",
            "1538: Log(reward=18.0, actor_loss=None, critic_loss=2085.113718032837, divergence=None, entropy=None)\n",
            "1552: Log(reward=14.0, actor_loss=None, critic_loss=1575.6278080940247, divergence=None, entropy=None)\n",
            "1569: Log(reward=17.0, actor_loss=None, critic_loss=2283.1190605163574, divergence=None, entropy=None)\n",
            "1580: Log(reward=11.0, actor_loss=None, critic_loss=3740.6694049835205, divergence=None, entropy=None)\n",
            "1590: Log(reward=10.0, actor_loss=None, critic_loss=2461.102289199829, divergence=None, entropy=None)\n",
            "1606: Log(reward=16.0, actor_loss=None, critic_loss=3238.8939895629883, divergence=None, entropy=None)\n",
            "1621: Log(reward=15.0, actor_loss=None, critic_loss=4135.012512207031, divergence=None, entropy=None)\n",
            "1631: Log(reward=10.0, actor_loss=None, critic_loss=4590.2260456085205, divergence=None, entropy=None)\n",
            "1639: Log(reward=8.0, actor_loss=None, critic_loss=3080.31885433197, divergence=None, entropy=None)\n",
            "1679: Log(reward=40.0, actor_loss=None, critic_loss=2582.202812194824, divergence=None, entropy=None)\n",
            "1689: Log(reward=10.0, actor_loss=None, critic_loss=4149.498796463013, divergence=None, entropy=None)\n",
            "1703: Log(reward=14.0, actor_loss=None, critic_loss=3831.122486114502, divergence=None, entropy=None)\n",
            "1714: Log(reward=11.0, actor_loss=None, critic_loss=2869.0391731262207, divergence=None, entropy=None)\n",
            "1724: Log(reward=10.0, actor_loss=None, critic_loss=4115.950347900391, divergence=None, entropy=None)\n",
            "1736: Log(reward=12.0, actor_loss=None, critic_loss=2927.7344284057617, divergence=None, entropy=None)\n",
            "1747: Log(reward=11.0, actor_loss=None, critic_loss=3945.148946762085, divergence=None, entropy=None)\n",
            "1755: Log(reward=8.0, actor_loss=None, critic_loss=2593.381212234497, divergence=None, entropy=None)\n",
            "1767: Log(reward=12.0, actor_loss=None, critic_loss=3961.1826734542847, divergence=None, entropy=None)\n",
            "1789: Log(reward=22.0, actor_loss=None, critic_loss=3441.370506286621, divergence=None, entropy=None)\n",
            "1799: Log(reward=10.0, actor_loss=None, critic_loss=2932.0810346603394, divergence=None, entropy=None)\n",
            "1808: Log(reward=9.0, actor_loss=None, critic_loss=3170.1729135513306, divergence=None, entropy=None)\n",
            "1817: Log(reward=9.0, actor_loss=None, critic_loss=2463.3062052726746, divergence=None, entropy=None)\n",
            "1826: Log(reward=9.0, actor_loss=None, critic_loss=2804.6630353927612, divergence=None, entropy=None)\n",
            "1837: Log(reward=11.0, actor_loss=None, critic_loss=2984.4772906303406, divergence=None, entropy=None)\n",
            "1846: Log(reward=9.0, actor_loss=None, critic_loss=2182.5195066928864, divergence=None, entropy=None)\n",
            "1864: Log(reward=18.0, actor_loss=None, critic_loss=3065.0872254371643, divergence=None, entropy=None)\n",
            "1877: Log(reward=13.0, actor_loss=None, critic_loss=2642.5977845191956, divergence=None, entropy=None)\n",
            "1886: Log(reward=9.0, actor_loss=None, critic_loss=2938.1901664733887, divergence=None, entropy=None)\n",
            "1900: Log(reward=14.0, actor_loss=None, critic_loss=1916.553081035614, divergence=None, entropy=None)\n",
            "1911: Log(reward=11.0, actor_loss=None, critic_loss=2326.5883684158325, divergence=None, entropy=None)\n",
            "1921: Log(reward=10.0, actor_loss=None, critic_loss=1874.7055740356445, divergence=None, entropy=None)\n",
            "1932: Log(reward=11.0, actor_loss=None, critic_loss=2399.559380531311, divergence=None, entropy=None)\n",
            "1946: Log(reward=14.0, actor_loss=None, critic_loss=1608.0498470067978, divergence=None, entropy=None)\n",
            "1969: Log(reward=23.0, actor_loss=None, critic_loss=1762.9356379508972, divergence=None, entropy=None)\n",
            "1987: Log(reward=18.0, actor_loss=None, critic_loss=1546.4027411937714, divergence=None, entropy=None)\n",
            "2001: Log(reward=14.0, actor_loss=None, critic_loss=1498.1907311081886, divergence=None, entropy=None)\n",
            "2010: Log(reward=9.0, actor_loss=None, critic_loss=1966.5978927612305, divergence=None, entropy=None)\n",
            "2020: Log(reward=10.0, actor_loss=None, critic_loss=1270.4551267623901, divergence=None, entropy=None)\n",
            "2032: Log(reward=12.0, actor_loss=None, critic_loss=1814.8485250473022, divergence=None, entropy=None)\n",
            "2043: Log(reward=11.0, actor_loss=None, critic_loss=1084.5770629644394, divergence=None, entropy=None)\n",
            "2052: Log(reward=9.0, actor_loss=None, critic_loss=727.4039269983768, divergence=None, entropy=None)\n",
            "2062: Log(reward=10.0, actor_loss=None, critic_loss=1134.4563921689987, divergence=None, entropy=None)\n",
            "2075: Log(reward=13.0, actor_loss=None, critic_loss=1259.3726716041565, divergence=None, entropy=None)\n",
            "2083: Log(reward=8.0, actor_loss=None, critic_loss=835.4732515215874, divergence=None, entropy=None)\n",
            "2098: Log(reward=15.0, actor_loss=None, critic_loss=1041.7663761377335, divergence=None, entropy=None)\n",
            "2109: Log(reward=11.0, actor_loss=None, critic_loss=807.2247260212898, divergence=None, entropy=None)\n",
            "2117: Log(reward=8.0, actor_loss=None, critic_loss=1055.3183901309967, divergence=None, entropy=None)\n",
            "2128: Log(reward=11.0, actor_loss=None, critic_loss=958.316937327385, divergence=None, entropy=None)\n",
            "2149: Log(reward=21.0, actor_loss=None, critic_loss=1167.6364719867706, divergence=None, entropy=None)\n",
            "2158: Log(reward=9.0, actor_loss=None, critic_loss=789.6903369426727, divergence=None, entropy=None)\n",
            "2169: Log(reward=11.0, actor_loss=None, critic_loss=1354.423864364624, divergence=None, entropy=None)\n",
            "2179: Log(reward=10.0, actor_loss=None, critic_loss=1237.2130942344666, divergence=None, entropy=None)\n",
            "2190: Log(reward=11.0, actor_loss=None, critic_loss=1218.3296024799347, divergence=None, entropy=None)\n",
            "2200: Log(reward=10.0, actor_loss=None, critic_loss=938.4104388952255, divergence=None, entropy=None)\n",
            "2212: Log(reward=12.0, actor_loss=None, critic_loss=834.4993977546692, divergence=None, entropy=None)\n",
            "2222: Log(reward=10.0, actor_loss=None, critic_loss=735.2824716567993, divergence=None, entropy=None)\n",
            "2231: Log(reward=9.0, actor_loss=None, critic_loss=1050.5230135917664, divergence=None, entropy=None)\n",
            "2240: Log(reward=9.0, actor_loss=None, critic_loss=1536.0494737625122, divergence=None, entropy=None)\n",
            "2252: Log(reward=12.0, actor_loss=None, critic_loss=1469.4574844837189, divergence=None, entropy=None)\n",
            "2263: Log(reward=11.0, actor_loss=None, critic_loss=1395.8195533752441, divergence=None, entropy=None)\n",
            "2272: Log(reward=9.0, actor_loss=None, critic_loss=1562.1871447563171, divergence=None, entropy=None)\n",
            "2282: Log(reward=10.0, actor_loss=None, critic_loss=1704.3830652236938, divergence=None, entropy=None)\n",
            "2292: Log(reward=10.0, actor_loss=None, critic_loss=1857.9587841033936, divergence=None, entropy=None)\n",
            "2302: Log(reward=10.0, actor_loss=None, critic_loss=1881.1473665237427, divergence=None, entropy=None)\n",
            "2317: Log(reward=15.0, actor_loss=None, critic_loss=1934.0278491973877, divergence=None, entropy=None)\n",
            "2334: Log(reward=17.0, actor_loss=None, critic_loss=1726.447660446167, divergence=None, entropy=None)\n",
            "2346: Log(reward=12.0, actor_loss=None, critic_loss=2669.2145624160767, divergence=None, entropy=None)\n",
            "2355: Log(reward=9.0, actor_loss=None, critic_loss=3603.127202987671, divergence=None, entropy=None)\n",
            "2368: Log(reward=13.0, actor_loss=None, critic_loss=2585.0785608291626, divergence=None, entropy=None)\n",
            "2376: Log(reward=8.0, actor_loss=None, critic_loss=2975.7739906311035, divergence=None, entropy=None)\n",
            "2388: Log(reward=12.0, actor_loss=None, critic_loss=2089.733582496643, divergence=None, entropy=None)\n",
            "2398: Log(reward=10.0, actor_loss=None, critic_loss=3565.5844011306763, divergence=None, entropy=None)\n",
            "2408: Log(reward=10.0, actor_loss=None, critic_loss=2819.761202812195, divergence=None, entropy=None)\n",
            "2417: Log(reward=9.0, actor_loss=None, critic_loss=2035.8645248413086, divergence=None, entropy=None)\n",
            "2432: Log(reward=15.0, actor_loss=None, critic_loss=2957.0941004753113, divergence=None, entropy=None)\n",
            "2442: Log(reward=10.0, actor_loss=None, critic_loss=1493.5340356826782, divergence=None, entropy=None)\n",
            "2452: Log(reward=10.0, actor_loss=None, critic_loss=2148.9747886657715, divergence=None, entropy=None)\n",
            "2461: Log(reward=9.0, actor_loss=None, critic_loss=1909.2786750793457, divergence=None, entropy=None)\n",
            "2469: Log(reward=8.0, actor_loss=None, critic_loss=1842.4387845993042, divergence=None, entropy=None)\n",
            "2479: Log(reward=10.0, actor_loss=None, critic_loss=2481.113461494446, divergence=None, entropy=None)\n",
            "2488: Log(reward=9.0, actor_loss=None, critic_loss=1983.6568729877472, divergence=None, entropy=None)\n",
            "2502: Log(reward=14.0, actor_loss=None, critic_loss=1897.6465039253235, divergence=None, entropy=None)\n",
            "2511: Log(reward=9.0, actor_loss=None, critic_loss=1867.210177898407, divergence=None, entropy=None)\n",
            "2519: Log(reward=8.0, actor_loss=None, critic_loss=1490.9117567539215, divergence=None, entropy=None)\n",
            "2533: Log(reward=14.0, actor_loss=None, critic_loss=2194.8418045043945, divergence=None, entropy=None)\n",
            "2543: Log(reward=10.0, actor_loss=None, critic_loss=1877.438583612442, divergence=None, entropy=None)\n",
            "2555: Log(reward=12.0, actor_loss=None, critic_loss=1739.0350623130798, divergence=None, entropy=None)\n",
            "2564: Log(reward=9.0, actor_loss=None, critic_loss=2660.3789081573486, divergence=None, entropy=None)\n",
            "2574: Log(reward=10.0, actor_loss=None, critic_loss=1161.0329575538635, divergence=None, entropy=None)\n",
            "2586: Log(reward=12.0, actor_loss=None, critic_loss=1249.2427334785461, divergence=None, entropy=None)\n",
            "2596: Log(reward=10.0, actor_loss=None, critic_loss=1419.1678278446198, divergence=None, entropy=None)\n",
            "2607: Log(reward=11.0, actor_loss=None, critic_loss=2278.837203979492, divergence=None, entropy=None)\n",
            "2616: Log(reward=9.0, actor_loss=None, critic_loss=1566.1269280910492, divergence=None, entropy=None)\n",
            "2627: Log(reward=11.0, actor_loss=None, critic_loss=1562.8665251731873, divergence=None, entropy=None)\n",
            "2638: Log(reward=11.0, actor_loss=None, critic_loss=1820.2742624282837, divergence=None, entropy=None)\n",
            "2648: Log(reward=10.0, actor_loss=None, critic_loss=1647.2741955518723, divergence=None, entropy=None)\n",
            "2657: Log(reward=9.0, actor_loss=None, critic_loss=1711.3148910999298, divergence=None, entropy=None)\n",
            "2667: Log(reward=10.0, actor_loss=None, critic_loss=701.7982466220856, divergence=None, entropy=None)\n",
            "2676: Log(reward=9.0, actor_loss=None, critic_loss=1604.9245071411133, divergence=None, entropy=None)\n",
            "2689: Log(reward=13.0, actor_loss=None, critic_loss=1055.2276072502136, divergence=None, entropy=None)\n",
            "2705: Log(reward=16.0, actor_loss=None, critic_loss=1181.4119943380356, divergence=None, entropy=None)\n",
            "2716: Log(reward=11.0, actor_loss=None, critic_loss=801.5226045846939, divergence=None, entropy=None)\n",
            "2726: Log(reward=10.0, actor_loss=None, critic_loss=986.4486517310143, divergence=None, entropy=None)\n",
            "2736: Log(reward=10.0, actor_loss=None, critic_loss=908.736752808094, divergence=None, entropy=None)\n",
            "2745: Log(reward=9.0, actor_loss=None, critic_loss=858.8629179000854, divergence=None, entropy=None)\n",
            "2754: Log(reward=9.0, actor_loss=None, critic_loss=762.4840517044067, divergence=None, entropy=None)\n",
            "2766: Log(reward=12.0, actor_loss=None, critic_loss=602.8252658843994, divergence=None, entropy=None)\n",
            "2777: Log(reward=11.0, actor_loss=None, critic_loss=630.1043337583542, divergence=None, entropy=None)\n",
            "2788: Log(reward=11.0, actor_loss=None, critic_loss=869.8607611656189, divergence=None, entropy=None)\n",
            "2797: Log(reward=9.0, actor_loss=None, critic_loss=517.0131633281708, divergence=None, entropy=None)\n",
            "2807: Log(reward=10.0, actor_loss=None, critic_loss=619.0319311618805, divergence=None, entropy=None)\n",
            "2816: Log(reward=9.0, actor_loss=None, critic_loss=422.57846236228943, divergence=None, entropy=None)\n",
            "2825: Log(reward=9.0, actor_loss=None, critic_loss=384.7012706398964, divergence=None, entropy=None)\n",
            "2833: Log(reward=8.0, actor_loss=None, critic_loss=433.9492402076721, divergence=None, entropy=None)\n",
            "2842: Log(reward=9.0, actor_loss=None, critic_loss=362.1825923025608, divergence=None, entropy=None)\n",
            "2861: Log(reward=19.0, actor_loss=None, critic_loss=388.30595019459724, divergence=None, entropy=None)\n",
            "2891: Log(reward=30.0, actor_loss=None, critic_loss=385.99021577835083, divergence=None, entropy=None)\n",
            "2934: Log(reward=43.0, actor_loss=None, critic_loss=259.17142301797867, divergence=None, entropy=None)\n",
            "2983: Log(reward=49.0, actor_loss=None, critic_loss=242.19206619262695, divergence=None, entropy=None)\n",
            "3021: Log(reward=38.0, actor_loss=None, critic_loss=238.88043546676636, divergence=None, entropy=None)\n",
            "3045: Log(reward=24.0, actor_loss=None, critic_loss=222.18554949760437, divergence=None, entropy=None)\n",
            "3076: Log(reward=31.0, actor_loss=None, critic_loss=191.73618933558464, divergence=None, entropy=None)\n",
            "3112: Log(reward=36.0, actor_loss=None, critic_loss=162.51129072904587, divergence=None, entropy=None)\n",
            "3128: Log(reward=16.0, actor_loss=None, critic_loss=133.32833221554756, divergence=None, entropy=None)\n",
            "3154: Log(reward=26.0, actor_loss=None, critic_loss=114.55357334017754, divergence=None, entropy=None)\n",
            "3190: Log(reward=36.0, actor_loss=None, critic_loss=99.0513865351677, divergence=None, entropy=None)\n",
            "3212: Log(reward=22.0, actor_loss=None, critic_loss=115.25601130723953, divergence=None, entropy=None)\n",
            "3232: Log(reward=20.0, actor_loss=None, critic_loss=110.00801062583923, divergence=None, entropy=None)\n",
            "3252: Log(reward=20.0, actor_loss=None, critic_loss=103.86264723539352, divergence=None, entropy=None)\n",
            "3280: Log(reward=28.0, actor_loss=None, critic_loss=77.11062335968018, divergence=None, entropy=None)\n",
            "3295: Log(reward=15.0, actor_loss=None, critic_loss=73.02188515663147, divergence=None, entropy=None)\n",
            "3342: Log(reward=47.0, actor_loss=None, critic_loss=63.408373951911926, divergence=None, entropy=None)\n",
            "3365: Log(reward=23.0, actor_loss=None, critic_loss=84.31689137220383, divergence=None, entropy=None)\n",
            "3382: Log(reward=17.0, actor_loss=None, critic_loss=60.345678210258484, divergence=None, entropy=None)\n",
            "3408: Log(reward=26.0, actor_loss=None, critic_loss=57.46585667133331, divergence=None, entropy=None)\n",
            "3425: Log(reward=17.0, actor_loss=None, critic_loss=74.91464102268219, divergence=None, entropy=None)\n",
            "3447: Log(reward=22.0, actor_loss=None, critic_loss=50.5424599647522, divergence=None, entropy=None)\n",
            "3505: Log(reward=58.0, actor_loss=None, critic_loss=70.05292874574661, divergence=None, entropy=None)\n",
            "3527: Log(reward=22.0, actor_loss=None, critic_loss=60.82166349887848, divergence=None, entropy=None)\n",
            "3550: Log(reward=23.0, actor_loss=None, critic_loss=74.45946091413498, divergence=None, entropy=None)\n",
            "3585: Log(reward=35.0, actor_loss=None, critic_loss=80.84158140420914, divergence=None, entropy=None)\n",
            "3607: Log(reward=22.0, actor_loss=None, critic_loss=29.1916680932045, divergence=None, entropy=None)\n",
            "3638: Log(reward=31.0, actor_loss=None, critic_loss=63.37140664458275, divergence=None, entropy=None)\n",
            "3664: Log(reward=26.0, actor_loss=None, critic_loss=58.725852370262146, divergence=None, entropy=None)\n",
            "3688: Log(reward=24.0, actor_loss=None, critic_loss=51.8497474193573, divergence=None, entropy=None)\n",
            "3765: Log(reward=77.0, actor_loss=None, critic_loss=42.315720200538635, divergence=None, entropy=None)\n",
            "3824: Log(reward=59.0, actor_loss=None, critic_loss=48.65341076254845, divergence=None, entropy=None)\n",
            "3937: Log(reward=113.0, actor_loss=None, critic_loss=63.51604640483856, divergence=None, entropy=None)\n",
            "3978: Log(reward=41.0, actor_loss=None, critic_loss=33.61678048968315, divergence=None, entropy=None)\n",
            "4015: Log(reward=37.0, actor_loss=None, critic_loss=49.95469206571579, divergence=None, entropy=None)\n",
            "4128: Log(reward=113.0, actor_loss=None, critic_loss=62.64754697680473, divergence=None, entropy=None)\n",
            "4214: Log(reward=86.0, actor_loss=None, critic_loss=51.53645506501198, divergence=None, entropy=None)\n",
            "4249: Log(reward=35.0, actor_loss=None, critic_loss=51.6060948073864, divergence=None, entropy=None)\n",
            "4298: Log(reward=49.0, actor_loss=None, critic_loss=64.36686739325523, divergence=None, entropy=None)\n",
            "4343: Log(reward=45.0, actor_loss=None, critic_loss=49.795536279678345, divergence=None, entropy=None)\n",
            "4453: Log(reward=110.0, actor_loss=None, critic_loss=41.093787878751755, divergence=None, entropy=None)\n",
            "4506: Log(reward=53.0, actor_loss=None, critic_loss=55.500797003507614, divergence=None, entropy=None)\n",
            "4576: Log(reward=70.0, actor_loss=None, critic_loss=58.4793426990509, divergence=None, entropy=None)\n",
            "4687: Log(reward=111.0, actor_loss=None, critic_loss=45.46916005015373, divergence=None, entropy=None)\n",
            "4768: Log(reward=81.0, actor_loss=None, critic_loss=58.42716437578201, divergence=None, entropy=None)\n",
            "4822: Log(reward=54.0, actor_loss=None, critic_loss=67.6510221362114, divergence=None, entropy=None)\n",
            "4875: Log(reward=53.0, actor_loss=None, critic_loss=60.74691015481949, divergence=None, entropy=None)\n",
            "4966: Log(reward=91.0, actor_loss=None, critic_loss=57.38893721997738, divergence=None, entropy=None)\n",
            "5006: Log(reward=40.0, actor_loss=None, critic_loss=51.119025245308876, divergence=None, entropy=None)\n",
            "5156: Log(reward=150.0, actor_loss=None, critic_loss=60.67330393195152, divergence=None, entropy=None)\n",
            "5203: Log(reward=47.0, actor_loss=None, critic_loss=83.42668521404266, divergence=None, entropy=None)\n",
            "5253: Log(reward=50.0, actor_loss=None, critic_loss=79.49478578567505, divergence=None, entropy=None)\n",
            "5371: Log(reward=118.0, actor_loss=None, critic_loss=68.94386821985245, divergence=None, entropy=None)\n",
            "5440: Log(reward=69.0, actor_loss=None, critic_loss=85.11514765024185, divergence=None, entropy=None)\n",
            "5479: Log(reward=39.0, actor_loss=None, critic_loss=69.04911290854216, divergence=None, entropy=None)\n",
            "5524: Log(reward=45.0, actor_loss=None, critic_loss=61.81944081187248, divergence=None, entropy=None)\n",
            "5607: Log(reward=83.0, actor_loss=None, critic_loss=67.41416148841381, divergence=None, entropy=None)\n",
            "5644: Log(reward=37.0, actor_loss=None, critic_loss=76.40227675437927, divergence=None, entropy=None)\n",
            "5698: Log(reward=54.0, actor_loss=None, critic_loss=34.65648999810219, divergence=None, entropy=None)\n",
            "5799: Log(reward=101.0, actor_loss=None, critic_loss=45.500018417835236, divergence=None, entropy=None)\n",
            "5836: Log(reward=37.0, actor_loss=None, critic_loss=56.4404690861702, divergence=None, entropy=None)\n",
            "5872: Log(reward=36.0, actor_loss=None, critic_loss=94.75331664085388, divergence=None, entropy=None)\n",
            "5913: Log(reward=41.0, actor_loss=None, critic_loss=67.63676664233208, divergence=None, entropy=None)\n",
            "6045: Log(reward=132.0, actor_loss=None, critic_loss=56.7289479970932, divergence=None, entropy=None)\n",
            "6127: Log(reward=82.0, actor_loss=None, critic_loss=106.69806054234505, divergence=None, entropy=None)\n",
            "6186: Log(reward=59.0, actor_loss=None, critic_loss=79.13923041522503, divergence=None, entropy=None)\n",
            "6386: Log(reward=200.0, actor_loss=None, critic_loss=67.49262464046478, divergence=None, entropy=None)\n",
            "6545: Log(reward=159.0, actor_loss=None, critic_loss=85.55329072475433, divergence=None, entropy=None)\n",
            "6587: Log(reward=42.0, actor_loss=None, critic_loss=35.63692118227482, divergence=None, entropy=None)\n",
            "6616: Log(reward=29.0, actor_loss=None, critic_loss=93.95998671650887, divergence=None, entropy=None)\n",
            "6690: Log(reward=74.0, actor_loss=None, critic_loss=76.2056420147419, divergence=None, entropy=None)\n",
            "6751: Log(reward=61.0, actor_loss=None, critic_loss=84.6419326364994, divergence=None, entropy=None)\n",
            "6874: Log(reward=123.0, actor_loss=None, critic_loss=52.55457980930805, divergence=None, entropy=None)\n",
            "6932: Log(reward=58.0, actor_loss=None, critic_loss=99.23119178414345, divergence=None, entropy=None)\n",
            "7007: Log(reward=75.0, actor_loss=None, critic_loss=73.86968951672316, divergence=None, entropy=None)\n",
            "7055: Log(reward=48.0, actor_loss=None, critic_loss=71.0911531150341, divergence=None, entropy=None)\n",
            "7141: Log(reward=86.0, actor_loss=None, critic_loss=92.2374888509512, divergence=None, entropy=None)\n",
            "7189: Log(reward=48.0, actor_loss=None, critic_loss=81.16443884372711, divergence=None, entropy=None)\n",
            "7316: Log(reward=127.0, actor_loss=None, critic_loss=55.89724989235401, divergence=None, entropy=None)\n",
            "7379: Log(reward=63.0, actor_loss=None, critic_loss=145.49695363640785, divergence=None, entropy=None)\n",
            "7439: Log(reward=60.0, actor_loss=None, critic_loss=87.81214344501495, divergence=None, entropy=None)\n",
            "7493: Log(reward=54.0, actor_loss=None, critic_loss=86.57974368333817, divergence=None, entropy=None)\n",
            "7543: Log(reward=50.0, actor_loss=None, critic_loss=61.2116224616766, divergence=None, entropy=None)\n",
            "7743: Log(reward=200.0, actor_loss=None, critic_loss=128.0022859722376, divergence=None, entropy=None)\n",
            "7796: Log(reward=53.0, actor_loss=None, critic_loss=110.35167133808136, divergence=None, entropy=None)\n",
            "7925: Log(reward=129.0, actor_loss=None, critic_loss=60.40199402719736, divergence=None, entropy=None)\n",
            "7964: Log(reward=39.0, actor_loss=None, critic_loss=72.49123857915401, divergence=None, entropy=None)\n",
            "8018: Log(reward=54.0, actor_loss=None, critic_loss=97.0759232416749, divergence=None, entropy=None)\n",
            "8218: Log(reward=200.0, actor_loss=None, critic_loss=82.20567884296179, divergence=None, entropy=None)\n",
            "8302: Log(reward=84.0, actor_loss=None, critic_loss=59.59077547490597, divergence=None, entropy=None)\n",
            "8341: Log(reward=39.0, actor_loss=None, critic_loss=58.53968404233456, divergence=None, entropy=None)\n",
            "8428: Log(reward=87.0, actor_loss=None, critic_loss=87.04785922914743, divergence=None, entropy=None)\n",
            "8507: Log(reward=79.0, actor_loss=None, critic_loss=118.56741450354457, divergence=None, entropy=None)\n",
            "8707: Log(reward=200.0, actor_loss=None, critic_loss=62.45566813647747, divergence=None, entropy=None)\n",
            "8790: Log(reward=83.0, actor_loss=None, critic_loss=87.03545207902789, divergence=None, entropy=None)\n",
            "8854: Log(reward=64.0, actor_loss=None, critic_loss=137.36444225907326, divergence=None, entropy=None)\n",
            "8889: Log(reward=35.0, actor_loss=None, critic_loss=57.34078170359135, divergence=None, entropy=None)\n",
            "8982: Log(reward=93.0, actor_loss=None, critic_loss=116.41976318508387, divergence=None, entropy=None)\n",
            "9042: Log(reward=60.0, actor_loss=None, critic_loss=140.24301792681217, divergence=None, entropy=None)\n",
            "9137: Log(reward=95.0, actor_loss=None, critic_loss=149.67913749814034, divergence=None, entropy=None)\n",
            "9158: Log(reward=21.0, actor_loss=None, critic_loss=125.7990487292409, divergence=None, entropy=None)\n",
            "9178: Log(reward=20.0, actor_loss=None, critic_loss=48.002505492419004, divergence=None, entropy=None)\n",
            "9204: Log(reward=26.0, actor_loss=None, critic_loss=119.03765750676394, divergence=None, entropy=None)\n",
            "9221: Log(reward=17.0, actor_loss=None, critic_loss=96.6048936843872, divergence=None, entropy=None)\n",
            "9245: Log(reward=24.0, actor_loss=None, critic_loss=114.4457818865776, divergence=None, entropy=None)\n",
            "9288: Log(reward=43.0, actor_loss=None, critic_loss=131.39218750596046, divergence=None, entropy=None)\n",
            "9328: Log(reward=40.0, actor_loss=None, critic_loss=130.09784537553787, divergence=None, entropy=None)\n",
            "9354: Log(reward=26.0, actor_loss=None, critic_loss=125.96491072326899, divergence=None, entropy=None)\n",
            "9398: Log(reward=44.0, actor_loss=None, critic_loss=129.21079055964947, divergence=None, entropy=None)\n",
            "9423: Log(reward=25.0, actor_loss=None, critic_loss=159.2914623916149, divergence=None, entropy=None)\n",
            "9460: Log(reward=37.0, actor_loss=None, critic_loss=126.8838779181242, divergence=None, entropy=None)\n",
            "9474: Log(reward=14.0, actor_loss=None, critic_loss=57.797552511096, divergence=None, entropy=None)\n",
            "9498: Log(reward=24.0, actor_loss=None, critic_loss=132.91234923899174, divergence=None, entropy=None)\n",
            "9513: Log(reward=15.0, actor_loss=None, critic_loss=108.25797406584024, divergence=None, entropy=None)\n",
            "9539: Log(reward=26.0, actor_loss=None, critic_loss=140.14053949713707, divergence=None, entropy=None)\n",
            "9557: Log(reward=18.0, actor_loss=None, critic_loss=130.9589041173458, divergence=None, entropy=None)\n",
            "9569: Log(reward=12.0, actor_loss=None, critic_loss=110.41334709525108, divergence=None, entropy=None)\n",
            "9582: Log(reward=13.0, actor_loss=None, critic_loss=76.60543943941593, divergence=None, entropy=None)\n",
            "9601: Log(reward=19.0, actor_loss=None, critic_loss=254.29529684782028, divergence=None, entropy=None)\n",
            "9633: Log(reward=32.0, actor_loss=None, critic_loss=113.79237178713083, divergence=None, entropy=None)\n",
            "9654: Log(reward=21.0, actor_loss=None, critic_loss=78.98975875973701, divergence=None, entropy=None)\n",
            "9814: Log(reward=160.0, actor_loss=None, critic_loss=170.92815957963467, divergence=None, entropy=None)\n",
            "9847: Log(reward=33.0, actor_loss=None, critic_loss=181.56882047653198, divergence=None, entropy=None)\n",
            "9889: Log(reward=42.0, actor_loss=None, critic_loss=131.57474499940872, divergence=None, entropy=None)\n",
            "9903: Log(reward=14.0, actor_loss=None, critic_loss=109.28932444006205, divergence=None, entropy=None)\n",
            "9915: Log(reward=12.0, actor_loss=None, critic_loss=107.4561667740345, divergence=None, entropy=None)\n",
            "9930: Log(reward=15.0, actor_loss=None, critic_loss=134.41193091124296, divergence=None, entropy=None)\n",
            "9943: Log(reward=13.0, actor_loss=None, critic_loss=86.91083140671253, divergence=None, entropy=None)\n",
            "9953: Log(reward=10.0, actor_loss=None, critic_loss=109.30900233983994, divergence=None, entropy=None)\n",
            "9965: Log(reward=12.0, actor_loss=None, critic_loss=119.25556502491236, divergence=None, entropy=None)\n",
            "9975: Log(reward=10.0, actor_loss=None, critic_loss=159.74935007840395, divergence=None, entropy=None)\n",
            "9985: Log(reward=10.0, actor_loss=None, critic_loss=106.33568388223648, divergence=None, entropy=None)\n",
            "9996: Log(reward=11.0, actor_loss=None, critic_loss=144.74722906947136, divergence=None, entropy=None)\n",
            "10008: Log(reward=12.0, actor_loss=None, critic_loss=173.84677748382092, divergence=None, entropy=None)\n",
            "10017: Log(reward=9.0, actor_loss=None, critic_loss=129.654587328434, divergence=None, entropy=None)\n",
            "10030: Log(reward=13.0, actor_loss=None, critic_loss=169.20705652236938, divergence=None, entropy=None)\n",
            "10048: Log(reward=18.0, actor_loss=None, critic_loss=106.91098634898663, divergence=None, entropy=None)\n",
            "10085: Log(reward=37.0, actor_loss=None, critic_loss=197.31216302514076, divergence=None, entropy=None)\n",
            "10101: Log(reward=16.0, actor_loss=None, critic_loss=114.4603338688612, divergence=None, entropy=None)\n",
            "10113: Log(reward=12.0, actor_loss=None, critic_loss=112.55107566714287, divergence=None, entropy=None)\n",
            "10127: Log(reward=14.0, actor_loss=None, critic_loss=120.50198651105165, divergence=None, entropy=None)\n",
            "10327: Log(reward=200.0, actor_loss=None, critic_loss=195.39520919322968, divergence=None, entropy=None)\n",
            "10450: Log(reward=123.0, actor_loss=None, critic_loss=145.70305775105953, divergence=None, entropy=None)\n",
            "10485: Log(reward=35.0, actor_loss=None, critic_loss=101.81687074899673, divergence=None, entropy=None)\n",
            "10524: Log(reward=39.0, actor_loss=None, critic_loss=98.42249005287886, divergence=None, entropy=None)\n",
            "10549: Log(reward=25.0, actor_loss=None, critic_loss=148.76457600295544, divergence=None, entropy=None)\n",
            "10749: Log(reward=200.0, actor_loss=None, critic_loss=117.11974600702524, divergence=None, entropy=None)\n",
            "10764: Log(reward=15.0, actor_loss=None, critic_loss=118.04257649183273, divergence=None, entropy=None)\n",
            "10775: Log(reward=11.0, actor_loss=None, critic_loss=149.30775167047977, divergence=None, entropy=None)\n",
            "10786: Log(reward=11.0, actor_loss=None, critic_loss=147.37447395920753, divergence=None, entropy=None)\n",
            "10795: Log(reward=9.0, actor_loss=None, critic_loss=181.04444658756256, divergence=None, entropy=None)\n",
            "10805: Log(reward=10.0, actor_loss=None, critic_loss=94.91465173661709, divergence=None, entropy=None)\n",
            "10815: Log(reward=10.0, actor_loss=None, critic_loss=99.54878658801317, divergence=None, entropy=None)\n",
            "10869: Log(reward=54.0, actor_loss=None, critic_loss=136.63047796487808, divergence=None, entropy=None)\n",
            "10900: Log(reward=31.0, actor_loss=None, critic_loss=160.09957964718342, divergence=None, entropy=None)\n",
            "10969: Log(reward=69.0, actor_loss=None, critic_loss=140.64763535559177, divergence=None, entropy=None)\n",
            "11169: Log(reward=200.0, actor_loss=None, critic_loss=116.59729561209679, divergence=None, entropy=None)\n",
            "11179: Log(reward=10.0, actor_loss=None, critic_loss=72.32223466038704, divergence=None, entropy=None)\n",
            "11189: Log(reward=10.0, actor_loss=None, critic_loss=106.98961679637432, divergence=None, entropy=None)\n",
            "11198: Log(reward=9.0, actor_loss=None, critic_loss=147.29010596871376, divergence=None, entropy=None)\n",
            "11206: Log(reward=8.0, actor_loss=None, critic_loss=94.60956507921219, divergence=None, entropy=None)\n",
            "11217: Log(reward=11.0, actor_loss=None, critic_loss=143.47473043203354, divergence=None, entropy=None)\n",
            "11227: Log(reward=10.0, actor_loss=None, critic_loss=192.20493912696838, divergence=None, entropy=None)\n",
            "11237: Log(reward=10.0, actor_loss=None, critic_loss=86.48508657515049, divergence=None, entropy=None)\n",
            "11246: Log(reward=9.0, actor_loss=None, critic_loss=86.79516097158194, divergence=None, entropy=None)\n",
            "11255: Log(reward=9.0, actor_loss=None, critic_loss=141.98092991113663, divergence=None, entropy=None)\n",
            "11265: Log(reward=10.0, actor_loss=None, critic_loss=201.23672322928905, divergence=None, entropy=None)\n",
            "11274: Log(reward=9.0, actor_loss=None, critic_loss=108.16331593692303, divergence=None, entropy=None)\n",
            "11283: Log(reward=9.0, actor_loss=None, critic_loss=125.21641448140144, divergence=None, entropy=None)\n",
            "11291: Log(reward=8.0, actor_loss=None, critic_loss=65.8198207616806, divergence=None, entropy=None)\n",
            "11301: Log(reward=10.0, actor_loss=None, critic_loss=152.13996489346027, divergence=None, entropy=None)\n",
            "11310: Log(reward=9.0, actor_loss=None, critic_loss=148.60897395014763, divergence=None, entropy=None)\n",
            "11320: Log(reward=10.0, actor_loss=None, critic_loss=88.54690104722977, divergence=None, entropy=None)\n",
            "11328: Log(reward=8.0, actor_loss=None, critic_loss=237.8409089744091, divergence=None, entropy=None)\n",
            "11338: Log(reward=10.0, actor_loss=None, critic_loss=95.54301779717207, divergence=None, entropy=None)\n",
            "11348: Log(reward=10.0, actor_loss=None, critic_loss=95.8408540263772, divergence=None, entropy=None)\n",
            "11358: Log(reward=10.0, actor_loss=None, critic_loss=104.3333725631237, divergence=None, entropy=None)\n",
            "11368: Log(reward=10.0, actor_loss=None, critic_loss=146.68171060085297, divergence=None, entropy=None)\n",
            "11377: Log(reward=9.0, actor_loss=None, critic_loss=220.66803592443466, divergence=None, entropy=None)\n",
            "11386: Log(reward=9.0, actor_loss=None, critic_loss=230.52951896190643, divergence=None, entropy=None)\n",
            "11395: Log(reward=9.0, actor_loss=None, critic_loss=173.43513226509094, divergence=None, entropy=None)\n",
            "11404: Log(reward=9.0, actor_loss=None, critic_loss=147.1393706202507, divergence=None, entropy=None)\n",
            "11413: Log(reward=9.0, actor_loss=None, critic_loss=134.81860534846783, divergence=None, entropy=None)\n",
            "11422: Log(reward=9.0, actor_loss=None, critic_loss=198.6804795563221, divergence=None, entropy=None)\n",
            "11431: Log(reward=9.0, actor_loss=None, critic_loss=166.91774190962315, divergence=None, entropy=None)\n",
            "11440: Log(reward=9.0, actor_loss=None, critic_loss=70.55967615544796, divergence=None, entropy=None)\n",
            "11449: Log(reward=9.0, actor_loss=None, critic_loss=198.3506237268448, divergence=None, entropy=None)\n",
            "11459: Log(reward=10.0, actor_loss=None, critic_loss=185.51736649870872, divergence=None, entropy=None)\n",
            "11469: Log(reward=10.0, actor_loss=None, critic_loss=193.94619078934193, divergence=None, entropy=None)\n",
            "11479: Log(reward=10.0, actor_loss=None, critic_loss=181.46149945259094, divergence=None, entropy=None)\n",
            "11489: Log(reward=10.0, actor_loss=None, critic_loss=319.38147896528244, divergence=None, entropy=None)\n",
            "11498: Log(reward=9.0, actor_loss=None, critic_loss=268.6884396895766, divergence=None, entropy=None)\n",
            "11507: Log(reward=9.0, actor_loss=None, critic_loss=145.95750352740288, divergence=None, entropy=None)\n",
            "11518: Log(reward=11.0, actor_loss=None, critic_loss=417.7803128361702, divergence=None, entropy=None)\n",
            "11528: Log(reward=10.0, actor_loss=None, critic_loss=332.4205242395401, divergence=None, entropy=None)\n",
            "11538: Log(reward=10.0, actor_loss=None, critic_loss=312.2221297323704, divergence=None, entropy=None)\n",
            "11546: Log(reward=8.0, actor_loss=None, critic_loss=542.1840341091156, divergence=None, entropy=None)\n",
            "11555: Log(reward=9.0, actor_loss=None, critic_loss=694.9272427558899, divergence=None, entropy=None)\n",
            "11564: Log(reward=9.0, actor_loss=None, critic_loss=254.31973764300346, divergence=None, entropy=None)\n",
            "11573: Log(reward=9.0, actor_loss=None, critic_loss=915.1406543254852, divergence=None, entropy=None)\n",
            "11584: Log(reward=11.0, actor_loss=None, critic_loss=677.9021561145782, divergence=None, entropy=None)\n",
            "11593: Log(reward=9.0, actor_loss=None, critic_loss=717.298406124115, divergence=None, entropy=None)\n",
            "11603: Log(reward=10.0, actor_loss=None, critic_loss=932.9899098873138, divergence=None, entropy=None)\n",
            "11612: Log(reward=9.0, actor_loss=None, critic_loss=1081.6440229415894, divergence=None, entropy=None)\n",
            "11620: Log(reward=8.0, actor_loss=None, critic_loss=525.0977501273155, divergence=None, entropy=None)\n",
            "11629: Log(reward=9.0, actor_loss=None, critic_loss=1700.7377996444702, divergence=None, entropy=None)\n",
            "11637: Log(reward=8.0, actor_loss=None, critic_loss=1055.8334486484528, divergence=None, entropy=None)\n",
            "11646: Log(reward=9.0, actor_loss=None, critic_loss=1893.143765449524, divergence=None, entropy=None)\n",
            "11656: Log(reward=10.0, actor_loss=None, critic_loss=1135.4841158390045, divergence=None, entropy=None)\n",
            "11667: Log(reward=11.0, actor_loss=None, critic_loss=876.7476134300232, divergence=None, entropy=None)\n",
            "11675: Log(reward=8.0, actor_loss=None, critic_loss=1558.0775294303894, divergence=None, entropy=None)\n",
            "11684: Log(reward=9.0, actor_loss=None, critic_loss=1830.2334413528442, divergence=None, entropy=None)\n",
            "11693: Log(reward=9.0, actor_loss=None, critic_loss=1453.2660236358643, divergence=None, entropy=None)\n",
            "11701: Log(reward=8.0, actor_loss=None, critic_loss=1396.9889817237854, divergence=None, entropy=None)\n",
            "11710: Log(reward=9.0, actor_loss=None, critic_loss=4225.545902252197, divergence=None, entropy=None)\n",
            "11719: Log(reward=9.0, actor_loss=None, critic_loss=2676.628101348877, divergence=None, entropy=None)\n",
            "11728: Log(reward=9.0, actor_loss=None, critic_loss=1860.751256942749, divergence=None, entropy=None)\n",
            "11738: Log(reward=10.0, actor_loss=None, critic_loss=4269.1440172195435, divergence=None, entropy=None)\n",
            "11748: Log(reward=10.0, actor_loss=None, critic_loss=2880.058464050293, divergence=None, entropy=None)\n",
            "11759: Log(reward=11.0, actor_loss=None, critic_loss=3454.4727354049683, divergence=None, entropy=None)\n",
            "11769: Log(reward=10.0, actor_loss=None, critic_loss=6819.977760314941, divergence=None, entropy=None)\n",
            "11778: Log(reward=9.0, actor_loss=None, critic_loss=9534.523403167725, divergence=None, entropy=None)\n",
            "11788: Log(reward=10.0, actor_loss=None, critic_loss=7546.391448974609, divergence=None, entropy=None)\n",
            "11798: Log(reward=10.0, actor_loss=None, critic_loss=7334.436635971069, divergence=None, entropy=None)\n",
            "11807: Log(reward=9.0, actor_loss=None, critic_loss=11601.39011001587, divergence=None, entropy=None)\n",
            "11816: Log(reward=9.0, actor_loss=None, critic_loss=5137.50866317749, divergence=None, entropy=None)\n",
            "11825: Log(reward=9.0, actor_loss=None, critic_loss=5987.688613891602, divergence=None, entropy=None)\n",
            "11834: Log(reward=9.0, actor_loss=None, critic_loss=5686.043727874756, divergence=None, entropy=None)\n",
            "11842: Log(reward=8.0, actor_loss=None, critic_loss=11699.047897338867, divergence=None, entropy=None)\n",
            "11851: Log(reward=9.0, actor_loss=None, critic_loss=5064.672857284546, divergence=None, entropy=None)\n",
            "11861: Log(reward=10.0, actor_loss=None, critic_loss=8959.9284324646, divergence=None, entropy=None)\n",
            "11870: Log(reward=9.0, actor_loss=None, critic_loss=8862.536590576172, divergence=None, entropy=None)\n",
            "11879: Log(reward=9.0, actor_loss=None, critic_loss=11166.866149902344, divergence=None, entropy=None)\n",
            "11888: Log(reward=9.0, actor_loss=None, critic_loss=11152.562309265137, divergence=None, entropy=None)\n",
            "11897: Log(reward=9.0, actor_loss=None, critic_loss=7228.145658493042, divergence=None, entropy=None)\n",
            "11905: Log(reward=8.0, actor_loss=None, critic_loss=10775.73295211792, divergence=None, entropy=None)\n",
            "11915: Log(reward=10.0, actor_loss=None, critic_loss=8972.15163230896, divergence=None, entropy=None)\n",
            "11924: Log(reward=9.0, actor_loss=None, critic_loss=12386.06664276123, divergence=None, entropy=None)\n",
            "11932: Log(reward=8.0, actor_loss=None, critic_loss=16557.96859741211, divergence=None, entropy=None)\n",
            "11940: Log(reward=8.0, actor_loss=None, critic_loss=10386.586990356445, divergence=None, entropy=None)\n",
            "11951: Log(reward=11.0, actor_loss=None, critic_loss=18012.988430023193, divergence=None, entropy=None)\n",
            "11960: Log(reward=9.0, actor_loss=None, critic_loss=22066.774772644043, divergence=None, entropy=None)\n",
            "11970: Log(reward=10.0, actor_loss=None, critic_loss=33855.00005340576, divergence=None, entropy=None)\n",
            "11979: Log(reward=9.0, actor_loss=None, critic_loss=28211.35531616211, divergence=None, entropy=None)\n",
            "11988: Log(reward=9.0, actor_loss=None, critic_loss=24713.97999572754, divergence=None, entropy=None)\n",
            "11997: Log(reward=9.0, actor_loss=None, critic_loss=18952.725311279297, divergence=None, entropy=None)\n",
            "12006: Log(reward=9.0, actor_loss=None, critic_loss=25164.628074645996, divergence=None, entropy=None)\n",
            "12016: Log(reward=10.0, actor_loss=None, critic_loss=28722.16799545288, divergence=None, entropy=None)\n",
            "12025: Log(reward=9.0, actor_loss=None, critic_loss=12947.401977539062, divergence=None, entropy=None)\n",
            "12035: Log(reward=10.0, actor_loss=None, critic_loss=33823.71829986572, divergence=None, entropy=None)\n",
            "12045: Log(reward=10.0, actor_loss=None, critic_loss=12906.058647155762, divergence=None, entropy=None)\n",
            "12055: Log(reward=10.0, actor_loss=None, critic_loss=12599.793239593506, divergence=None, entropy=None)\n",
            "12065: Log(reward=10.0, actor_loss=None, critic_loss=14179.054988861084, divergence=None, entropy=None)\n",
            "12073: Log(reward=8.0, actor_loss=None, critic_loss=47540.3291015625, divergence=None, entropy=None)\n",
            "12083: Log(reward=10.0, actor_loss=None, critic_loss=21473.926063537598, divergence=None, entropy=None)\n",
            "12093: Log(reward=10.0, actor_loss=None, critic_loss=16387.08940887451, divergence=None, entropy=None)\n",
            "12103: Log(reward=10.0, actor_loss=None, critic_loss=44516.213439941406, divergence=None, entropy=None)\n",
            "12112: Log(reward=9.0, actor_loss=None, critic_loss=39411.983963012695, divergence=None, entropy=None)\n",
            "12120: Log(reward=8.0, actor_loss=None, critic_loss=46817.45115661621, divergence=None, entropy=None)\n",
            "12130: Log(reward=10.0, actor_loss=None, critic_loss=35656.00622558594, divergence=None, entropy=None)\n",
            "12140: Log(reward=10.0, actor_loss=None, critic_loss=30586.33416748047, divergence=None, entropy=None)\n",
            "12150: Log(reward=10.0, actor_loss=None, critic_loss=47066.87461853027, divergence=None, entropy=None)\n",
            "12160: Log(reward=10.0, actor_loss=None, critic_loss=30509.281127929688, divergence=None, entropy=None)\n",
            "12169: Log(reward=9.0, actor_loss=None, critic_loss=65815.79898071289, divergence=None, entropy=None)\n",
            "12179: Log(reward=10.0, actor_loss=None, critic_loss=47536.092041015625, divergence=None, entropy=None)\n",
            "12189: Log(reward=10.0, actor_loss=None, critic_loss=48133.946212768555, divergence=None, entropy=None)\n",
            "12198: Log(reward=9.0, actor_loss=None, critic_loss=47503.32873535156, divergence=None, entropy=None)\n",
            "12207: Log(reward=9.0, actor_loss=None, critic_loss=69843.53646850586, divergence=None, entropy=None)\n",
            "12216: Log(reward=9.0, actor_loss=None, critic_loss=65073.4150390625, divergence=None, entropy=None)\n",
            "12226: Log(reward=10.0, actor_loss=None, critic_loss=52567.36181640625, divergence=None, entropy=None)\n",
            "12235: Log(reward=9.0, actor_loss=None, critic_loss=70862.70101928711, divergence=None, entropy=None)\n",
            "12243: Log(reward=8.0, actor_loss=None, critic_loss=25320.71826171875, divergence=None, entropy=None)\n",
            "12252: Log(reward=9.0, actor_loss=None, critic_loss=53670.67579650879, divergence=None, entropy=None)\n",
            "12260: Log(reward=8.0, actor_loss=None, critic_loss=72368.95352172852, divergence=None, entropy=None)\n",
            "12269: Log(reward=9.0, actor_loss=None, critic_loss=26743.85720062256, divergence=None, entropy=None)\n",
            "12278: Log(reward=9.0, actor_loss=None, critic_loss=119497.13732910156, divergence=None, entropy=None)\n",
            "12288: Log(reward=10.0, actor_loss=None, critic_loss=56304.032287597656, divergence=None, entropy=None)\n",
            "12297: Log(reward=9.0, actor_loss=None, critic_loss=108800.56887817383, divergence=None, entropy=None)\n",
            "12307: Log(reward=10.0, actor_loss=None, critic_loss=103010.14459228516, divergence=None, entropy=None)\n",
            "12315: Log(reward=8.0, actor_loss=None, critic_loss=126007.48596191406, divergence=None, entropy=None)\n",
            "12324: Log(reward=9.0, actor_loss=None, critic_loss=35479.31904602051, divergence=None, entropy=None)\n",
            "12333: Log(reward=9.0, actor_loss=None, critic_loss=108153.95266723633, divergence=None, entropy=None)\n",
            "12342: Log(reward=9.0, actor_loss=None, critic_loss=86267.96011352539, divergence=None, entropy=None)\n",
            "12352: Log(reward=10.0, actor_loss=None, critic_loss=78212.76983642578, divergence=None, entropy=None)\n",
            "12362: Log(reward=10.0, actor_loss=None, critic_loss=119687.60922241211, divergence=None, entropy=None)\n",
            "12371: Log(reward=9.0, actor_loss=None, critic_loss=63802.96081542969, divergence=None, entropy=None)\n",
            "12380: Log(reward=9.0, actor_loss=None, critic_loss=107591.69882202148, divergence=None, entropy=None)\n",
            "12388: Log(reward=8.0, actor_loss=None, critic_loss=150531.1248779297, divergence=None, entropy=None)\n",
            "12398: Log(reward=10.0, actor_loss=None, critic_loss=62008.414001464844, divergence=None, entropy=None)\n",
            "12408: Log(reward=10.0, actor_loss=None, critic_loss=65612.29025268555, divergence=None, entropy=None)\n",
            "12417: Log(reward=9.0, actor_loss=None, critic_loss=134219.65209960938, divergence=None, entropy=None)\n",
            "12425: Log(reward=8.0, actor_loss=None, critic_loss=141699.09832763672, divergence=None, entropy=None)\n",
            "12435: Log(reward=10.0, actor_loss=None, critic_loss=132196.86920166016, divergence=None, entropy=None)\n",
            "12444: Log(reward=9.0, actor_loss=None, critic_loss=126500.25527954102, divergence=None, entropy=None)\n",
            "12454: Log(reward=10.0, actor_loss=None, critic_loss=170837.3642578125, divergence=None, entropy=None)\n",
            "12463: Log(reward=9.0, actor_loss=None, critic_loss=121025.93658447266, divergence=None, entropy=None)\n",
            "12473: Log(reward=10.0, actor_loss=None, critic_loss=177271.1387939453, divergence=None, entropy=None)\n",
            "12483: Log(reward=10.0, actor_loss=None, critic_loss=166027.4672241211, divergence=None, entropy=None)\n",
            "12494: Log(reward=11.0, actor_loss=None, critic_loss=167800.7872314453, divergence=None, entropy=None)\n",
            "12504: Log(reward=10.0, actor_loss=None, critic_loss=117443.08489990234, divergence=None, entropy=None)\n",
            "12512: Log(reward=8.0, actor_loss=None, critic_loss=125905.73803710938, divergence=None, entropy=None)\n",
            "12521: Log(reward=9.0, actor_loss=None, critic_loss=170425.49224853516, divergence=None, entropy=None)\n",
            "12531: Log(reward=10.0, actor_loss=None, critic_loss=194271.94256591797, divergence=None, entropy=None)\n",
            "12541: Log(reward=10.0, actor_loss=None, critic_loss=126492.47161865234, divergence=None, entropy=None)\n",
            "12551: Log(reward=10.0, actor_loss=None, critic_loss=120479.96966552734, divergence=None, entropy=None)\n",
            "12559: Log(reward=8.0, actor_loss=None, critic_loss=202943.01879882812, divergence=None, entropy=None)\n",
            "12569: Log(reward=10.0, actor_loss=None, critic_loss=152965.99908447266, divergence=None, entropy=None)\n",
            "12579: Log(reward=10.0, actor_loss=None, critic_loss=120959.80126953125, divergence=None, entropy=None)\n",
            "12588: Log(reward=9.0, actor_loss=None, critic_loss=171349.61895751953, divergence=None, entropy=None)\n",
            "12597: Log(reward=9.0, actor_loss=None, critic_loss=216514.04559326172, divergence=None, entropy=None)\n",
            "12607: Log(reward=10.0, actor_loss=None, critic_loss=190368.4204711914, divergence=None, entropy=None)\n",
            "12616: Log(reward=9.0, actor_loss=None, critic_loss=100243.83013916016, divergence=None, entropy=None)\n",
            "12624: Log(reward=8.0, actor_loss=None, critic_loss=155159.1001586914, divergence=None, entropy=None)\n",
            "12634: Log(reward=10.0, actor_loss=None, critic_loss=198710.9761352539, divergence=None, entropy=None)\n",
            "12644: Log(reward=10.0, actor_loss=None, critic_loss=187736.2514038086, divergence=None, entropy=None)\n",
            "12654: Log(reward=10.0, actor_loss=None, critic_loss=152890.69311523438, divergence=None, entropy=None)\n",
            "12665: Log(reward=11.0, actor_loss=None, critic_loss=167824.69555664062, divergence=None, entropy=None)\n",
            "12674: Log(reward=9.0, actor_loss=None, critic_loss=203751.91345214844, divergence=None, entropy=None)\n",
            "12683: Log(reward=9.0, actor_loss=None, critic_loss=137236.81048583984, divergence=None, entropy=None)\n",
            "12692: Log(reward=9.0, actor_loss=None, critic_loss=178225.63739013672, divergence=None, entropy=None)\n",
            "12701: Log(reward=9.0, actor_loss=None, critic_loss=200202.333984375, divergence=None, entropy=None)\n",
            "12711: Log(reward=10.0, actor_loss=None, critic_loss=289514.44860839844, divergence=None, entropy=None)\n",
            "12721: Log(reward=10.0, actor_loss=None, critic_loss=294323.66735839844, divergence=None, entropy=None)\n",
            "12730: Log(reward=9.0, actor_loss=None, critic_loss=207523.4136352539, divergence=None, entropy=None)\n",
            "12740: Log(reward=10.0, actor_loss=None, critic_loss=235030.2553100586, divergence=None, entropy=None)\n",
            "12750: Log(reward=10.0, actor_loss=None, critic_loss=278907.70751953125, divergence=None, entropy=None)\n",
            "12760: Log(reward=10.0, actor_loss=None, critic_loss=250545.3497314453, divergence=None, entropy=None)\n",
            "12770: Log(reward=10.0, actor_loss=None, critic_loss=107550.40850830078, divergence=None, entropy=None)\n",
            "12779: Log(reward=9.0, actor_loss=None, critic_loss=179364.3256225586, divergence=None, entropy=None)\n",
            "12789: Log(reward=10.0, actor_loss=None, critic_loss=187915.5538330078, divergence=None, entropy=None)\n",
            "12797: Log(reward=8.0, actor_loss=None, critic_loss=280753.8381347656, divergence=None, entropy=None)\n",
            "12806: Log(reward=9.0, actor_loss=None, critic_loss=223939.63641357422, divergence=None, entropy=None)\n",
            "12816: Log(reward=10.0, actor_loss=None, critic_loss=176000.28826904297, divergence=None, entropy=None)\n",
            "12826: Log(reward=10.0, actor_loss=None, critic_loss=271158.87225341797, divergence=None, entropy=None)\n",
            "12836: Log(reward=10.0, actor_loss=None, critic_loss=264842.11267089844, divergence=None, entropy=None)\n",
            "12846: Log(reward=10.0, actor_loss=None, critic_loss=254300.1260986328, divergence=None, entropy=None)\n",
            "12856: Log(reward=10.0, actor_loss=None, critic_loss=287435.22509765625, divergence=None, entropy=None)\n",
            "12866: Log(reward=10.0, actor_loss=None, critic_loss=319114.1667480469, divergence=None, entropy=None)\n",
            "12877: Log(reward=11.0, actor_loss=None, critic_loss=179643.78814697266, divergence=None, entropy=None)\n",
            "12886: Log(reward=9.0, actor_loss=None, critic_loss=146624.91522216797, divergence=None, entropy=None)\n",
            "12896: Log(reward=10.0, actor_loss=None, critic_loss=351573.0679321289, divergence=None, entropy=None)\n",
            "12906: Log(reward=10.0, actor_loss=None, critic_loss=192527.52606201172, divergence=None, entropy=None)\n",
            "12915: Log(reward=9.0, actor_loss=None, critic_loss=287118.96484375, divergence=None, entropy=None)\n",
            "12925: Log(reward=10.0, actor_loss=None, critic_loss=214992.6083984375, divergence=None, entropy=None)\n",
            "12936: Log(reward=11.0, actor_loss=None, critic_loss=236104.4393310547, divergence=None, entropy=None)\n",
            "12944: Log(reward=8.0, actor_loss=None, critic_loss=172408.1224975586, divergence=None, entropy=None)\n",
            "12954: Log(reward=10.0, actor_loss=None, critic_loss=134534.4087524414, divergence=None, entropy=None)\n",
            "12963: Log(reward=9.0, actor_loss=None, critic_loss=379350.7373046875, divergence=None, entropy=None)\n",
            "12973: Log(reward=10.0, actor_loss=None, critic_loss=384571.7712402344, divergence=None, entropy=None)\n",
            "12982: Log(reward=9.0, actor_loss=None, critic_loss=320522.73889160156, divergence=None, entropy=None)\n",
            "12991: Log(reward=9.0, actor_loss=None, critic_loss=240586.8857421875, divergence=None, entropy=None)\n",
            "13000: Log(reward=9.0, actor_loss=None, critic_loss=335823.55352783203, divergence=None, entropy=None)\n",
            "13010: Log(reward=10.0, actor_loss=None, critic_loss=235309.251953125, divergence=None, entropy=None)\n",
            "13019: Log(reward=9.0, actor_loss=None, critic_loss=374019.2165527344, divergence=None, entropy=None)\n",
            "13029: Log(reward=10.0, actor_loss=None, critic_loss=313723.1390991211, divergence=None, entropy=None)\n",
            "13039: Log(reward=10.0, actor_loss=None, critic_loss=284674.6141357422, divergence=None, entropy=None)\n",
            "13049: Log(reward=10.0, actor_loss=None, critic_loss=327724.7578125, divergence=None, entropy=None)\n",
            "13058: Log(reward=9.0, actor_loss=None, critic_loss=276870.5900878906, divergence=None, entropy=None)\n",
            "13068: Log(reward=10.0, actor_loss=None, critic_loss=218436.50354003906, divergence=None, entropy=None)\n",
            "13076: Log(reward=8.0, actor_loss=None, critic_loss=342149.60546875, divergence=None, entropy=None)\n",
            "13085: Log(reward=9.0, actor_loss=None, critic_loss=330419.3129272461, divergence=None, entropy=None)\n",
            "13094: Log(reward=9.0, actor_loss=None, critic_loss=257299.57720947266, divergence=None, entropy=None)\n",
            "13104: Log(reward=10.0, actor_loss=None, critic_loss=279999.80084228516, divergence=None, entropy=None)\n",
            "13114: Log(reward=10.0, actor_loss=None, critic_loss=151829.03372192383, divergence=None, entropy=None)\n",
            "13123: Log(reward=9.0, actor_loss=None, critic_loss=241490.5994873047, divergence=None, entropy=None)\n",
            "13131: Log(reward=8.0, actor_loss=None, critic_loss=170947.9621887207, divergence=None, entropy=None)\n",
            "13141: Log(reward=10.0, actor_loss=None, critic_loss=231821.24884033203, divergence=None, entropy=None)\n",
            "13150: Log(reward=9.0, actor_loss=None, critic_loss=198138.33374023438, divergence=None, entropy=None)\n",
            "13160: Log(reward=10.0, actor_loss=None, critic_loss=376981.3391723633, divergence=None, entropy=None)\n",
            "13170: Log(reward=10.0, actor_loss=None, critic_loss=257637.26568603516, divergence=None, entropy=None)\n",
            "13180: Log(reward=10.0, actor_loss=None, critic_loss=299054.2317504883, divergence=None, entropy=None)\n",
            "13190: Log(reward=10.0, actor_loss=None, critic_loss=263856.05712890625, divergence=None, entropy=None)\n",
            "13199: Log(reward=9.0, actor_loss=None, critic_loss=132583.1310119629, divergence=None, entropy=None)\n",
            "13209: Log(reward=10.0, actor_loss=None, critic_loss=290439.6653442383, divergence=None, entropy=None)\n",
            "13218: Log(reward=9.0, actor_loss=None, critic_loss=200777.6403503418, divergence=None, entropy=None)\n",
            "13227: Log(reward=9.0, actor_loss=None, critic_loss=212780.1987915039, divergence=None, entropy=None)\n",
            "13235: Log(reward=8.0, actor_loss=None, critic_loss=352750.46032714844, divergence=None, entropy=None)\n",
            "13245: Log(reward=10.0, actor_loss=None, critic_loss=125041.53611755371, divergence=None, entropy=None)\n",
            "13253: Log(reward=8.0, actor_loss=None, critic_loss=178826.625, divergence=None, entropy=None)\n",
            "13263: Log(reward=10.0, actor_loss=None, critic_loss=278720.28216552734, divergence=None, entropy=None)\n",
            "13272: Log(reward=9.0, actor_loss=None, critic_loss=236995.38342285156, divergence=None, entropy=None)\n",
            "13281: Log(reward=9.0, actor_loss=None, critic_loss=278104.154296875, divergence=None, entropy=None)\n",
            "13291: Log(reward=10.0, actor_loss=None, critic_loss=305406.9719238281, divergence=None, entropy=None)\n",
            "13300: Log(reward=9.0, actor_loss=None, critic_loss=251911.10150146484, divergence=None, entropy=None)\n",
            "13309: Log(reward=9.0, actor_loss=None, critic_loss=273915.88119506836, divergence=None, entropy=None)\n",
            "13318: Log(reward=9.0, actor_loss=None, critic_loss=309702.6599121094, divergence=None, entropy=None)\n",
            "13328: Log(reward=10.0, actor_loss=None, critic_loss=243922.41497802734, divergence=None, entropy=None)\n",
            "13337: Log(reward=9.0, actor_loss=None, critic_loss=277777.0637817383, divergence=None, entropy=None)\n",
            "13347: Log(reward=10.0, actor_loss=None, critic_loss=255046.7032470703, divergence=None, entropy=None)\n",
            "13357: Log(reward=10.0, actor_loss=None, critic_loss=261515.9995727539, divergence=None, entropy=None)\n",
            "13366: Log(reward=9.0, actor_loss=None, critic_loss=245941.0124206543, divergence=None, entropy=None)\n",
            "13375: Log(reward=9.0, actor_loss=None, critic_loss=242157.6483154297, divergence=None, entropy=None)\n",
            "13384: Log(reward=9.0, actor_loss=None, critic_loss=119432.2064666748, divergence=None, entropy=None)\n",
            "13394: Log(reward=10.0, actor_loss=None, critic_loss=206160.6593322754, divergence=None, entropy=None)\n",
            "13403: Log(reward=9.0, actor_loss=None, critic_loss=247373.6407775879, divergence=None, entropy=None)\n",
            "13412: Log(reward=9.0, actor_loss=None, critic_loss=123621.44401550293, divergence=None, entropy=None)\n",
            "13420: Log(reward=8.0, actor_loss=None, critic_loss=252395.0425415039, divergence=None, entropy=None)\n",
            "13429: Log(reward=9.0, actor_loss=None, critic_loss=181167.43322753906, divergence=None, entropy=None)\n",
            "13438: Log(reward=9.0, actor_loss=None, critic_loss=149505.65350341797, divergence=None, entropy=None)\n",
            "13448: Log(reward=10.0, actor_loss=None, critic_loss=252227.37109375, divergence=None, entropy=None)\n",
            "13459: Log(reward=11.0, actor_loss=None, critic_loss=237484.53073120117, divergence=None, entropy=None)\n",
            "13468: Log(reward=9.0, actor_loss=None, critic_loss=175713.63052368164, divergence=None, entropy=None)\n",
            "13478: Log(reward=10.0, actor_loss=None, critic_loss=265706.2947692871, divergence=None, entropy=None)\n",
            "13488: Log(reward=10.0, actor_loss=None, critic_loss=244224.82180786133, divergence=None, entropy=None)\n",
            "13498: Log(reward=10.0, actor_loss=None, critic_loss=138918.64823913574, divergence=None, entropy=None)\n",
            "13506: Log(reward=8.0, actor_loss=None, critic_loss=220392.3716430664, divergence=None, entropy=None)\n",
            "13515: Log(reward=9.0, actor_loss=None, critic_loss=138979.01596069336, divergence=None, entropy=None)\n",
            "13524: Log(reward=9.0, actor_loss=None, critic_loss=123131.04107666016, divergence=None, entropy=None)\n",
            "13532: Log(reward=8.0, actor_loss=None, critic_loss=192941.36630249023, divergence=None, entropy=None)\n",
            "13541: Log(reward=9.0, actor_loss=None, critic_loss=176770.71907043457, divergence=None, entropy=None)\n",
            "13550: Log(reward=9.0, actor_loss=None, critic_loss=77187.38009643555, divergence=None, entropy=None)\n",
            "13560: Log(reward=10.0, actor_loss=None, critic_loss=111165.63603210449, divergence=None, entropy=None)\n",
            "13568: Log(reward=8.0, actor_loss=None, critic_loss=131051.71182250977, divergence=None, entropy=None)\n",
            "13578: Log(reward=10.0, actor_loss=None, critic_loss=99426.0330657959, divergence=None, entropy=None)\n",
            "13588: Log(reward=10.0, actor_loss=None, critic_loss=138909.67861938477, divergence=None, entropy=None)\n",
            "13598: Log(reward=10.0, actor_loss=None, critic_loss=83496.72904205322, divergence=None, entropy=None)\n",
            "13608: Log(reward=10.0, actor_loss=None, critic_loss=119583.4666595459, divergence=None, entropy=None)\n",
            "13617: Log(reward=9.0, actor_loss=None, critic_loss=100009.38734436035, divergence=None, entropy=None)\n",
            "13625: Log(reward=8.0, actor_loss=None, critic_loss=76588.98040008545, divergence=None, entropy=None)\n",
            "13635: Log(reward=10.0, actor_loss=None, critic_loss=144125.09729003906, divergence=None, entropy=None)\n",
            "13644: Log(reward=9.0, actor_loss=None, critic_loss=46757.857597351074, divergence=None, entropy=None)\n",
            "13653: Log(reward=9.0, actor_loss=None, critic_loss=92767.89692687988, divergence=None, entropy=None)\n",
            "13663: Log(reward=10.0, actor_loss=None, critic_loss=104476.95553970337, divergence=None, entropy=None)\n",
            "13674: Log(reward=11.0, actor_loss=None, critic_loss=100966.62578582764, divergence=None, entropy=None)\n",
            "13684: Log(reward=10.0, actor_loss=None, critic_loss=97174.97384643555, divergence=None, entropy=None)\n",
            "13694: Log(reward=10.0, actor_loss=None, critic_loss=75935.64162826538, divergence=None, entropy=None)\n",
            "13703: Log(reward=9.0, actor_loss=None, critic_loss=44823.29381561279, divergence=None, entropy=None)\n",
            "13713: Log(reward=10.0, actor_loss=None, critic_loss=56357.50821685791, divergence=None, entropy=None)\n",
            "13723: Log(reward=10.0, actor_loss=None, critic_loss=70311.48345184326, divergence=None, entropy=None)\n",
            "13731: Log(reward=8.0, actor_loss=None, critic_loss=47986.89098358154, divergence=None, entropy=None)\n",
            "13740: Log(reward=9.0, actor_loss=None, critic_loss=53504.08475494385, divergence=None, entropy=None)\n",
            "13749: Log(reward=9.0, actor_loss=None, critic_loss=67656.39660644531, divergence=None, entropy=None)\n",
            "13759: Log(reward=10.0, actor_loss=None, critic_loss=80970.1734161377, divergence=None, entropy=None)\n",
            "13769: Log(reward=10.0, actor_loss=None, critic_loss=65605.25163269043, divergence=None, entropy=None)\n",
            "13779: Log(reward=10.0, actor_loss=None, critic_loss=34041.616970062256, divergence=None, entropy=None)\n",
            "13789: Log(reward=10.0, actor_loss=None, critic_loss=37895.645179748535, divergence=None, entropy=None)\n",
            "13797: Log(reward=8.0, actor_loss=None, critic_loss=32121.714805603027, divergence=None, entropy=None)\n",
            "13806: Log(reward=9.0, actor_loss=None, critic_loss=24526.226570129395, divergence=None, entropy=None)\n",
            "13843: Log(reward=37.0, actor_loss=None, critic_loss=23657.52063369751, divergence=None, entropy=None)\n",
            "13905: Log(reward=62.0, actor_loss=None, critic_loss=38072.66987609863, divergence=None, entropy=None)\n",
            "13944: Log(reward=39.0, actor_loss=None, critic_loss=30978.820114135742, divergence=None, entropy=None)\n",
            "13985: Log(reward=41.0, actor_loss=None, critic_loss=16216.612854003906, divergence=None, entropy=None)\n",
            "14018: Log(reward=33.0, actor_loss=None, critic_loss=10233.101287841797, divergence=None, entropy=None)\n",
            "14047: Log(reward=29.0, actor_loss=None, critic_loss=34420.03527069092, divergence=None, entropy=None)\n",
            "14066: Log(reward=19.0, actor_loss=None, critic_loss=28599.0636138916, divergence=None, entropy=None)\n",
            "14086: Log(reward=20.0, actor_loss=None, critic_loss=12902.887733459473, divergence=None, entropy=None)\n",
            "14108: Log(reward=22.0, actor_loss=None, critic_loss=16847.649932861328, divergence=None, entropy=None)\n",
            "14129: Log(reward=21.0, actor_loss=None, critic_loss=31045.259796142578, divergence=None, entropy=None)\n",
            "14144: Log(reward=15.0, actor_loss=None, critic_loss=12590.442611694336, divergence=None, entropy=None)\n",
            "14163: Log(reward=19.0, actor_loss=None, critic_loss=10025.997253417969, divergence=None, entropy=None)\n",
            "14177: Log(reward=14.0, actor_loss=None, critic_loss=9913.979629516602, divergence=None, entropy=None)\n",
            "14192: Log(reward=15.0, actor_loss=None, critic_loss=12261.396087646484, divergence=None, entropy=None)\n",
            "14209: Log(reward=17.0, actor_loss=None, critic_loss=7698.866027832031, divergence=None, entropy=None)\n",
            "14223: Log(reward=14.0, actor_loss=None, critic_loss=31934.489044189453, divergence=None, entropy=None)\n",
            "14239: Log(reward=16.0, actor_loss=None, critic_loss=12733.7744140625, divergence=None, entropy=None)\n",
            "14254: Log(reward=15.0, actor_loss=None, critic_loss=12742.315673828125, divergence=None, entropy=None)\n",
            "14268: Log(reward=14.0, actor_loss=None, critic_loss=23245.16925048828, divergence=None, entropy=None)\n",
            "14279: Log(reward=11.0, actor_loss=None, critic_loss=20576.444046020508, divergence=None, entropy=None)\n",
            "14292: Log(reward=13.0, actor_loss=None, critic_loss=12577.779922485352, divergence=None, entropy=None)\n",
            "14305: Log(reward=13.0, actor_loss=None, critic_loss=7636.159133911133, divergence=None, entropy=None)\n",
            "14317: Log(reward=12.0, actor_loss=None, critic_loss=8943.240371704102, divergence=None, entropy=None)\n",
            "14331: Log(reward=14.0, actor_loss=None, critic_loss=7121.361251831055, divergence=None, entropy=None)\n",
            "14345: Log(reward=14.0, actor_loss=None, critic_loss=9379.58999633789, divergence=None, entropy=None)\n",
            "14358: Log(reward=13.0, actor_loss=None, critic_loss=22207.34292602539, divergence=None, entropy=None)\n",
            "14372: Log(reward=14.0, actor_loss=None, critic_loss=8132.803604125977, divergence=None, entropy=None)\n",
            "14384: Log(reward=12.0, actor_loss=None, critic_loss=8004.269622802734, divergence=None, entropy=None)\n",
            "14397: Log(reward=13.0, actor_loss=None, critic_loss=6474.103775024414, divergence=None, entropy=None)\n",
            "14412: Log(reward=15.0, actor_loss=None, critic_loss=6116.675491333008, divergence=None, entropy=None)\n",
            "14422: Log(reward=10.0, actor_loss=None, critic_loss=7807.848495483398, divergence=None, entropy=None)\n",
            "14434: Log(reward=12.0, actor_loss=None, critic_loss=23506.676391601562, divergence=None, entropy=None)\n",
            "14445: Log(reward=11.0, actor_loss=None, critic_loss=11547.183029174805, divergence=None, entropy=None)\n",
            "14458: Log(reward=13.0, actor_loss=None, critic_loss=9028.49137878418, divergence=None, entropy=None)\n",
            "14469: Log(reward=11.0, actor_loss=None, critic_loss=9523.642303466797, divergence=None, entropy=None)\n",
            "14481: Log(reward=12.0, actor_loss=None, critic_loss=23893.277893066406, divergence=None, entropy=None)\n",
            "14494: Log(reward=13.0, actor_loss=None, critic_loss=9067.641082763672, divergence=None, entropy=None)\n",
            "14505: Log(reward=11.0, actor_loss=None, critic_loss=11190.937484741211, divergence=None, entropy=None)\n",
            "14517: Log(reward=12.0, actor_loss=None, critic_loss=9326.922103881836, divergence=None, entropy=None)\n",
            "14530: Log(reward=13.0, actor_loss=None, critic_loss=65844.53256225586, divergence=None, entropy=None)\n",
            "14542: Log(reward=12.0, actor_loss=None, critic_loss=14189.933776855469, divergence=None, entropy=None)\n",
            "14556: Log(reward=14.0, actor_loss=None, critic_loss=56996.25296020508, divergence=None, entropy=None)\n",
            "14569: Log(reward=13.0, actor_loss=None, critic_loss=11839.34683227539, divergence=None, entropy=None)\n",
            "14582: Log(reward=13.0, actor_loss=None, critic_loss=12406.819473266602, divergence=None, entropy=None)\n",
            "14594: Log(reward=12.0, actor_loss=None, critic_loss=13060.68196105957, divergence=None, entropy=None)\n",
            "14606: Log(reward=12.0, actor_loss=None, critic_loss=31546.989532470703, divergence=None, entropy=None)\n",
            "14617: Log(reward=11.0, actor_loss=None, critic_loss=17942.307189941406, divergence=None, entropy=None)\n",
            "14627: Log(reward=10.0, actor_loss=None, critic_loss=10581.92366027832, divergence=None, entropy=None)\n",
            "14640: Log(reward=13.0, actor_loss=None, critic_loss=20775.164764404297, divergence=None, entropy=None)\n",
            "14651: Log(reward=11.0, actor_loss=None, critic_loss=11911.073669433594, divergence=None, entropy=None)\n",
            "14665: Log(reward=14.0, actor_loss=None, critic_loss=31260.70880126953, divergence=None, entropy=None)\n",
            "14679: Log(reward=14.0, actor_loss=None, critic_loss=31618.935455322266, divergence=None, entropy=None)\n",
            "14692: Log(reward=13.0, actor_loss=None, critic_loss=17690.450302124023, divergence=None, entropy=None)\n",
            "14706: Log(reward=14.0, actor_loss=None, critic_loss=74154.47489929199, divergence=None, entropy=None)\n",
            "14718: Log(reward=12.0, actor_loss=None, critic_loss=9889.550170898438, divergence=None, entropy=None)\n",
            "14730: Log(reward=12.0, actor_loss=None, critic_loss=17161.88150024414, divergence=None, entropy=None)\n",
            "14743: Log(reward=13.0, actor_loss=None, critic_loss=15504.828002929688, divergence=None, entropy=None)\n",
            "14756: Log(reward=13.0, actor_loss=None, critic_loss=17776.828826904297, divergence=None, entropy=None)\n",
            "14770: Log(reward=14.0, actor_loss=None, critic_loss=25618.80502319336, divergence=None, entropy=None)\n",
            "14785: Log(reward=15.0, actor_loss=None, critic_loss=19525.190643310547, divergence=None, entropy=None)\n",
            "14797: Log(reward=12.0, actor_loss=None, critic_loss=43573.27941894531, divergence=None, entropy=None)\n",
            "14810: Log(reward=13.0, actor_loss=None, critic_loss=41409.60429382324, divergence=None, entropy=None)\n",
            "14824: Log(reward=14.0, actor_loss=None, critic_loss=34476.065338134766, divergence=None, entropy=None)\n",
            "14839: Log(reward=15.0, actor_loss=None, critic_loss=12209.271118164062, divergence=None, entropy=None)\n",
            "14850: Log(reward=11.0, actor_loss=None, critic_loss=12061.596984863281, divergence=None, entropy=None)\n",
            "14864: Log(reward=14.0, actor_loss=None, critic_loss=15957.131103515625, divergence=None, entropy=None)\n",
            "14878: Log(reward=14.0, actor_loss=None, critic_loss=16945.7179107666, divergence=None, entropy=None)\n",
            "14890: Log(reward=12.0, actor_loss=None, critic_loss=30037.791961669922, divergence=None, entropy=None)\n",
            "14905: Log(reward=15.0, actor_loss=None, critic_loss=19777.420867919922, divergence=None, entropy=None)\n",
            "14918: Log(reward=13.0, actor_loss=None, critic_loss=21577.9248046875, divergence=None, entropy=None)\n",
            "14934: Log(reward=16.0, actor_loss=None, critic_loss=17286.643005371094, divergence=None, entropy=None)\n",
            "14950: Log(reward=16.0, actor_loss=None, critic_loss=13016.400527954102, divergence=None, entropy=None)\n",
            "14965: Log(reward=15.0, actor_loss=None, critic_loss=50545.125396728516, divergence=None, entropy=None)\n",
            "15020: Log(reward=55.0, actor_loss=None, critic_loss=12887.060119628906, divergence=None, entropy=None)\n",
            "15034: Log(reward=14.0, actor_loss=None, critic_loss=13745.017028808594, divergence=None, entropy=None)\n",
            "15052: Log(reward=18.0, actor_loss=None, critic_loss=37017.517517089844, divergence=None, entropy=None)\n",
            "15070: Log(reward=18.0, actor_loss=None, critic_loss=13201.157653808594, divergence=None, entropy=None)\n",
            "15125: Log(reward=55.0, actor_loss=None, critic_loss=40026.81799316406, divergence=None, entropy=None)\n",
            "15143: Log(reward=18.0, actor_loss=None, critic_loss=34942.58822631836, divergence=None, entropy=None)\n",
            "15161: Log(reward=18.0, actor_loss=None, critic_loss=57972.36361694336, divergence=None, entropy=None)\n",
            "15211: Log(reward=50.0, actor_loss=None, critic_loss=95043.17739868164, divergence=None, entropy=None)\n",
            "15229: Log(reward=18.0, actor_loss=None, critic_loss=34357.690368652344, divergence=None, entropy=None)\n",
            "15248: Log(reward=19.0, actor_loss=None, critic_loss=22617.705078125, divergence=None, entropy=None)\n",
            "15302: Log(reward=54.0, actor_loss=None, critic_loss=19445.51528930664, divergence=None, entropy=None)\n",
            "15318: Log(reward=16.0, actor_loss=None, critic_loss=25929.85513305664, divergence=None, entropy=None)\n",
            "15372: Log(reward=54.0, actor_loss=None, critic_loss=26750.51025390625, divergence=None, entropy=None)\n",
            "15390: Log(reward=18.0, actor_loss=None, critic_loss=129096.68646240234, divergence=None, entropy=None)\n",
            "15443: Log(reward=53.0, actor_loss=None, critic_loss=24672.37469482422, divergence=None, entropy=None)\n",
            "15493: Log(reward=50.0, actor_loss=None, critic_loss=45032.147552490234, divergence=None, entropy=None)\n",
            "15511: Log(reward=18.0, actor_loss=None, critic_loss=80276.0788269043, divergence=None, entropy=None)\n",
            "15531: Log(reward=20.0, actor_loss=None, critic_loss=15308.970977783203, divergence=None, entropy=None)\n",
            "15546: Log(reward=15.0, actor_loss=None, critic_loss=12980.497802734375, divergence=None, entropy=None)\n",
            "15565: Log(reward=19.0, actor_loss=None, critic_loss=52198.54669189453, divergence=None, entropy=None)\n",
            "15580: Log(reward=15.0, actor_loss=None, critic_loss=30359.392303466797, divergence=None, entropy=None)\n",
            "15639: Log(reward=59.0, actor_loss=None, critic_loss=30137.350158691406, divergence=None, entropy=None)\n",
            "15658: Log(reward=19.0, actor_loss=None, critic_loss=38610.85348510742, divergence=None, entropy=None)\n",
            "15711: Log(reward=53.0, actor_loss=None, critic_loss=31192.651794433594, divergence=None, entropy=None)\n",
            "15768: Log(reward=57.0, actor_loss=None, critic_loss=16421.63690185547, divergence=None, entropy=None)\n",
            "15830: Log(reward=62.0, actor_loss=None, critic_loss=22202.108612060547, divergence=None, entropy=None)\n",
            "15849: Log(reward=19.0, actor_loss=None, critic_loss=41181.781005859375, divergence=None, entropy=None)\n",
            "15869: Log(reward=20.0, actor_loss=None, critic_loss=20833.267364501953, divergence=None, entropy=None)\n",
            "15928: Log(reward=59.0, actor_loss=None, critic_loss=24589.530639648438, divergence=None, entropy=None)\n",
            "15985: Log(reward=57.0, actor_loss=None, critic_loss=17740.154296875, divergence=None, entropy=None)\n",
            "16002: Log(reward=17.0, actor_loss=None, critic_loss=52929.882385253906, divergence=None, entropy=None)\n",
            "16063: Log(reward=61.0, actor_loss=None, critic_loss=22893.817626953125, divergence=None, entropy=None)\n",
            "16122: Log(reward=59.0, actor_loss=None, critic_loss=23908.719787597656, divergence=None, entropy=None)\n",
            "16185: Log(reward=63.0, actor_loss=None, critic_loss=74029.30084228516, divergence=None, entropy=None)\n",
            "16246: Log(reward=61.0, actor_loss=None, critic_loss=204220.81494140625, divergence=None, entropy=None)\n",
            "16262: Log(reward=16.0, actor_loss=None, critic_loss=31276.387145996094, divergence=None, entropy=None)\n",
            "16328: Log(reward=66.0, actor_loss=None, critic_loss=73343.18841552734, divergence=None, entropy=None)\n",
            "16388: Log(reward=60.0, actor_loss=None, critic_loss=29487.544677734375, divergence=None, entropy=None)\n",
            "16461: Log(reward=73.0, actor_loss=None, critic_loss=147417.41986083984, divergence=None, entropy=None)\n",
            "16520: Log(reward=59.0, actor_loss=None, critic_loss=202571.87442016602, divergence=None, entropy=None)\n",
            "16588: Log(reward=68.0, actor_loss=None, critic_loss=27144.163024902344, divergence=None, entropy=None)\n",
            "16651: Log(reward=63.0, actor_loss=None, critic_loss=78884.1802368164, divergence=None, entropy=None)\n",
            "16724: Log(reward=73.0, actor_loss=None, critic_loss=141943.8868408203, divergence=None, entropy=None)\n",
            "16789: Log(reward=65.0, actor_loss=None, critic_loss=34811.544342041016, divergence=None, entropy=None)\n",
            "16858: Log(reward=69.0, actor_loss=None, critic_loss=25372.13818359375, divergence=None, entropy=None)\n",
            "16937: Log(reward=79.0, actor_loss=None, critic_loss=44710.76013183594, divergence=None, entropy=None)\n",
            "17023: Log(reward=86.0, actor_loss=None, critic_loss=18392.53924560547, divergence=None, entropy=None)\n",
            "17099: Log(reward=76.0, actor_loss=None, critic_loss=52537.337173461914, divergence=None, entropy=None)\n",
            "17187: Log(reward=88.0, actor_loss=None, critic_loss=74837.1079711914, divergence=None, entropy=None)\n",
            "17259: Log(reward=72.0, actor_loss=None, critic_loss=34737.08605957031, divergence=None, entropy=None)\n",
            "17353: Log(reward=94.0, actor_loss=None, critic_loss=15983.655456542969, divergence=None, entropy=None)\n",
            "17434: Log(reward=81.0, actor_loss=None, critic_loss=108064.85656738281, divergence=None, entropy=None)\n",
            "17521: Log(reward=87.0, actor_loss=None, critic_loss=31487.95736694336, divergence=None, entropy=None)\n",
            "17608: Log(reward=87.0, actor_loss=None, critic_loss=28830.369018554688, divergence=None, entropy=None)\n",
            "17708: Log(reward=100.0, actor_loss=None, critic_loss=33756.294830322266, divergence=None, entropy=None)\n",
            "17790: Log(reward=82.0, actor_loss=None, critic_loss=24764.613159179688, divergence=None, entropy=None)\n",
            "17871: Log(reward=81.0, actor_loss=None, critic_loss=38950.95947265625, divergence=None, entropy=None)\n",
            "17968: Log(reward=97.0, actor_loss=None, critic_loss=35174.81799316406, divergence=None, entropy=None)\n",
            "18055: Log(reward=87.0, actor_loss=None, critic_loss=64711.15197753906, divergence=None, entropy=None)\n",
            "18152: Log(reward=97.0, actor_loss=None, critic_loss=58906.612060546875, divergence=None, entropy=None)\n",
            "18241: Log(reward=89.0, actor_loss=None, critic_loss=88230.68859863281, divergence=None, entropy=None)\n",
            "18334: Log(reward=93.0, actor_loss=None, critic_loss=63545.28091430664, divergence=None, entropy=None)\n",
            "18435: Log(reward=101.0, actor_loss=None, critic_loss=32291.122192382812, divergence=None, entropy=None)\n",
            "18519: Log(reward=84.0, actor_loss=None, critic_loss=222091.52349853516, divergence=None, entropy=None)\n",
            "18616: Log(reward=97.0, actor_loss=None, critic_loss=83884.57434082031, divergence=None, entropy=None)\n",
            "18709: Log(reward=93.0, actor_loss=None, critic_loss=286102.3182067871, divergence=None, entropy=None)\n",
            "18806: Log(reward=97.0, actor_loss=None, critic_loss=42833.061462402344, divergence=None, entropy=None)\n",
            "18926: Log(reward=120.0, actor_loss=None, critic_loss=17367.518127441406, divergence=None, entropy=None)\n",
            "19065: Log(reward=139.0, actor_loss=None, critic_loss=38732.511779785156, divergence=None, entropy=None)\n",
            "19162: Log(reward=97.0, actor_loss=None, critic_loss=48747.702697753906, divergence=None, entropy=None)\n",
            "19362: Log(reward=200.0, actor_loss=None, critic_loss=271776.2176513672, divergence=None, entropy=None)\n",
            "19474: Log(reward=112.0, actor_loss=None, critic_loss=51392.17852783203, divergence=None, entropy=None)\n",
            "19591: Log(reward=117.0, actor_loss=None, critic_loss=31136.550262451172, divergence=None, entropy=None)\n",
            "19711: Log(reward=120.0, actor_loss=None, critic_loss=180692.23095703125, divergence=None, entropy=None)\n",
            "19873: Log(reward=162.0, actor_loss=None, critic_loss=34287.939697265625, divergence=None, entropy=None)\n",
            "19995: Log(reward=122.0, actor_loss=None, critic_loss=20411.0576171875, divergence=None, entropy=None)\n",
            "20195: Log(reward=200.0, actor_loss=None, critic_loss=63371.32360839844, divergence=None, entropy=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi agent training\n",
        "# Note that this should train in far fewer steps!\n",
        "\n",
        "from pearll.models import Critic, EpsilonGreedyActor, ActorCritic\n",
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import DiscreteQHead\n",
        "from pearll.settings import PopulationSettings\n",
        "\n",
        "\n",
        "encoder = IdentityEncoder()\n",
        "torso = MLP(layer_sizes=[4, 64, 32], activation_fn=T.nn.ReLU)\n",
        "head = DiscreteQHead(input_shape=32, output_shape=2)\n",
        "\n",
        "# Epsilon greedy policy included!\n",
        "actor = EpsilonGreedyActor(\n",
        "    critic_encoder=encoder, critic_torso=torso, critic_head=head\n",
        ")\n",
        "critic = Critic(encoder=encoder, torso=torso, head=head, create_target=True)\n",
        "settings = PopulationSettings(\n",
        "    actor_population_size=5,\n",
        "    critic_population_size=5,\n",
        "    actor_distribution=\"normal\",\n",
        "    critic_distribution=\"normal\"\n",
        ")\n",
        "\n",
        "# Note the need for the vector environment!\n",
        "agent = DQN(\n",
        "  env=gym.vector.make(\"CartPole-v0\", num_envs=5, asynchronous=True),\n",
        "  model = ActorCritic(actor, critic, settings),\n",
        "  logger_settings = LoggerSettings(log_frequency=(\"episode\", 1), verbose=True),\n",
        "  explorer_settings=ExplorerSettings(start_steps=1000),\n",
        ")\n",
        "\n",
        "# max episode reward = 200\n",
        "agent.fit(num_steps=20000, batch_size=32, critic_epochs=16, train_frequency=(\"episode\", 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wddXnLgx1uMk",
        "outputId": "41f49c79-b8d2-4c79-97a5-d37ed2706fa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device cpu\n",
            "31: Log(reward=32.0, actor_loss=None, critic_loss=None, divergence=None, entropy=None)\n",
            "66: Log(reward=35.0, actor_loss=None, critic_loss=622.3754959106445, divergence=None, entropy=None)\n",
            "93: Log(reward=27.0, actor_loss=None, critic_loss=636.6338005065918, divergence=None, entropy=None)\n",
            "170: Log(reward=77.0, actor_loss=None, critic_loss=508.28544425964355, divergence=None, entropy=None)\n",
            "196: Log(reward=26.0, actor_loss=None, critic_loss=299.8132076263428, divergence=None, entropy=None)\n",
            "254: Log(reward=58.0, actor_loss=None, critic_loss=184.33353519439697, divergence=None, entropy=None)\n",
            "290: Log(reward=36.0, actor_loss=None, critic_loss=157.3399338722229, divergence=None, entropy=None)\n",
            "355: Log(reward=65.0, actor_loss=None, critic_loss=168.7603464126587, divergence=None, entropy=None)\n",
            "437: Log(reward=82.0, actor_loss=None, critic_loss=203.29157638549805, divergence=None, entropy=None)\n",
            "456: Log(reward=19.0, actor_loss=None, critic_loss=162.80573916435242, divergence=None, entropy=None)\n",
            "482: Log(reward=26.0, actor_loss=None, critic_loss=190.76061844825745, divergence=None, entropy=None)\n",
            "530: Log(reward=48.0, actor_loss=None, critic_loss=288.4514112472534, divergence=None, entropy=None)\n",
            "575: Log(reward=45.0, actor_loss=None, critic_loss=229.64659976959229, divergence=None, entropy=None)\n",
            "654: Log(reward=79.0, actor_loss=None, critic_loss=319.2956910133362, divergence=None, entropy=None)\n",
            "703: Log(reward=49.0, actor_loss=None, critic_loss=530.5318994522095, divergence=None, entropy=None)\n",
            "731: Log(reward=28.0, actor_loss=None, critic_loss=553.6685266494751, divergence=None, entropy=None)\n",
            "761: Log(reward=30.0, actor_loss=None, critic_loss=476.0600242614746, divergence=None, entropy=None)\n",
            "792: Log(reward=31.0, actor_loss=None, critic_loss=622.1351842880249, divergence=None, entropy=None)\n",
            "860: Log(reward=68.0, actor_loss=None, critic_loss=709.4688835144043, divergence=None, entropy=None)\n",
            "945: Log(reward=85.0, actor_loss=None, critic_loss=693.8723850250244, divergence=None, entropy=None)\n",
            "962: Log(reward=17.0, actor_loss=None, critic_loss=1001.158932685852, divergence=None, entropy=None)\n",
            "996: Log(reward=34.0, actor_loss=None, critic_loss=803.7428321838379, divergence=None, entropy=None)\n",
            "1023: Log(reward=27.0, actor_loss=None, critic_loss=892.6942329406738, divergence=None, entropy=None)\n",
            "1081: Log(reward=58.0, actor_loss=None, critic_loss=915.0581912994385, divergence=None, entropy=None)\n",
            "1101: Log(reward=20.0, actor_loss=None, critic_loss=1133.602897644043, divergence=None, entropy=None)\n",
            "1127: Log(reward=26.0, actor_loss=None, critic_loss=1148.4280490875244, divergence=None, entropy=None)\n",
            "1147: Log(reward=20.0, actor_loss=None, critic_loss=1388.6370162963867, divergence=None, entropy=None)\n",
            "1184: Log(reward=37.0, actor_loss=None, critic_loss=1471.5418167114258, divergence=None, entropy=None)\n",
            "1208: Log(reward=24.0, actor_loss=None, critic_loss=1506.6522254943848, divergence=None, entropy=None)\n",
            "1243: Log(reward=35.0, actor_loss=None, critic_loss=1870.636775970459, divergence=None, entropy=None)\n",
            "1259: Log(reward=16.0, actor_loss=None, critic_loss=2047.1751174926758, divergence=None, entropy=None)\n",
            "1276: Log(reward=17.0, actor_loss=None, critic_loss=2072.0132808685303, divergence=None, entropy=None)\n",
            "1293: Log(reward=17.0, actor_loss=None, critic_loss=1654.4176712036133, divergence=None, entropy=None)\n",
            "1309: Log(reward=16.0, actor_loss=None, critic_loss=2803.100929260254, divergence=None, entropy=None)\n",
            "1350: Log(reward=41.0, actor_loss=None, critic_loss=3108.567138671875, divergence=None, entropy=None)\n",
            "1372: Log(reward=22.0, actor_loss=None, critic_loss=3529.2985229492188, divergence=None, entropy=None)\n",
            "1422: Log(reward=50.0, actor_loss=None, critic_loss=2696.4006729125977, divergence=None, entropy=None)\n",
            "1438: Log(reward=16.0, actor_loss=None, critic_loss=4194.894569396973, divergence=None, entropy=None)\n",
            "1463: Log(reward=25.0, actor_loss=None, critic_loss=3906.3327407836914, divergence=None, entropy=None)\n",
            "1496: Log(reward=33.0, actor_loss=None, critic_loss=4635.981353759766, divergence=None, entropy=None)\n",
            "1515: Log(reward=19.0, actor_loss=None, critic_loss=4739.628997802734, divergence=None, entropy=None)\n",
            "1541: Log(reward=26.0, actor_loss=None, critic_loss=5680.223281860352, divergence=None, entropy=None)\n",
            "1558: Log(reward=17.0, actor_loss=None, critic_loss=7353.539306640625, divergence=None, entropy=None)\n",
            "1578: Log(reward=20.0, actor_loss=None, critic_loss=5502.897125244141, divergence=None, entropy=None)\n",
            "1597: Log(reward=19.0, actor_loss=None, critic_loss=8456.805236816406, divergence=None, entropy=None)\n",
            "1613: Log(reward=16.0, actor_loss=None, critic_loss=6792.138702392578, divergence=None, entropy=None)\n",
            "1629: Log(reward=16.0, actor_loss=None, critic_loss=7639.727149963379, divergence=None, entropy=None)\n",
            "1660: Log(reward=31.0, actor_loss=None, critic_loss=10339.980575561523, divergence=None, entropy=None)\n",
            "1675: Log(reward=15.0, actor_loss=None, critic_loss=10132.318588256836, divergence=None, entropy=None)\n",
            "1691: Log(reward=16.0, actor_loss=None, critic_loss=9985.338195800781, divergence=None, entropy=None)\n",
            "1714: Log(reward=23.0, actor_loss=None, critic_loss=9206.734817504883, divergence=None, entropy=None)\n",
            "1734: Log(reward=20.0, actor_loss=None, critic_loss=9820.144744873047, divergence=None, entropy=None)\n",
            "1756: Log(reward=22.0, actor_loss=None, critic_loss=8613.664665222168, divergence=None, entropy=None)\n",
            "1775: Log(reward=19.0, actor_loss=None, critic_loss=14034.903900146484, divergence=None, entropy=None)\n",
            "1790: Log(reward=15.0, actor_loss=None, critic_loss=10108.404815673828, divergence=None, entropy=None)\n",
            "1804: Log(reward=14.0, actor_loss=None, critic_loss=10851.313949584961, divergence=None, entropy=None)\n",
            "1820: Log(reward=16.0, actor_loss=None, critic_loss=13177.747955322266, divergence=None, entropy=None)\n",
            "1833: Log(reward=13.0, actor_loss=None, critic_loss=13703.33381652832, divergence=None, entropy=None)\n",
            "1846: Log(reward=13.0, actor_loss=None, critic_loss=14778.229949951172, divergence=None, entropy=None)\n",
            "1878: Log(reward=32.0, actor_loss=None, critic_loss=11958.581405639648, divergence=None, entropy=None)\n",
            "1891: Log(reward=13.0, actor_loss=None, critic_loss=14691.846176147461, divergence=None, entropy=None)\n",
            "1905: Log(reward=14.0, actor_loss=None, critic_loss=23022.054595947266, divergence=None, entropy=None)\n",
            "1927: Log(reward=22.0, actor_loss=None, critic_loss=15918.304260253906, divergence=None, entropy=None)\n",
            "1940: Log(reward=13.0, actor_loss=None, critic_loss=17871.83154296875, divergence=None, entropy=None)\n",
            "1957: Log(reward=17.0, actor_loss=None, critic_loss=21105.920806884766, divergence=None, entropy=None)\n",
            "1985: Log(reward=28.0, actor_loss=None, critic_loss=21346.640563964844, divergence=None, entropy=None)\n",
            "1999: Log(reward=14.0, actor_loss=None, critic_loss=18582.81915283203, divergence=None, entropy=None)\n",
            "2012: Log(reward=13.0, actor_loss=None, critic_loss=23031.888244628906, divergence=None, entropy=None)\n",
            "2032: Log(reward=20.0, actor_loss=None, critic_loss=22266.010696411133, divergence=None, entropy=None)\n",
            "2045: Log(reward=13.0, actor_loss=None, critic_loss=25471.438049316406, divergence=None, entropy=None)\n",
            "2057: Log(reward=12.0, actor_loss=None, critic_loss=22292.8505859375, divergence=None, entropy=None)\n",
            "2076: Log(reward=19.0, actor_loss=None, critic_loss=18990.369171142578, divergence=None, entropy=None)\n",
            "2089: Log(reward=13.0, actor_loss=None, critic_loss=27895.944763183594, divergence=None, entropy=None)\n",
            "2100: Log(reward=11.0, actor_loss=None, critic_loss=24992.36376953125, divergence=None, entropy=None)\n",
            "2113: Log(reward=13.0, actor_loss=None, critic_loss=18161.180877685547, divergence=None, entropy=None)\n",
            "2128: Log(reward=15.0, actor_loss=None, critic_loss=24027.481689453125, divergence=None, entropy=None)\n",
            "2140: Log(reward=12.0, actor_loss=None, critic_loss=23960.06689453125, divergence=None, entropy=None)\n",
            "2156: Log(reward=16.0, actor_loss=None, critic_loss=25365.64794921875, divergence=None, entropy=None)\n",
            "2168: Log(reward=12.0, actor_loss=None, critic_loss=22135.199676513672, divergence=None, entropy=None)\n",
            "2179: Log(reward=11.0, actor_loss=None, critic_loss=28053.455291748047, divergence=None, entropy=None)\n",
            "2193: Log(reward=14.0, actor_loss=None, critic_loss=22981.723358154297, divergence=None, entropy=None)\n",
            "2211: Log(reward=18.0, actor_loss=None, critic_loss=19831.01513671875, divergence=None, entropy=None)\n",
            "2224: Log(reward=13.0, actor_loss=None, critic_loss=21890.91928100586, divergence=None, entropy=None)\n",
            "2235: Log(reward=11.0, actor_loss=None, critic_loss=20959.244140625, divergence=None, entropy=None)\n",
            "2247: Log(reward=12.0, actor_loss=None, critic_loss=24902.33901977539, divergence=None, entropy=None)\n",
            "2264: Log(reward=17.0, actor_loss=None, critic_loss=26689.89715576172, divergence=None, entropy=None)\n",
            "2277: Log(reward=13.0, actor_loss=None, critic_loss=18877.92074584961, divergence=None, entropy=None)\n",
            "2289: Log(reward=12.0, actor_loss=None, critic_loss=19459.704818725586, divergence=None, entropy=None)\n",
            "2301: Log(reward=12.0, actor_loss=None, critic_loss=21972.679077148438, divergence=None, entropy=None)\n",
            "2313: Log(reward=12.0, actor_loss=None, critic_loss=24724.25653076172, divergence=None, entropy=None)\n",
            "2323: Log(reward=10.0, actor_loss=None, critic_loss=21704.850395202637, divergence=None, entropy=None)\n",
            "2338: Log(reward=15.0, actor_loss=None, critic_loss=25458.31576538086, divergence=None, entropy=None)\n",
            "2363: Log(reward=25.0, actor_loss=None, critic_loss=18619.555633544922, divergence=None, entropy=None)\n",
            "2380: Log(reward=17.0, actor_loss=None, critic_loss=20157.438934326172, divergence=None, entropy=None)\n",
            "2396: Log(reward=16.0, actor_loss=None, critic_loss=19154.56265258789, divergence=None, entropy=None)\n",
            "2406: Log(reward=10.0, actor_loss=None, critic_loss=21201.09942626953, divergence=None, entropy=None)\n",
            "2418: Log(reward=12.0, actor_loss=None, critic_loss=19778.076141357422, divergence=None, entropy=None)\n",
            "2432: Log(reward=14.0, actor_loss=None, critic_loss=22882.669006347656, divergence=None, entropy=None)\n",
            "2443: Log(reward=11.0, actor_loss=None, critic_loss=16781.212982177734, divergence=None, entropy=None)\n",
            "2456: Log(reward=13.0, actor_loss=None, critic_loss=17651.800170898438, divergence=None, entropy=None)\n",
            "2470: Log(reward=14.0, actor_loss=None, critic_loss=16208.188583374023, divergence=None, entropy=None)\n",
            "2482: Log(reward=12.0, actor_loss=None, critic_loss=16321.536407470703, divergence=None, entropy=None)\n",
            "2495: Log(reward=13.0, actor_loss=None, critic_loss=15894.75048828125, divergence=None, entropy=None)\n",
            "2506: Log(reward=11.0, actor_loss=None, critic_loss=15817.274291992188, divergence=None, entropy=None)\n",
            "2519: Log(reward=13.0, actor_loss=None, critic_loss=12927.838409423828, divergence=None, entropy=None)\n",
            "2536: Log(reward=17.0, actor_loss=None, critic_loss=16580.860931396484, divergence=None, entropy=None)\n",
            "2552: Log(reward=16.0, actor_loss=None, critic_loss=12487.275756835938, divergence=None, entropy=None)\n",
            "2570: Log(reward=18.0, actor_loss=None, critic_loss=14314.790939331055, divergence=None, entropy=None)\n",
            "2586: Log(reward=16.0, actor_loss=None, critic_loss=17135.022583007812, divergence=None, entropy=None)\n",
            "2601: Log(reward=15.0, actor_loss=None, critic_loss=13706.347442626953, divergence=None, entropy=None)\n",
            "2618: Log(reward=17.0, actor_loss=None, critic_loss=15049.654815673828, divergence=None, entropy=None)\n",
            "2635: Log(reward=17.0, actor_loss=None, critic_loss=15542.952331542969, divergence=None, entropy=None)\n",
            "2651: Log(reward=16.0, actor_loss=None, critic_loss=20752.7978515625, divergence=None, entropy=None)\n",
            "2668: Log(reward=17.0, actor_loss=None, critic_loss=18036.29766845703, divergence=None, entropy=None)\n",
            "2691: Log(reward=23.0, actor_loss=None, critic_loss=17999.198120117188, divergence=None, entropy=None)\n",
            "2711: Log(reward=20.0, actor_loss=None, critic_loss=16180.626892089844, divergence=None, entropy=None)\n",
            "2725: Log(reward=14.0, actor_loss=None, critic_loss=19941.757354736328, divergence=None, entropy=None)\n",
            "2743: Log(reward=18.0, actor_loss=None, critic_loss=19656.775115966797, divergence=None, entropy=None)\n",
            "2756: Log(reward=13.0, actor_loss=None, critic_loss=16046.97036743164, divergence=None, entropy=None)\n",
            "2776: Log(reward=20.0, actor_loss=None, critic_loss=26087.062377929688, divergence=None, entropy=None)\n",
            "2800: Log(reward=24.0, actor_loss=None, critic_loss=15928.58187866211, divergence=None, entropy=None)\n",
            "2822: Log(reward=22.0, actor_loss=None, critic_loss=15264.578567504883, divergence=None, entropy=None)\n",
            "2847: Log(reward=25.0, actor_loss=None, critic_loss=15848.330535888672, divergence=None, entropy=None)\n",
            "2870: Log(reward=23.0, actor_loss=None, critic_loss=21565.972137451172, divergence=None, entropy=None)\n",
            "2892: Log(reward=22.0, actor_loss=None, critic_loss=27182.09405517578, divergence=None, entropy=None)\n",
            "2911: Log(reward=19.0, actor_loss=None, critic_loss=21109.782806396484, divergence=None, entropy=None)\n",
            "2943: Log(reward=32.0, actor_loss=None, critic_loss=15961.137557983398, divergence=None, entropy=None)\n",
            "2964: Log(reward=21.0, actor_loss=None, critic_loss=25026.519104003906, divergence=None, entropy=None)\n",
            "2984: Log(reward=20.0, actor_loss=None, critic_loss=32115.55340576172, divergence=None, entropy=None)\n",
            "3006: Log(reward=22.0, actor_loss=None, critic_loss=24690.36834716797, divergence=None, entropy=None)\n",
            "3030: Log(reward=24.0, actor_loss=None, critic_loss=31595.123321533203, divergence=None, entropy=None)\n",
            "3082: Log(reward=52.0, actor_loss=None, critic_loss=30032.432250976562, divergence=None, entropy=None)\n",
            "3110: Log(reward=28.0, actor_loss=None, critic_loss=26205.979110717773, divergence=None, entropy=None)\n",
            "3135: Log(reward=25.0, actor_loss=None, critic_loss=38883.819091796875, divergence=None, entropy=None)\n",
            "3188: Log(reward=53.0, actor_loss=None, critic_loss=34400.690521240234, divergence=None, entropy=None)\n",
            "3214: Log(reward=26.0, actor_loss=None, critic_loss=42338.0048828125, divergence=None, entropy=None)\n",
            "3263: Log(reward=49.0, actor_loss=None, critic_loss=28143.47540283203, divergence=None, entropy=None)\n",
            "3307: Log(reward=44.0, actor_loss=None, critic_loss=41636.004333496094, divergence=None, entropy=None)\n",
            "3349: Log(reward=42.0, actor_loss=None, critic_loss=39435.06396484375, divergence=None, entropy=None)\n",
            "3394: Log(reward=45.0, actor_loss=None, critic_loss=45277.831970214844, divergence=None, entropy=None)\n",
            "3448: Log(reward=54.0, actor_loss=None, critic_loss=35953.888244628906, divergence=None, entropy=None)\n",
            "3496: Log(reward=48.0, actor_loss=None, critic_loss=36853.80471801758, divergence=None, entropy=None)\n",
            "3547: Log(reward=51.0, actor_loss=None, critic_loss=42244.379455566406, divergence=None, entropy=None)\n",
            "3570: Log(reward=23.0, actor_loss=None, critic_loss=43116.42791748047, divergence=None, entropy=None)\n",
            "3598: Log(reward=28.0, actor_loss=None, critic_loss=36322.130920410156, divergence=None, entropy=None)\n",
            "3653: Log(reward=55.0, actor_loss=None, critic_loss=35653.35217285156, divergence=None, entropy=None)\n",
            "3698: Log(reward=45.0, actor_loss=None, critic_loss=36486.06268310547, divergence=None, entropy=None)\n",
            "3722: Log(reward=24.0, actor_loss=None, critic_loss=45037.62322998047, divergence=None, entropy=None)\n",
            "3760: Log(reward=38.0, actor_loss=None, critic_loss=37738.58874511719, divergence=None, entropy=None)\n",
            "3808: Log(reward=48.0, actor_loss=None, critic_loss=40326.36178588867, divergence=None, entropy=None)\n",
            "3870: Log(reward=62.0, actor_loss=None, critic_loss=29341.9375, divergence=None, entropy=None)\n",
            "3930: Log(reward=60.0, actor_loss=None, critic_loss=36018.50061035156, divergence=None, entropy=None)\n",
            "3989: Log(reward=59.0, actor_loss=None, critic_loss=35849.82638549805, divergence=None, entropy=None)\n",
            "4044: Log(reward=55.0, actor_loss=None, critic_loss=36943.633361816406, divergence=None, entropy=None)\n",
            "4101: Log(reward=57.0, actor_loss=None, critic_loss=29443.136322021484, divergence=None, entropy=None)\n",
            "4147: Log(reward=46.0, actor_loss=None, critic_loss=26533.56707763672, divergence=None, entropy=None)\n",
            "4207: Log(reward=60.0, actor_loss=None, critic_loss=32355.47625732422, divergence=None, entropy=None)\n",
            "4281: Log(reward=74.0, actor_loss=None, critic_loss=24352.29019165039, divergence=None, entropy=None)\n",
            "4481: Log(reward=200.0, actor_loss=None, critic_loss=25281.968505859375, divergence=None, entropy=None)\n",
            "4539: Log(reward=58.0, actor_loss=None, critic_loss=25250.367095947266, divergence=None, entropy=None)\n",
            "4606: Log(reward=67.0, actor_loss=None, critic_loss=26473.270263671875, divergence=None, entropy=None)\n",
            "4670: Log(reward=64.0, actor_loss=None, critic_loss=24624.947059631348, divergence=None, entropy=None)\n",
            "4759: Log(reward=89.0, actor_loss=None, critic_loss=21374.21841430664, divergence=None, entropy=None)\n",
            "4842: Log(reward=83.0, actor_loss=None, critic_loss=23501.598114013672, divergence=None, entropy=None)\n",
            "4954: Log(reward=112.0, actor_loss=None, critic_loss=21774.95184326172, divergence=None, entropy=None)\n",
            "5011: Log(reward=57.0, actor_loss=None, critic_loss=17989.318893432617, divergence=None, entropy=None)\n",
            "5090: Log(reward=79.0, actor_loss=None, critic_loss=12696.388549804688, divergence=None, entropy=None)\n",
            "5145: Log(reward=55.0, actor_loss=None, critic_loss=14103.436935424805, divergence=None, entropy=None)\n",
            "5212: Log(reward=67.0, actor_loss=None, critic_loss=18507.26416015625, divergence=None, entropy=None)\n",
            "5308: Log(reward=96.0, actor_loss=None, critic_loss=14237.933158874512, divergence=None, entropy=None)\n",
            "5404: Log(reward=96.0, actor_loss=None, critic_loss=16146.974197387695, divergence=None, entropy=None)\n",
            "5549: Log(reward=145.0, actor_loss=None, critic_loss=15355.054748535156, divergence=None, entropy=None)\n",
            "5699: Log(reward=150.0, actor_loss=None, critic_loss=11990.861846923828, divergence=None, entropy=None)\n",
            "5736: Log(reward=37.0, actor_loss=None, critic_loss=11390.715377807617, divergence=None, entropy=None)\n",
            "5829: Log(reward=93.0, actor_loss=None, critic_loss=10181.508209228516, divergence=None, entropy=None)\n",
            "5944: Log(reward=115.0, actor_loss=None, critic_loss=10552.453140258789, divergence=None, entropy=None)\n",
            "6087: Log(reward=143.0, actor_loss=None, critic_loss=8124.088806152344, divergence=None, entropy=None)\n",
            "6228: Log(reward=141.0, actor_loss=None, critic_loss=8887.43002319336, divergence=None, entropy=None)\n",
            "6358: Log(reward=130.0, actor_loss=None, critic_loss=9497.295532226562, divergence=None, entropy=None)\n",
            "6471: Log(reward=113.0, actor_loss=None, critic_loss=6493.839912414551, divergence=None, entropy=None)\n",
            "6566: Log(reward=95.0, actor_loss=None, critic_loss=6888.958969116211, divergence=None, entropy=None)\n",
            "6706: Log(reward=140.0, actor_loss=None, critic_loss=7829.637420654297, divergence=None, entropy=None)\n",
            "6836: Log(reward=130.0, actor_loss=None, critic_loss=6662.536193847656, divergence=None, entropy=None)\n",
            "6981: Log(reward=145.0, actor_loss=None, critic_loss=6529.87801361084, divergence=None, entropy=None)\n",
            "7072: Log(reward=91.0, actor_loss=None, critic_loss=5698.329887390137, divergence=None, entropy=None)\n",
            "7205: Log(reward=133.0, actor_loss=None, critic_loss=5766.7746658325195, divergence=None, entropy=None)\n",
            "7341: Log(reward=136.0, actor_loss=None, critic_loss=5859.779693603516, divergence=None, entropy=None)\n",
            "7429: Log(reward=88.0, actor_loss=None, critic_loss=4584.625450134277, divergence=None, entropy=None)\n",
            "7629: Log(reward=200.0, actor_loss=None, critic_loss=4457.141227722168, divergence=None, entropy=None)\n",
            "7757: Log(reward=128.0, actor_loss=None, critic_loss=4109.842575073242, divergence=None, entropy=None)\n",
            "7855: Log(reward=98.0, actor_loss=None, critic_loss=4313.466171264648, divergence=None, entropy=None)\n",
            "7914: Log(reward=59.0, actor_loss=None, critic_loss=4433.317451477051, divergence=None, entropy=None)\n",
            "8020: Log(reward=106.0, actor_loss=None, critic_loss=4600.3935470581055, divergence=None, entropy=None)\n",
            "8115: Log(reward=95.0, actor_loss=None, critic_loss=4634.786064147949, divergence=None, entropy=None)\n",
            "8210: Log(reward=95.0, actor_loss=None, critic_loss=5408.840408325195, divergence=None, entropy=None)\n",
            "8297: Log(reward=87.0, actor_loss=None, critic_loss=2798.892623901367, divergence=None, entropy=None)\n",
            "8387: Log(reward=90.0, actor_loss=None, critic_loss=3499.5626525878906, divergence=None, entropy=None)\n",
            "8469: Log(reward=82.0, actor_loss=None, critic_loss=4026.353281021118, divergence=None, entropy=None)\n",
            "8563: Log(reward=94.0, actor_loss=None, critic_loss=3984.88938331604, divergence=None, entropy=None)\n",
            "8663: Log(reward=100.0, actor_loss=None, critic_loss=4057.8473052978516, divergence=None, entropy=None)\n",
            "8769: Log(reward=106.0, actor_loss=None, critic_loss=5513.7550621032715, divergence=None, entropy=None)\n",
            "8914: Log(reward=145.0, actor_loss=None, critic_loss=4570.502582550049, divergence=None, entropy=None)\n",
            "9079: Log(reward=165.0, actor_loss=None, critic_loss=3631.633499145508, divergence=None, entropy=None)\n",
            "9181: Log(reward=102.0, actor_loss=None, critic_loss=3649.398338317871, divergence=None, entropy=None)\n",
            "9381: Log(reward=200.0, actor_loss=None, critic_loss=4327.826301574707, divergence=None, entropy=None)\n",
            "9451: Log(reward=70.0, actor_loss=None, critic_loss=5233.38981628418, divergence=None, entropy=None)\n",
            "9514: Log(reward=63.0, actor_loss=None, critic_loss=5254.498762130737, divergence=None, entropy=None)\n",
            "9576: Log(reward=62.0, actor_loss=None, critic_loss=5182.264465332031, divergence=None, entropy=None)\n",
            "9646: Log(reward=70.0, actor_loss=None, critic_loss=5851.23420715332, divergence=None, entropy=None)\n",
            "9808: Log(reward=162.0, actor_loss=None, critic_loss=5458.966159820557, divergence=None, entropy=None)\n",
            "9881: Log(reward=73.0, actor_loss=None, critic_loss=4543.58145904541, divergence=None, entropy=None)\n",
            "9990: Log(reward=109.0, actor_loss=None, critic_loss=6172.612396240234, divergence=None, entropy=None)\n",
            "10143: Log(reward=153.0, actor_loss=None, critic_loss=5803.7255859375, divergence=None, entropy=None)\n",
            "10298: Log(reward=155.0, actor_loss=None, critic_loss=3799.1379013061523, divergence=None, entropy=None)\n",
            "10443: Log(reward=145.0, actor_loss=None, critic_loss=5345.780914306641, divergence=None, entropy=None)\n",
            "10586: Log(reward=143.0, actor_loss=None, critic_loss=5153.458541870117, divergence=None, entropy=None)\n",
            "10755: Log(reward=169.0, actor_loss=None, critic_loss=3840.453395843506, divergence=None, entropy=None)\n",
            "10955: Log(reward=200.0, actor_loss=None, critic_loss=4273.272201538086, divergence=None, entropy=None)\n",
            "11130: Log(reward=175.0, actor_loss=None, critic_loss=4189.0085525512695, divergence=None, entropy=None)\n",
            "11321: Log(reward=191.0, actor_loss=None, critic_loss=2593.927806854248, divergence=None, entropy=None)\n",
            "11438: Log(reward=117.0, actor_loss=None, critic_loss=4838.650875091553, divergence=None, entropy=None)\n",
            "11503: Log(reward=65.0, actor_loss=None, critic_loss=4252.508792877197, divergence=None, entropy=None)\n",
            "11584: Log(reward=81.0, actor_loss=None, critic_loss=2967.4237937927246, divergence=None, entropy=None)\n",
            "11650: Log(reward=66.0, actor_loss=None, critic_loss=2283.1535987854004, divergence=None, entropy=None)\n",
            "11733: Log(reward=83.0, actor_loss=None, critic_loss=2754.3146934509277, divergence=None, entropy=None)\n",
            "11796: Log(reward=63.0, actor_loss=None, critic_loss=2701.6874351501465, divergence=None, entropy=None)\n",
            "11996: Log(reward=200.0, actor_loss=None, critic_loss=2724.2822189331055, divergence=None, entropy=None)\n",
            "12168: Log(reward=172.0, actor_loss=None, critic_loss=2608.255813598633, divergence=None, entropy=None)\n",
            "12368: Log(reward=200.0, actor_loss=None, critic_loss=3003.7529830932617, divergence=None, entropy=None)\n",
            "12545: Log(reward=177.0, actor_loss=None, critic_loss=1618.8768253326416, divergence=None, entropy=None)\n",
            "12745: Log(reward=200.0, actor_loss=None, critic_loss=2218.2991676330566, divergence=None, entropy=None)\n",
            "12945: Log(reward=200.0, actor_loss=None, critic_loss=2211.1439361572266, divergence=None, entropy=None)\n",
            "13075: Log(reward=130.0, actor_loss=None, critic_loss=1996.4630012512207, divergence=None, entropy=None)\n",
            "13237: Log(reward=162.0, actor_loss=None, critic_loss=1746.0513668060303, divergence=None, entropy=None)\n",
            "13381: Log(reward=144.0, actor_loss=None, critic_loss=1943.9921321868896, divergence=None, entropy=None)\n",
            "13574: Log(reward=193.0, actor_loss=None, critic_loss=1619.7812747955322, divergence=None, entropy=None)\n",
            "13760: Log(reward=186.0, actor_loss=None, critic_loss=1415.7130966186523, divergence=None, entropy=None)\n",
            "13955: Log(reward=195.0, actor_loss=None, critic_loss=1179.4553413391113, divergence=None, entropy=None)\n",
            "14109: Log(reward=154.0, actor_loss=None, critic_loss=1649.2614669799805, divergence=None, entropy=None)\n",
            "14309: Log(reward=200.0, actor_loss=None, critic_loss=2168.424602508545, divergence=None, entropy=None)\n",
            "14509: Log(reward=200.0, actor_loss=None, critic_loss=1328.9776887893677, divergence=None, entropy=None)\n",
            "14683: Log(reward=174.0, actor_loss=None, critic_loss=1508.3334770202637, divergence=None, entropy=None)\n",
            "14883: Log(reward=200.0, actor_loss=None, critic_loss=1079.5032863616943, divergence=None, entropy=None)\n",
            "15083: Log(reward=200.0, actor_loss=None, critic_loss=1025.07666015625, divergence=None, entropy=None)\n",
            "15283: Log(reward=200.0, actor_loss=None, critic_loss=2977.924340248108, divergence=None, entropy=None)\n",
            "15483: Log(reward=200.0, actor_loss=None, critic_loss=2386.4371662139893, divergence=None, entropy=None)\n",
            "15683: Log(reward=200.0, actor_loss=None, critic_loss=1117.2176475524902, divergence=None, entropy=None)\n",
            "15870: Log(reward=187.0, actor_loss=None, critic_loss=1345.564811706543, divergence=None, entropy=None)\n",
            "16070: Log(reward=200.0, actor_loss=None, critic_loss=1411.4589648246765, divergence=None, entropy=None)\n",
            "16224: Log(reward=154.0, actor_loss=None, critic_loss=1814.1333541870117, divergence=None, entropy=None)\n",
            "16417: Log(reward=193.0, actor_loss=None, critic_loss=1942.602014541626, divergence=None, entropy=None)\n",
            "16617: Log(reward=200.0, actor_loss=None, critic_loss=1361.2054996490479, divergence=None, entropy=None)\n",
            "16738: Log(reward=121.0, actor_loss=None, critic_loss=2511.7508573532104, divergence=None, entropy=None)\n",
            "16938: Log(reward=200.0, actor_loss=None, critic_loss=1486.2390155792236, divergence=None, entropy=None)\n",
            "17123: Log(reward=185.0, actor_loss=None, critic_loss=1912.3723888397217, divergence=None, entropy=None)\n",
            "17280: Log(reward=157.0, actor_loss=None, critic_loss=1403.0097846984863, divergence=None, entropy=None)\n",
            "17480: Log(reward=200.0, actor_loss=None, critic_loss=1182.5164706707, divergence=None, entropy=None)\n",
            "17577: Log(reward=97.0, actor_loss=None, critic_loss=1927.612590789795, divergence=None, entropy=None)\n",
            "17716: Log(reward=139.0, actor_loss=None, critic_loss=1733.9860534667969, divergence=None, entropy=None)\n",
            "17916: Log(reward=200.0, actor_loss=None, critic_loss=2316.567356109619, divergence=None, entropy=None)\n",
            "18116: Log(reward=200.0, actor_loss=None, critic_loss=1906.497304201126, divergence=None, entropy=None)\n",
            "18316: Log(reward=200.0, actor_loss=None, critic_loss=1941.7110004425049, divergence=None, entropy=None)\n",
            "18516: Log(reward=200.0, actor_loss=None, critic_loss=1393.8180351257324, divergence=None, entropy=None)\n",
            "18669: Log(reward=153.0, actor_loss=None, critic_loss=792.4165725708008, divergence=None, entropy=None)\n",
            "18869: Log(reward=200.0, actor_loss=None, critic_loss=1221.9830169677734, divergence=None, entropy=None)\n",
            "19035: Log(reward=166.0, actor_loss=None, critic_loss=1567.1308298110962, divergence=None, entropy=None)\n",
            "19235: Log(reward=200.0, actor_loss=None, critic_loss=1472.8519277572632, divergence=None, entropy=None)\n",
            "19365: Log(reward=130.0, actor_loss=None, critic_loss=1070.4821968078613, divergence=None, entropy=None)\n",
            "19495: Log(reward=130.0, actor_loss=None, critic_loss=1086.2811698913574, divergence=None, entropy=None)\n",
            "19681: Log(reward=186.0, actor_loss=None, critic_loss=1862.6808862686157, divergence=None, entropy=None)\n",
            "19841: Log(reward=160.0, actor_loss=None, critic_loss=1073.9515914916992, divergence=None, entropy=None)\n",
            "20041: Log(reward=200.0, actor_loss=None, critic_loss=1318.6236963272095, divergence=None, entropy=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evolutionary Computation\n",
        "\n",
        "Pearl also support evolutionary computation algorithms (EC). There are two instances Pearl supports where this is useful:\n",
        "\n",
        "1. When trying to optimize some black box function where only the function value for different inputs is known. We'll use the sphere function to demonstrate this for simplicity.\n",
        "2. When trying to optimize an agent to interact with some environment, like RL. For this we will once again use gym CartPole as an example.\n",
        "\n",
        "For both cases, we will implement the OpenAI evolutionary strategy. Once again, an implementation of this algorithm is done in Pearl already but we'll implement it again here."
      ],
      "metadata": {
        "id": "R1QEo1zU5Yh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.agents import BaseAgent\n",
        "from pearll.models import ActorCritic\n",
        "from pearll.updaters.evolution import BaseEvolutionUpdater, NoisyGradientAscent\n",
        "from pearll.buffers import BaseBuffer, RolloutBuffer\n",
        "from pearll.explorers import BaseExplorer\n",
        "from pearll.callbacks import BaseCallback\n",
        "from pearll.common.type_aliases import Log\n",
        "# The utils file in common contains some useful data manipulation functions.\n",
        "from pearll.common.utils import filter_rewards\n",
        "from pearll.settings import (\n",
        "    BufferSettings,\n",
        "    ExplorerSettings,\n",
        "    LoggerSettings,\n",
        "    MiscellaneousSettings,\n",
        "    OptimizerSettings,\n",
        "    Settings,\n",
        ")\n",
        "\n",
        "from typing import Type, Optional, List\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch as T\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "class EvolutionaryStrategy(BaseAgent):\n",
        "  # Note that many of the agent parameters are grouped into settings objects for\n",
        "  # a cleaner interface. The __init__() method's goal is to simply intialize all\n",
        "  # the other modules inputted to the class, should generally be quite simple to\n",
        "  # implement.\n",
        "  def __init__(\n",
        "    self,\n",
        "    env: gym.vector.VectorEnv,\n",
        "    model: Optional[ActorCritic] = None,\n",
        "    updater_class: Type[BaseEvolutionUpdater] = NoisyGradientAscent,\n",
        "    learning_rate: float = 1e-3,\n",
        "    buffer_class: Type[BaseBuffer] = RolloutBuffer,\n",
        "    buffer_settings: BufferSettings = BufferSettings(),\n",
        "    action_explorer_class: Type[BaseExplorer] = BaseExplorer,\n",
        "    explorer_settings: ExplorerSettings = ExplorerSettings(start_steps=0),\n",
        "    callbacks: Optional[List[Type[BaseCallback]]] = None,\n",
        "    callback_settings: Optional[List[Settings]] = None,\n",
        "    logger_settings: LoggerSettings = LoggerSettings(),\n",
        "    misc_settings: MiscellaneousSettings = MiscellaneousSettings(),\n",
        "  ) -> None:\n",
        "    # The BaseAgent handles intialization of many of the modules, this can be\n",
        "    # done since the submodules within share the same initialization interface.\n",
        "    super().__init__(\n",
        "      env=env,\n",
        "      model=model,\n",
        "      action_explorer_class=action_explorer_class,\n",
        "      explorer_settings=explorer_settings,\n",
        "      buffer_class=buffer_class,\n",
        "      buffer_settings=buffer_settings,\n",
        "      logger_settings=logger_settings,\n",
        "      callbacks=callbacks,\n",
        "      callback_settings=callback_settings,\n",
        "      misc_settings=misc_settings,\n",
        "    )\n",
        "\n",
        "    self.learning_rate = learning_rate\n",
        "    self.grad_ascent = updater_class(model=self.model)\n",
        "\n",
        "  # Abstract method needs to be implemented. The _fit() method defines the\n",
        "  # actual training update step. This can be quite simple as well though\n",
        "  # thanks to the pre-implemented flexible components that handle much of the \n",
        "  # deep logic.\n",
        "  def _fit(\n",
        "      self, batch_size: int, actor_epochs: int = 1, critic_epochs: int = 1\n",
        "  ) -> Log:\n",
        "    divergences = np.zeros(actor_epochs)\n",
        "    entropies = np.zeros(actor_epochs)\n",
        "\n",
        "    # online learning, get all trajectories collected to train with.\n",
        "    trajectories = self.buffer.all(flatten_env=False)\n",
        "\n",
        "    # process rewards\n",
        "    rewards = trajectories.rewards.squeeze()\n",
        "    rewards = filter_rewards(rewards, trajectories.dones.squeeze())\n",
        "    if rewards.ndim > 1:\n",
        "      rewards = rewards.sum(axis=-1)\n",
        "    scaled_rewards = scale(rewards)\n",
        "\n",
        "    # update steps\n",
        "    optimization_direction = np.dot(self.grad_ascent.normal_dist.T, scaled_rewards) / (\n",
        "        np.mean(self.grad_ascent.std) * self.env.num_envs\n",
        "    )\n",
        "    for i in range(actor_epochs):\n",
        "      log = self.grad_ascent(\n",
        "          learning_rate=self.learning_rate,\n",
        "          optimization_direction=optimization_direction,\n",
        "      )\n",
        "      divergences[i] = log.divergence\n",
        "      entropies[i] = log.entropy\n",
        "    self.buffer.reset()\n",
        "\n",
        "    # Returns a Log object which contains useful training statistics to be\n",
        "    # logged in Tensorboad.\n",
        "    return Log(divergence=divergences.sum(), entropy=entropies.mean())"
      ],
      "metadata": {
        "id": "BFhgUrnL__9T"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "class Sphere(gym.Env):\n",
        "  \"\"\"\n",
        "  Sphere(2) function.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.action_space = gym.spaces.Box(low=-100, high=100, shape=(2,))\n",
        "    self.observation_space = gym.spaces.Discrete(1)\n",
        "\n",
        "  def step(self, action):\n",
        "    return 0, -(action[0] ** 2 + action[1] ** 2), False, {}\n",
        "\n",
        "  def reset(self):\n",
        "    return 0"
      ],
      "metadata": {
        "id": "UIPmn5sL3s2_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models import Dummy, ActorCritic\n",
        "from pearll.settings import PopulationSettings\n",
        "\n",
        "\n",
        "POPULATION_SIZE = 10\n",
        "settings = PopulationSettings(\n",
        "    actor_population_size=POPULATION_SIZE,\n",
        "    actor_distribution=\"normal\"\n",
        ")\n",
        "\n",
        "env = gym.vector.SyncVectorEnv([lambda: Sphere() for _ in range(POPULATION_SIZE)])\n",
        "actor = Dummy(space=env.single_action_space, state=np.array([10, 10]))\n",
        "critic = Dummy(space=env.single_action_space, state=np.array([10, 10]))\n",
        "\n",
        "agent = EvolutionaryStrategy(\n",
        "    env=env,\n",
        "    model=ActorCritic(actor, critic, settings),\n",
        "    learning_rate=1,\n",
        "    logger_settings=LoggerSettings(log_frequency=(\"step\", 1), verbose=True)\n",
        ")\n",
        "\n",
        "# in this case batch_size doesn't actually matter since not used in _fit\n",
        "# max reward = 0\n",
        "# Note that an agent.log file has also been saved which stores all the trajectories run.\n",
        "agent.fit(num_steps=20, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZkBUwPoCfjd",
        "outputId": "88b9bab9-ac40-4f2b-aa90-4e6f793b6fd0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device cpu\n",
            "0: Log(reward=-209.6029469773056, actor_loss=None, critic_loss=None, divergence=None, entropy=None)\n",
            "1: Log(reward=-185.33994487608464, actor_loss=None, critic_loss=None, divergence=0.28313300013542175, entropy=1.4189385175704956)\n",
            "2: Log(reward=-167.7082046027137, actor_loss=None, critic_loss=None, divergence=0.16955488920211792, entropy=1.4189385175704956)\n",
            "3: Log(reward=-142.34828060831654, actor_loss=None, critic_loss=None, divergence=0.15549159049987793, entropy=1.4189385175704956)\n",
            "4: Log(reward=-121.34994700509928, actor_loss=None, critic_loss=None, divergence=0.22404944896697998, entropy=1.4189385175704956)\n",
            "5: Log(reward=-108.65142401244948, actor_loss=None, critic_loss=None, divergence=0.2476799488067627, entropy=1.4189385175704956)\n",
            "6: Log(reward=-84.33634550448184, actor_loss=None, critic_loss=None, divergence=0.13305053114891052, entropy=1.4189385175704956)\n",
            "7: Log(reward=-68.19667188972528, actor_loss=None, critic_loss=None, divergence=0.20013868808746338, entropy=1.4189385175704956)\n",
            "8: Log(reward=-55.339663967596195, actor_loss=None, critic_loss=None, divergence=0.25139719247817993, entropy=1.4189385175704956)\n",
            "9: Log(reward=-42.64832319626104, actor_loss=None, critic_loss=None, divergence=0.41703224182128906, entropy=1.4189385175704956)\n",
            "10: Log(reward=-31.55450589332482, actor_loss=None, critic_loss=None, divergence=0.24269551038742065, entropy=1.4189385175704956)\n",
            "11: Log(reward=-28.09539549029962, actor_loss=None, critic_loss=None, divergence=0.2753028869628906, entropy=1.4189385175704956)\n",
            "12: Log(reward=-13.16767857027852, actor_loss=None, critic_loss=None, divergence=0.5803120136260986, entropy=1.4189385175704956)\n",
            "13: Log(reward=-5.406273402883206, actor_loss=None, critic_loss=None, divergence=0.2042505443096161, entropy=1.4189385175704956)\n",
            "14: Log(reward=-7.218114644084972, actor_loss=None, critic_loss=None, divergence=0.188350647687912, entropy=1.4189385175704956)\n",
            "15: Log(reward=-2.249943556890565, actor_loss=None, critic_loss=None, divergence=0.3396792411804199, entropy=1.4189385175704956)\n",
            "16: Log(reward=-2.1503026464886306, actor_loss=None, critic_loss=None, divergence=0.09999972581863403, entropy=1.4189385175704956)\n",
            "17: Log(reward=-0.9025987731131085, actor_loss=None, critic_loss=None, divergence=0.07624003291130066, entropy=1.4189385175704956)\n",
            "18: Log(reward=-2.5339721739291092, actor_loss=None, critic_loss=None, divergence=0.010383963584899902, entropy=1.4189385175704956)\n",
            "19: Log(reward=-1.882483662181026, actor_loss=None, critic_loss=None, divergence=0.03662371635437012, entropy=1.4189385175704956)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import CategoricalHead\n",
        "from pearll.models import Dummy, ActorCritic, Actor\n",
        "from pearll.settings import PopulationSettings\n",
        "\n",
        "\n",
        "POPULATION_SIZE=20\n",
        "env = gym.vector.make(\"CartPole-v0\", num_envs=POPULATION_SIZE, asynchronous=True)\n",
        "\n",
        "actor = Actor(\n",
        "  encoder=IdentityEncoder(),\n",
        "  torso=MLP(layer_sizes=[4, 20, 10], activation_fn=T.nn.ReLU), \n",
        "  head=CategoricalHead(input_shape=10, action_size=2, activation_fn=T.nn.Tanh)\n",
        ")\n",
        "critic = Dummy(space=env.single_action_space)\n",
        "\n",
        "settings = PopulationSettings(\n",
        "  actor_population_size=POPULATION_SIZE,\n",
        "  actor_distribution=\"normal\",\n",
        "  actor_std=0.01,\n",
        ")\n",
        "\n",
        "agent = EvolutionaryStrategy(\n",
        "  env=env,\n",
        "  model=ActorCritic(actor, critic, settings),\n",
        "  learning_rate=0.001,\n",
        "  logger_settings=LoggerSettings(log_frequency=(\"episode\", 1), verbose=True)\n",
        ")\n",
        "\n",
        "# in this case batch_size doesn't actually matter since not used in _fit\n",
        "# max reward = 200\n",
        "# Note that an agent.log file has also been saved which stores all the trajectories run.\n",
        "agent.fit(num_steps=20000, batch_size=0, train_frequency=(\"episode\", 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfGD2wQ9E7TB",
        "outputId": "32a6ec13-7660-4f5a-f57e-744eeef5ae3e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device cpu\n",
            "50: Log(reward=51.0, actor_loss=None, critic_loss=None, divergence=0.0, entropy=-3.1862316131591797)\n",
            "102: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.4583632946014404, entropy=-3.1862316131591797)\n",
            "159: Log(reward=57.0, actor_loss=None, critic_loss=None, divergence=2.850159168243408, entropy=-3.1862316131591797)\n",
            "226: Log(reward=67.0, actor_loss=None, critic_loss=None, divergence=2.5895583629608154, entropy=-3.1862316131591797)\n",
            "284: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.703207492828369, entropy=-3.1862316131591797)\n",
            "337: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.8306572437286377, entropy=-3.1862316131591797)\n",
            "389: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.676666021347046, entropy=-3.1862316131591797)\n",
            "432: Log(reward=43.0, actor_loss=None, critic_loss=None, divergence=2.6999988555908203, entropy=-3.1862316131591797)\n",
            "480: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=3.0383827686309814, entropy=-3.1862316131591797)\n",
            "518: Log(reward=38.0, actor_loss=None, critic_loss=None, divergence=2.3986594676971436, entropy=-3.1862316131591797)\n",
            "557: Log(reward=39.0, actor_loss=None, critic_loss=None, divergence=2.6093783378601074, entropy=-3.1862316131591797)\n",
            "595: Log(reward=38.0, actor_loss=None, critic_loss=None, divergence=2.6947457790374756, entropy=-3.1862316131591797)\n",
            "649: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.6504929065704346, entropy=-3.1862316131591797)\n",
            "701: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.488332748413086, entropy=-3.1862316131591797)\n",
            "749: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=2.5373334884643555, entropy=-3.1862316131591797)\n",
            "786: Log(reward=37.0, actor_loss=None, critic_loss=None, divergence=2.581646203994751, entropy=-3.1862316131591797)\n",
            "842: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.683417558670044, entropy=-3.1862316131591797)\n",
            "895: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.392101287841797, entropy=-3.1862316131591797)\n",
            "949: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.545389175415039, entropy=-3.1862316131591797)\n",
            "1020: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.6750998497009277, entropy=-3.1862316131591797)\n",
            "1071: Log(reward=51.0, actor_loss=None, critic_loss=None, divergence=2.8013150691986084, entropy=-3.1862316131591797)\n",
            "1137: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.2507200241088867, entropy=-3.1862316131591797)\n",
            "1179: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.758117437362671, entropy=-3.1862316131591797)\n",
            "1256: Log(reward=77.0, actor_loss=None, critic_loss=None, divergence=2.935422658920288, entropy=-3.1862316131591797)\n",
            "1318: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.6329128742218018, entropy=-3.1862316131591797)\n",
            "1361: Log(reward=43.0, actor_loss=None, critic_loss=None, divergence=2.4266397953033447, entropy=-3.1862316131591797)\n",
            "1452: Log(reward=91.0, actor_loss=None, critic_loss=None, divergence=2.373307466506958, entropy=-3.1862316131591797)\n",
            "1513: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.132150411605835, entropy=-3.1862316131591797)\n",
            "1558: Log(reward=45.0, actor_loss=None, critic_loss=None, divergence=2.4283387660980225, entropy=-3.1862316131591797)\n",
            "1621: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.532494068145752, entropy=-3.1862316131591797)\n",
            "1658: Log(reward=37.0, actor_loss=None, critic_loss=None, divergence=2.695465564727783, entropy=-3.1862316131591797)\n",
            "1697: Log(reward=39.0, actor_loss=None, critic_loss=None, divergence=2.5083060264587402, entropy=-3.1862316131591797)\n",
            "1732: Log(reward=35.0, actor_loss=None, critic_loss=None, divergence=2.7374207973480225, entropy=-3.1862316131591797)\n",
            "1814: Log(reward=82.0, actor_loss=None, critic_loss=None, divergence=2.4828169345855713, entropy=-3.1862316131591797)\n",
            "1873: Log(reward=59.0, actor_loss=None, critic_loss=None, divergence=2.392620086669922, entropy=-3.1862316131591797)\n",
            "1909: Log(reward=36.0, actor_loss=None, critic_loss=None, divergence=2.896754026412964, entropy=-3.1862316131591797)\n",
            "1940: Log(reward=31.0, actor_loss=None, critic_loss=None, divergence=2.2508509159088135, entropy=-3.1862316131591797)\n",
            "1971: Log(reward=31.0, actor_loss=None, critic_loss=None, divergence=2.644336700439453, entropy=-3.1862316131591797)\n",
            "2040: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.2659666538238525, entropy=-3.1862316131591797)\n",
            "2102: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.2312846183776855, entropy=-3.1862316131591797)\n",
            "2147: Log(reward=45.0, actor_loss=None, critic_loss=None, divergence=2.1966781616210938, entropy=-3.1862316131591797)\n",
            "2188: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.6989614963531494, entropy=-3.1862316131591797)\n",
            "2238: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=2.4401819705963135, entropy=-3.1862316131591797)\n",
            "2292: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.2899279594421387, entropy=-3.1862316131591797)\n",
            "2359: Log(reward=67.0, actor_loss=None, critic_loss=None, divergence=2.5134077072143555, entropy=-3.1862316131591797)\n",
            "2407: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=2.3089442253112793, entropy=-3.1862316131591797)\n",
            "2449: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.2738049030303955, entropy=-3.1862316131591797)\n",
            "2485: Log(reward=36.0, actor_loss=None, critic_loss=None, divergence=2.586029291152954, entropy=-3.1862316131591797)\n",
            "2533: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=2.6710660457611084, entropy=-3.1862316131591797)\n",
            "2569: Log(reward=36.0, actor_loss=None, critic_loss=None, divergence=2.5539040565490723, entropy=-3.1862316131591797)\n",
            "2614: Log(reward=45.0, actor_loss=None, critic_loss=None, divergence=2.632918357849121, entropy=-3.1862316131591797)\n",
            "2653: Log(reward=39.0, actor_loss=None, critic_loss=None, divergence=2.499258279800415, entropy=-3.1862316131591797)\n",
            "2686: Log(reward=33.0, actor_loss=None, critic_loss=None, divergence=2.518017053604126, entropy=-3.1862316131591797)\n",
            "2741: Log(reward=55.0, actor_loss=None, critic_loss=None, divergence=2.6075000762939453, entropy=-3.1862316131591797)\n",
            "2796: Log(reward=55.0, actor_loss=None, critic_loss=None, divergence=2.716893434524536, entropy=-3.1862316131591797)\n",
            "2848: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.5308029651641846, entropy=-3.1862316131591797)\n",
            "2917: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.548774242401123, entropy=-3.1862316131591797)\n",
            "2959: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.315479040145874, entropy=-3.1862316131591797)\n",
            "3056: Log(reward=97.0, actor_loss=None, critic_loss=None, divergence=2.5483710765838623, entropy=-3.1862316131591797)\n",
            "3127: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.847860097885132, entropy=-3.1862316131591797)\n",
            "3164: Log(reward=37.0, actor_loss=None, critic_loss=None, divergence=2.4585680961608887, entropy=-3.1862316131591797)\n",
            "3212: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=2.30899715423584, entropy=-3.1862316131591797)\n",
            "3255: Log(reward=43.0, actor_loss=None, critic_loss=None, divergence=2.661466598510742, entropy=-3.1862316131591797)\n",
            "3321: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.5683767795562744, entropy=-3.1862316131591797)\n",
            "3382: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.0113062858581543, entropy=-3.1862316131591797)\n",
            "3415: Log(reward=33.0, actor_loss=None, critic_loss=None, divergence=2.562843084335327, entropy=-3.1862316131591797)\n",
            "3477: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.7441651821136475, entropy=-3.1862316131591797)\n",
            "3514: Log(reward=37.0, actor_loss=None, critic_loss=None, divergence=2.6396279335021973, entropy=-3.1862316131591797)\n",
            "3574: Log(reward=60.0, actor_loss=None, critic_loss=None, divergence=2.337193250656128, entropy=-3.1862316131591797)\n",
            "3633: Log(reward=59.0, actor_loss=None, critic_loss=None, divergence=2.719205617904663, entropy=-3.1862316131591797)\n",
            "3695: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.69511079788208, entropy=-3.1862316131591797)\n",
            "3764: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.8003580570220947, entropy=-3.1862316131591797)\n",
            "3836: Log(reward=72.0, actor_loss=None, critic_loss=None, divergence=2.5488898754119873, entropy=-3.1862316131591797)\n",
            "3907: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.5109314918518066, entropy=-3.1862316131591797)\n",
            "3961: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.583789348602295, entropy=-3.1862316131591797)\n",
            "4027: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.4460980892181396, entropy=-3.1862316131591797)\n",
            "4087: Log(reward=60.0, actor_loss=None, critic_loss=None, divergence=2.5645382404327393, entropy=-3.1862316131591797)\n",
            "4120: Log(reward=33.0, actor_loss=None, critic_loss=None, divergence=2.3614513874053955, entropy=-3.1862316131591797)\n",
            "4189: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.3333663940429688, entropy=-3.1862316131591797)\n",
            "4233: Log(reward=44.0, actor_loss=None, critic_loss=None, divergence=2.4022865295410156, entropy=-3.1862316131591797)\n",
            "4305: Log(reward=72.0, actor_loss=None, critic_loss=None, divergence=2.5415289402008057, entropy=-3.1862316131591797)\n",
            "4415: Log(reward=110.0, actor_loss=None, critic_loss=None, divergence=2.5245747566223145, entropy=-3.1862316131591797)\n",
            "4490: Log(reward=75.0, actor_loss=None, critic_loss=None, divergence=2.552307605743408, entropy=-3.1862316131591797)\n",
            "4574: Log(reward=84.0, actor_loss=None, critic_loss=None, divergence=2.417632579803467, entropy=-3.1862316131591797)\n",
            "4636: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.186643600463867, entropy=-3.1862316131591797)\n",
            "4705: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.5782864093780518, entropy=-3.1862316131591797)\n",
            "4773: Log(reward=68.0, actor_loss=None, critic_loss=None, divergence=2.922168731689453, entropy=-3.1862316131591797)\n",
            "4824: Log(reward=51.0, actor_loss=None, critic_loss=None, divergence=2.5529115200042725, entropy=-3.1862316131591797)\n",
            "4872: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=2.598745584487915, entropy=-3.1862316131591797)\n",
            "4955: Log(reward=83.0, actor_loss=None, critic_loss=None, divergence=2.7191531658172607, entropy=-3.1862316131591797)\n",
            "4997: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.4480929374694824, entropy=-3.1862316131591797)\n",
            "5051: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.1018245220184326, entropy=-3.1862316131591797)\n",
            "5090: Log(reward=39.0, actor_loss=None, critic_loss=None, divergence=2.680269956588745, entropy=-3.1862316131591797)\n",
            "5130: Log(reward=40.0, actor_loss=None, critic_loss=None, divergence=2.2172675132751465, entropy=-3.1862316131591797)\n",
            "5193: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.579883575439453, entropy=-3.1862316131591797)\n",
            "5238: Log(reward=45.0, actor_loss=None, critic_loss=None, divergence=2.709834098815918, entropy=-3.1862316131591797)\n",
            "5273: Log(reward=35.0, actor_loss=None, critic_loss=None, divergence=2.454049587249756, entropy=-3.1862316131591797)\n",
            "5311: Log(reward=38.0, actor_loss=None, critic_loss=None, divergence=2.781459093093872, entropy=-3.1862316131591797)\n",
            "5396: Log(reward=85.0, actor_loss=None, critic_loss=None, divergence=2.3443872928619385, entropy=-3.1862316131591797)\n",
            "5466: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.4813454151153564, entropy=-3.1862316131591797)\n",
            "5530: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.836906909942627, entropy=-3.1862316131591797)\n",
            "5622: Log(reward=92.0, actor_loss=None, critic_loss=None, divergence=2.5368735790252686, entropy=-3.1862316131591797)\n",
            "5707: Log(reward=85.0, actor_loss=None, critic_loss=None, divergence=2.6019153594970703, entropy=-3.1862316131591797)\n",
            "5780: Log(reward=73.0, actor_loss=None, critic_loss=None, divergence=2.7654566764831543, entropy=-3.1862316131591797)\n",
            "5838: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.399104356765747, entropy=-3.1862316131591797)\n",
            "5881: Log(reward=43.0, actor_loss=None, critic_loss=None, divergence=2.361621856689453, entropy=-3.1862316131591797)\n",
            "5923: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.7586677074432373, entropy=-3.1862316131591797)\n",
            "5987: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.2976160049438477, entropy=-3.1862316131591797)\n",
            "6063: Log(reward=76.0, actor_loss=None, critic_loss=None, divergence=2.4452173709869385, entropy=-3.1862316131591797)\n",
            "6104: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.6036524772644043, entropy=-3.1862316131591797)\n",
            "6145: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.401470422744751, entropy=-3.1862316131591797)\n",
            "6203: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.5797688961029053, entropy=-3.1862316131591797)\n",
            "6270: Log(reward=67.0, actor_loss=None, critic_loss=None, divergence=2.1590981483459473, entropy=-3.1862316131591797)\n",
            "6326: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.3988444805145264, entropy=-3.1862316131591797)\n",
            "6380: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.5235495567321777, entropy=-3.1862316131591797)\n",
            "6429: Log(reward=49.0, actor_loss=None, critic_loss=None, divergence=2.476614236831665, entropy=-3.1862316131591797)\n",
            "6486: Log(reward=57.0, actor_loss=None, critic_loss=None, divergence=2.225458860397339, entropy=-3.1862316131591797)\n",
            "6531: Log(reward=45.0, actor_loss=None, critic_loss=None, divergence=2.1482045650482178, entropy=-3.1862316131591797)\n",
            "6569: Log(reward=38.0, actor_loss=None, critic_loss=None, divergence=2.218034029006958, entropy=-3.1862316131591797)\n",
            "6619: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=2.6290454864501953, entropy=-3.1862316131591797)\n",
            "6668: Log(reward=49.0, actor_loss=None, critic_loss=None, divergence=2.3531806468963623, entropy=-3.1862316131591797)\n",
            "6730: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.209273099899292, entropy=-3.1862316131591797)\n",
            "6775: Log(reward=45.0, actor_loss=None, critic_loss=None, divergence=2.273195743560791, entropy=-3.1862316131591797)\n",
            "6866: Log(reward=91.0, actor_loss=None, critic_loss=None, divergence=2.5083742141723633, entropy=-3.1862316131591797)\n",
            "6907: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.710387945175171, entropy=-3.1862316131591797)\n",
            "6948: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.6481873989105225, entropy=-3.1862316131591797)\n",
            "6977: Log(reward=29.0, actor_loss=None, critic_loss=None, divergence=2.779165506362915, entropy=-3.1862316131591797)\n",
            "7041: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.816615343093872, entropy=-3.1862316131591797)\n",
            "7122: Log(reward=81.0, actor_loss=None, critic_loss=None, divergence=2.536376714706421, entropy=-3.1862316131591797)\n",
            "7180: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.421243906021118, entropy=-3.1862316131591797)\n",
            "7230: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=3.0374250411987305, entropy=-3.1862316131591797)\n",
            "7280: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=2.214484930038452, entropy=-3.1862316131591797)\n",
            "7327: Log(reward=47.0, actor_loss=None, critic_loss=None, divergence=2.4987404346466064, entropy=-3.1862316131591797)\n",
            "7419: Log(reward=92.0, actor_loss=None, critic_loss=None, divergence=2.3142189979553223, entropy=-3.1862316131591797)\n",
            "7476: Log(reward=57.0, actor_loss=None, critic_loss=None, divergence=2.3440606594085693, entropy=-3.1862316131591797)\n",
            "7536: Log(reward=60.0, actor_loss=None, critic_loss=None, divergence=2.4004745483398438, entropy=-3.1862316131591797)\n",
            "7599: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.433734178543091, entropy=-3.1862316131591797)\n",
            "7649: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=2.3827006816864014, entropy=-3.1862316131591797)\n",
            "7698: Log(reward=49.0, actor_loss=None, critic_loss=None, divergence=2.345841646194458, entropy=-3.1862316131591797)\n",
            "7739: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.328824043273926, entropy=-3.1862316131591797)\n",
            "7826: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.227066993713379, entropy=-3.1862316131591797)\n",
            "7896: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.8182554244995117, entropy=-3.1862316131591797)\n",
            "7962: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.6250758171081543, entropy=-3.1862316131591797)\n",
            "8015: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.9493207931518555, entropy=-3.1862316131591797)\n",
            "8088: Log(reward=73.0, actor_loss=None, critic_loss=None, divergence=2.6474099159240723, entropy=-3.1862316131591797)\n",
            "8151: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.3318161964416504, entropy=-3.1862316131591797)\n",
            "8209: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.2010512351989746, entropy=-3.1862316131591797)\n",
            "8280: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.6060519218444824, entropy=-3.1862316131591797)\n",
            "8353: Log(reward=73.0, actor_loss=None, critic_loss=None, divergence=2.622941255569458, entropy=-3.1862316131591797)\n",
            "8407: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.5897397994995117, entropy=-3.1862316131591797)\n",
            "8472: Log(reward=65.0, actor_loss=None, critic_loss=None, divergence=2.635181427001953, entropy=-3.1862316131591797)\n",
            "8559: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.3126232624053955, entropy=-3.1862316131591797)\n",
            "8663: Log(reward=104.0, actor_loss=None, critic_loss=None, divergence=2.642726182937622, entropy=-3.1862316131591797)\n",
            "8721: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.6201930046081543, entropy=-3.1862316131591797)\n",
            "8821: Log(reward=100.0, actor_loss=None, critic_loss=None, divergence=2.5957798957824707, entropy=-3.1862316131591797)\n",
            "8877: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.297694444656372, entropy=-3.1862316131591797)\n",
            "8987: Log(reward=110.0, actor_loss=None, critic_loss=None, divergence=2.551335573196411, entropy=-3.1862316131591797)\n",
            "9071: Log(reward=84.0, actor_loss=None, critic_loss=None, divergence=2.4027671813964844, entropy=-3.1862316131591797)\n",
            "9123: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.939148426055908, entropy=-3.1862316131591797)\n",
            "9217: Log(reward=94.0, actor_loss=None, critic_loss=None, divergence=2.0986499786376953, entropy=-3.1862316131591797)\n",
            "9269: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.687746286392212, entropy=-3.1862316131591797)\n",
            "9362: Log(reward=93.0, actor_loss=None, critic_loss=None, divergence=2.1773436069488525, entropy=-3.1862316131591797)\n",
            "9431: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.467177391052246, entropy=-3.1862316131591797)\n",
            "9515: Log(reward=84.0, actor_loss=None, critic_loss=None, divergence=2.233222246170044, entropy=-3.1862316131591797)\n",
            "9617: Log(reward=102.0, actor_loss=None, critic_loss=None, divergence=2.3585848808288574, entropy=-3.1862316131591797)\n",
            "9666: Log(reward=49.0, actor_loss=None, critic_loss=None, divergence=2.3423116207122803, entropy=-3.1862316131591797)\n",
            "9729: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.406543493270874, entropy=-3.1862316131591797)\n",
            "9768: Log(reward=39.0, actor_loss=None, critic_loss=None, divergence=2.4836344718933105, entropy=-3.1862316131591797)\n",
            "9823: Log(reward=55.0, actor_loss=None, critic_loss=None, divergence=2.397433280944824, entropy=-3.1862316131591797)\n",
            "9880: Log(reward=57.0, actor_loss=None, critic_loss=None, divergence=2.488138437271118, entropy=-3.1862316131591797)\n",
            "9934: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.550137758255005, entropy=-3.1862316131591797)\n",
            "9996: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.4877724647521973, entropy=-3.1862316131591797)\n",
            "10063: Log(reward=67.0, actor_loss=None, critic_loss=None, divergence=2.3308913707733154, entropy=-3.1862316131591797)\n",
            "10103: Log(reward=40.0, actor_loss=None, critic_loss=None, divergence=2.9039647579193115, entropy=-3.1862316131591797)\n",
            "10169: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.444652795791626, entropy=-3.1862316131591797)\n",
            "10242: Log(reward=73.0, actor_loss=None, critic_loss=None, divergence=2.547562599182129, entropy=-3.1862316131591797)\n",
            "10296: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.4160044193267822, entropy=-3.1862316131591797)\n",
            "10384: Log(reward=88.0, actor_loss=None, critic_loss=None, divergence=3.1587259769439697, entropy=-3.1862316131591797)\n",
            "10440: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.4573357105255127, entropy=-3.1862316131591797)\n",
            "10523: Log(reward=83.0, actor_loss=None, critic_loss=None, divergence=2.3804469108581543, entropy=-3.1862316131591797)\n",
            "10592: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.553964376449585, entropy=-3.1862316131591797)\n",
            "10661: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.397597074508667, entropy=-3.1862316131591797)\n",
            "10689: Log(reward=28.0, actor_loss=None, critic_loss=None, divergence=2.5121660232543945, entropy=-3.1862316131591797)\n",
            "10745: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.734175205230713, entropy=-3.1862316131591797)\n",
            "10797: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.5493569374084473, entropy=-3.1862316131591797)\n",
            "10858: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.490323066711426, entropy=-3.1862316131591797)\n",
            "10930: Log(reward=72.0, actor_loss=None, critic_loss=None, divergence=2.435425043106079, entropy=-3.1862316131591797)\n",
            "10978: Log(reward=48.0, actor_loss=None, critic_loss=None, divergence=2.5023348331451416, entropy=-3.1862316131591797)\n",
            "11068: Log(reward=90.0, actor_loss=None, critic_loss=None, divergence=2.192511558532715, entropy=-3.1862316131591797)\n",
            "11109: Log(reward=41.0, actor_loss=None, critic_loss=None, divergence=2.601494073867798, entropy=-3.1862316131591797)\n",
            "11180: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.6255433559417725, entropy=-3.1862316131591797)\n",
            "11220: Log(reward=40.0, actor_loss=None, critic_loss=None, divergence=2.654244899749756, entropy=-3.1862316131591797)\n",
            "11290: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.7693138122558594, entropy=-3.1862316131591797)\n",
            "11374: Log(reward=84.0, actor_loss=None, critic_loss=None, divergence=2.44248628616333, entropy=-3.1862316131591797)\n",
            "11423: Log(reward=49.0, actor_loss=None, critic_loss=None, divergence=3.0613765716552734, entropy=-3.1862316131591797)\n",
            "11465: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.5332212448120117, entropy=-3.1862316131591797)\n",
            "11507: Log(reward=42.0, actor_loss=None, critic_loss=None, divergence=2.1839559078216553, entropy=-3.1862316131591797)\n",
            "11558: Log(reward=51.0, actor_loss=None, critic_loss=None, divergence=2.352344512939453, entropy=-3.1862316131591797)\n",
            "11612: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.706777334213257, entropy=-3.1862316131591797)\n",
            "11699: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.4763572216033936, entropy=-3.1862316131591797)\n",
            "11755: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.5790421962738037, entropy=-3.1862316131591797)\n",
            "11814: Log(reward=59.0, actor_loss=None, critic_loss=None, divergence=2.285951852798462, entropy=-3.1862316131591797)\n",
            "11878: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.365473985671997, entropy=-3.1862316131591797)\n",
            "11941: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.5030336380004883, entropy=-3.1862316131591797)\n",
            "11992: Log(reward=51.0, actor_loss=None, critic_loss=None, divergence=2.841524124145508, entropy=-3.1862316131591797)\n",
            "12058: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.3781704902648926, entropy=-3.1862316131591797)\n",
            "12108: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=2.7085494995117188, entropy=-3.1862316131591797)\n",
            "12162: Log(reward=54.0, actor_loss=None, critic_loss=None, divergence=2.271479368209839, entropy=-3.1862316131591797)\n",
            "12226: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.4948930740356445, entropy=-3.1862316131591797)\n",
            "12302: Log(reward=76.0, actor_loss=None, critic_loss=None, divergence=2.2379868030548096, entropy=-3.1862316131591797)\n",
            "12367: Log(reward=65.0, actor_loss=None, critic_loss=None, divergence=2.3544604778289795, entropy=-3.1862316131591797)\n",
            "12462: Log(reward=95.0, actor_loss=None, critic_loss=None, divergence=2.567460775375366, entropy=-3.1862316131591797)\n",
            "12587: Log(reward=125.0, actor_loss=None, critic_loss=None, divergence=2.4646739959716797, entropy=-3.1862316131591797)\n",
            "12647: Log(reward=60.0, actor_loss=None, critic_loss=None, divergence=2.5253500938415527, entropy=-3.1862316131591797)\n",
            "12729: Log(reward=82.0, actor_loss=None, critic_loss=None, divergence=2.713745594024658, entropy=-3.1862316131591797)\n",
            "12858: Log(reward=129.0, actor_loss=None, critic_loss=None, divergence=2.5744740962982178, entropy=-3.1862316131591797)\n",
            "12935: Log(reward=77.0, actor_loss=None, critic_loss=None, divergence=2.4212963581085205, entropy=-3.1862316131591797)\n",
            "13020: Log(reward=85.0, actor_loss=None, critic_loss=None, divergence=2.2312827110290527, entropy=-3.1862316131591797)\n",
            "13148: Log(reward=128.0, actor_loss=None, critic_loss=None, divergence=2.342480421066284, entropy=-3.1862316131591797)\n",
            "13251: Log(reward=103.0, actor_loss=None, critic_loss=None, divergence=2.7502830028533936, entropy=-3.1862316131591797)\n",
            "13322: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.1282973289489746, entropy=-3.1862316131591797)\n",
            "13430: Log(reward=108.0, actor_loss=None, critic_loss=None, divergence=2.656644105911255, entropy=-3.1862316131591797)\n",
            "13488: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.362016201019287, entropy=-3.1862316131591797)\n",
            "13571: Log(reward=83.0, actor_loss=None, critic_loss=None, divergence=2.3691213130950928, entropy=-3.1862316131591797)\n",
            "13621: Log(reward=50.0, actor_loss=None, critic_loss=None, divergence=2.54266619682312, entropy=-3.1862316131591797)\n",
            "13710: Log(reward=89.0, actor_loss=None, critic_loss=None, divergence=2.5745911598205566, entropy=-3.1862316131591797)\n",
            "13779: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.3752100467681885, entropy=-3.1862316131591797)\n",
            "13848: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.7834362983703613, entropy=-3.1862316131591797)\n",
            "13900: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.5507071018218994, entropy=-3.1862316131591797)\n",
            "13969: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.220808744430542, entropy=-3.1862316131591797)\n",
            "14038: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.6419453620910645, entropy=-3.1862316131591797)\n",
            "14089: Log(reward=51.0, actor_loss=None, critic_loss=None, divergence=2.4379923343658447, entropy=-3.1862316131591797)\n",
            "14159: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.456575632095337, entropy=-3.1862316131591797)\n",
            "14242: Log(reward=83.0, actor_loss=None, critic_loss=None, divergence=2.693563461303711, entropy=-3.1862316131591797)\n",
            "14297: Log(reward=55.0, actor_loss=None, critic_loss=None, divergence=2.5189249515533447, entropy=-3.1862316131591797)\n",
            "14371: Log(reward=74.0, actor_loss=None, critic_loss=None, divergence=2.5320615768432617, entropy=-3.1862316131591797)\n",
            "14440: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.30531907081604, entropy=-3.1862316131591797)\n",
            "14492: Log(reward=52.0, actor_loss=None, critic_loss=None, divergence=2.544313907623291, entropy=-3.1862316131591797)\n",
            "14571: Log(reward=79.0, actor_loss=None, critic_loss=None, divergence=2.3101646900177, entropy=-3.1862316131591797)\n",
            "14647: Log(reward=76.0, actor_loss=None, critic_loss=None, divergence=2.32077956199646, entropy=-3.1862316131591797)\n",
            "14727: Log(reward=80.0, actor_loss=None, critic_loss=None, divergence=2.517627000808716, entropy=-3.1862316131591797)\n",
            "14814: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.3422281742095947, entropy=-3.1862316131591797)\n",
            "14871: Log(reward=57.0, actor_loss=None, critic_loss=None, divergence=2.693535566329956, entropy=-3.1862316131591797)\n",
            "14920: Log(reward=49.0, actor_loss=None, critic_loss=None, divergence=2.3436131477355957, entropy=-3.1862316131591797)\n",
            "14991: Log(reward=71.0, actor_loss=None, critic_loss=None, divergence=2.3255016803741455, entropy=-3.1862316131591797)\n",
            "15068: Log(reward=77.0, actor_loss=None, critic_loss=None, divergence=2.1965489387512207, entropy=-3.1862316131591797)\n",
            "15135: Log(reward=67.0, actor_loss=None, critic_loss=None, divergence=2.5528640747070312, entropy=-3.1862316131591797)\n",
            "15188: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.68068265914917, entropy=-3.1862316131591797)\n",
            "15255: Log(reward=67.0, actor_loss=None, critic_loss=None, divergence=2.2494142055511475, entropy=-3.1862316131591797)\n",
            "15308: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.500800848007202, entropy=-3.1862316131591797)\n",
            "15369: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.4326019287109375, entropy=-3.1862316131591797)\n",
            "15427: Log(reward=58.0, actor_loss=None, critic_loss=None, divergence=2.389794111251831, entropy=-3.1862316131591797)\n",
            "15464: Log(reward=37.0, actor_loss=None, critic_loss=None, divergence=1.9573503732681274, entropy=-3.1862316131591797)\n",
            "15517: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.7514545917510986, entropy=-3.1862316131591797)\n",
            "15573: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.4352736473083496, entropy=-3.1862316131591797)\n",
            "15629: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.285032033920288, entropy=-3.1862316131591797)\n",
            "15721: Log(reward=92.0, actor_loss=None, critic_loss=None, divergence=2.6482694149017334, entropy=-3.1862316131591797)\n",
            "15782: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.4910480976104736, entropy=-3.1862316131591797)\n",
            "15826: Log(reward=44.0, actor_loss=None, critic_loss=None, divergence=2.468959093093872, entropy=-3.1862316131591797)\n",
            "15904: Log(reward=78.0, actor_loss=None, critic_loss=None, divergence=2.6648716926574707, entropy=-3.1862316131591797)\n",
            "15957: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.6352789402008057, entropy=-3.1862316131591797)\n",
            "16029: Log(reward=72.0, actor_loss=None, critic_loss=None, divergence=2.2941231727600098, entropy=-3.1862316131591797)\n",
            "16125: Log(reward=96.0, actor_loss=None, critic_loss=None, divergence=2.4109785556793213, entropy=-3.1862316131591797)\n",
            "16194: Log(reward=69.0, actor_loss=None, critic_loss=None, divergence=2.6508688926696777, entropy=-3.1862316131591797)\n",
            "16247: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.4983084201812744, entropy=-3.1862316131591797)\n",
            "16308: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.3500137329101562, entropy=-3.1862316131591797)\n",
            "16397: Log(reward=89.0, actor_loss=None, critic_loss=None, divergence=2.711278200149536, entropy=-3.1862316131591797)\n",
            "16461: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.8782811164855957, entropy=-3.1862316131591797)\n",
            "16514: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.3376598358154297, entropy=-3.1862316131591797)\n",
            "16601: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.327838659286499, entropy=-3.1862316131591797)\n",
            "16667: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.42146897315979, entropy=-3.1862316131591797)\n",
            "16743: Log(reward=76.0, actor_loss=None, critic_loss=None, divergence=2.9286768436431885, entropy=-3.1862316131591797)\n",
            "16805: Log(reward=62.0, actor_loss=None, critic_loss=None, divergence=2.571049928665161, entropy=-3.1862316131591797)\n",
            "16884: Log(reward=79.0, actor_loss=None, critic_loss=None, divergence=2.5110461711883545, entropy=-3.1862316131591797)\n",
            "16968: Log(reward=84.0, actor_loss=None, critic_loss=None, divergence=2.6054024696350098, entropy=-3.1862316131591797)\n",
            "17053: Log(reward=85.0, actor_loss=None, critic_loss=None, divergence=2.7454798221588135, entropy=-3.1862316131591797)\n",
            "17123: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.5851316452026367, entropy=-3.1862316131591797)\n",
            "17179: Log(reward=56.0, actor_loss=None, critic_loss=None, divergence=2.385627269744873, entropy=-3.1862316131591797)\n",
            "17249: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.634202241897583, entropy=-3.1862316131591797)\n",
            "17337: Log(reward=88.0, actor_loss=None, critic_loss=None, divergence=2.393695116043091, entropy=-3.1862316131591797)\n",
            "17424: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.5196423530578613, entropy=-3.1862316131591797)\n",
            "17505: Log(reward=81.0, actor_loss=None, critic_loss=None, divergence=2.244432210922241, entropy=-3.1862316131591797)\n",
            "17594: Log(reward=89.0, actor_loss=None, critic_loss=None, divergence=1.9607775211334229, entropy=-3.1862316131591797)\n",
            "17660: Log(reward=66.0, actor_loss=None, critic_loss=None, divergence=2.5352392196655273, entropy=-3.1862316131591797)\n",
            "17742: Log(reward=82.0, actor_loss=None, critic_loss=None, divergence=2.441044569015503, entropy=-3.1862316131591797)\n",
            "17795: Log(reward=53.0, actor_loss=None, critic_loss=None, divergence=2.4713733196258545, entropy=-3.1862316131591797)\n",
            "17860: Log(reward=65.0, actor_loss=None, critic_loss=None, divergence=2.385066509246826, entropy=-3.1862316131591797)\n",
            "17919: Log(reward=59.0, actor_loss=None, critic_loss=None, divergence=2.8020083904266357, entropy=-3.1862316131591797)\n",
            "18018: Log(reward=99.0, actor_loss=None, critic_loss=None, divergence=2.2472071647644043, entropy=-3.1862316131591797)\n",
            "18081: Log(reward=63.0, actor_loss=None, critic_loss=None, divergence=2.378225326538086, entropy=-3.1862316131591797)\n",
            "18189: Log(reward=108.0, actor_loss=None, critic_loss=None, divergence=2.4066760540008545, entropy=-3.1862316131591797)\n",
            "18250: Log(reward=61.0, actor_loss=None, critic_loss=None, divergence=2.5050435066223145, entropy=-3.1862316131591797)\n",
            "18343: Log(reward=93.0, actor_loss=None, critic_loss=None, divergence=2.4370243549346924, entropy=-3.1862316131591797)\n",
            "18416: Log(reward=73.0, actor_loss=None, critic_loss=None, divergence=2.743726968765259, entropy=-3.1862316131591797)\n",
            "18480: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.5116078853607178, entropy=-3.1862316131591797)\n",
            "18544: Log(reward=64.0, actor_loss=None, critic_loss=None, divergence=2.2864906787872314, entropy=-3.1862316131591797)\n",
            "18647: Log(reward=103.0, actor_loss=None, critic_loss=None, divergence=2.5993869304656982, entropy=-3.1862316131591797)\n",
            "18717: Log(reward=70.0, actor_loss=None, critic_loss=None, divergence=2.367234468460083, entropy=-3.1862316131591797)\n",
            "18785: Log(reward=68.0, actor_loss=None, critic_loss=None, divergence=2.3570477962493896, entropy=-3.1862316131591797)\n",
            "18870: Log(reward=85.0, actor_loss=None, critic_loss=None, divergence=2.324793815612793, entropy=-3.1862316131591797)\n",
            "18950: Log(reward=80.0, actor_loss=None, critic_loss=None, divergence=2.4965174198150635, entropy=-3.1862316131591797)\n",
            "19037: Log(reward=87.0, actor_loss=None, critic_loss=None, divergence=2.454829692840576, entropy=-3.1862316131591797)\n",
            "19135: Log(reward=98.0, actor_loss=None, critic_loss=None, divergence=2.5640833377838135, entropy=-3.1862316131591797)\n",
            "19280: Log(reward=145.0, actor_loss=None, critic_loss=None, divergence=2.841600179672241, entropy=-3.1862316131591797)\n",
            "19401: Log(reward=121.0, actor_loss=None, critic_loss=None, divergence=2.3411812782287598, entropy=-3.1862316131591797)\n",
            "19501: Log(reward=100.0, actor_loss=None, critic_loss=None, divergence=2.623957872390747, entropy=-3.1862316131591797)\n",
            "19583: Log(reward=82.0, actor_loss=None, critic_loss=None, divergence=2.6071009635925293, entropy=-3.1862316131591797)\n",
            "19658: Log(reward=75.0, actor_loss=None, critic_loss=None, divergence=2.535198450088501, entropy=-3.1862316131591797)\n",
            "19754: Log(reward=96.0, actor_loss=None, critic_loss=None, divergence=2.7223358154296875, entropy=-3.1862316131591797)\n",
            "19868: Log(reward=114.0, actor_loss=None, critic_loss=None, divergence=2.5045437812805176, entropy=-3.1862316131591797)\n",
            "19950: Log(reward=82.0, actor_loss=None, critic_loss=None, divergence=2.5428361892700195, entropy=-3.1862316131591797)\n",
            "20035: Log(reward=85.0, actor_loss=None, critic_loss=None, divergence=2.4131948947906494, entropy=-3.1862316131591797)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Algorithms\n",
        "\n",
        "Hybrid algorithms can be implemented by simply having either the actor or critic be updated by an evolution updater while the other is updated by an actor or critic updater using backpropagation. The flow is the same as with the RL and EC agents. An example is CEM-RL, which is implemented [here](https://github.com/LondonNode/Pearl/blob/main/pearll/agents/cem_rl.py)."
      ],
      "metadata": {
        "id": "89zWmQ3vJToB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Post Training\n",
        "\n",
        "The `BaseAgent` class also includes many methods to make predictions after training:\n",
        "\n",
        "- `predict`: predict an action from the trained policy.\n",
        "- `action_distribution`: get the policy distribution given an observation.\n",
        "- `critic`: get the (Q) value of an observation (action pair).\n",
        "\n",
        "A plotting script is also included to plot the results of various runs. See the [technical report](https://arxiv.org/abs/2201.09568) for examples of this and more details."
      ],
      "metadata": {
        "id": "iuVcUDfoKSjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vector environment so just pick one of them by indexing\n",
        "obs = env.reset()[0]\n",
        "print(f\"Given observation: {obs}\")\n",
        "print(f\"Policy distribution: Categorical({agent.action_distribution(obs).probs})\")\n",
        "print(f\"Action sampled from policy distribution: {agent.predict(obs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3FPW0j5J32y",
        "outputId": "b3ac902e-adb7-488f-f905-aa0c6e83274c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given observation: [-0.01213112 -0.00454794 -0.02761681  0.01953347]\n",
            "Policy distribution: Categorical(tensor([0.5007, 0.4993], grad_fn=<SoftmaxBackward0>))\n",
            "Action sampled from policy distribution: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pearll.plot -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmnzUJqKLVEt",
        "outputId": "880b11ed-4f93-4d7d-a20c-9a6e4a95795a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: plot.py [-h] -p PATHS [PATHS ...] --metric METRIC --titles TITLES\n",
            "               [TITLES ...] [--num-cols NUM_COLS] [--interval INTERVAL]\n",
            "               [--legend LEGEND [LEGEND ...]] [--window WINDOW]\n",
            "               [--xlabel XLABEL] [--ylabel YLABEL] [--x-axis X_AXIS]\n",
            "               [--y-axis Y_AXIS] [--log-y]\n",
            "               [--save-types SAVE_TYPES [SAVE_TYPES ...]]\n",
            "               [--save-path SAVE_PATH]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  -p PATHS [PATHS ...], --paths PATHS [PATHS ...]\n",
            "  --metric METRIC\n",
            "  --titles TITLES [TITLES ...]\n",
            "  --num-cols NUM_COLS\n",
            "  --interval INTERVAL\n",
            "  --legend LEGEND [LEGEND ...]\n",
            "  --window WINDOW\n",
            "  --xlabel XLABEL\n",
            "  --ylabel YLABEL\n",
            "  --x-axis X_AXIS\n",
            "  --y-axis Y_AXIS\n",
            "  --log-y\n",
            "  --save-types SAVE_TYPES [SAVE_TYPES ...]\n",
            "  --save-path SAVE_PATH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "77L5umENMnLV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}