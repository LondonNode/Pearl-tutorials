{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 Buffers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMdcVyaWUPQi4BzP9/oJ7Qk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LondonNode/Pearl-tutorials/blob/main/2_Buffers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vVw8vR1DD4w"
      },
      "outputs": [],
      "source": [
        "!pip install pearll"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a tutorial for the `buffers` module within Pearl. This module has implementations of various experience buffers that allow for the storing and sampling of trajectories. There are three different types of buffer implemented: `ReplayBuffer`, `RolloutBuffer` and `HERBuffer`. A `BaseBuffer` class is also implemented for creating other buffers compatible with the rest of the library."
      ],
      "metadata": {
        "id": "uGIV3DUVDbf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Buffer\n",
        "\n",
        "The `BaseBuffer` is the base class from which all other buffers are derived. It includes 5 abstract methods to be defined by the user if a new buffer is made:\n",
        "\n",
        "- `reset`: reset the buffer to its inital empty state\n",
        "- `add_trajectory`: add a single trajectory to the buffer\n",
        "- `sample`: sample a batch of trajectories\n",
        "- `last`: get the most recent batch of trajectories stored\n",
        "- `all`: get all stored trajectories"
      ],
      "metadata": {
        "id": "bA57I2RxFSxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.buffers import BaseBuffer\n",
        "from pearll.common.type_aliases import Observation, Trajectories\n",
        "from pearll.common.enumerations import TrajectoryType\n",
        "\n",
        "import gym\n",
        "from typing import Union\n",
        "import torch as T\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class YourBuffer(BaseBuffer):\n",
        "  # The BaseBuffer __init__ method automatically creates arrays for the\n",
        "  # observations, actions, rewards and dones collected in MDP trajectories\n",
        "  # and checks if these arrays will fit in memory.\n",
        "  def __init__(\n",
        "    self,\n",
        "    env: gym.Env,\n",
        "    buffer_size: int,\n",
        "    device: Union[str, T.device] = \"auto\",\n",
        "  ) -> None:\n",
        "    super().__init__(\n",
        "        env,\n",
        "        buffer_size,\n",
        "        device,\n",
        "    )\n",
        "\n",
        "  # An implementation of reset is done in BaseBuffer that assumes only the\n",
        "  # BaseBuffer trajectory arrays are used. It's kept abstract to encourage the \n",
        "  # user to think about its implementation for themselves though.\n",
        "  def reset():\n",
        "    super().reset()\n",
        "\n",
        "  def add_trajectory(\n",
        "    self,\n",
        "    observation: Observation,\n",
        "    action: Union[np.ndarray, int],\n",
        "    reward: Union[float, np.ndarray],\n",
        "    next_observation: Observation,\n",
        "    done: Union[bool, np.ndarray],\n",
        "  ) -> None:\n",
        "    pass\n",
        "\n",
        "  def sample(\n",
        "    self,\n",
        "    batch_size: int,\n",
        "    flatten_env: bool = False, # whether to include an env axis in sampled trajectories\n",
        "    dtype: Union[str, TrajectoryType] = \"numpy\", # return type of trajectories, numpy or torch\n",
        "  ) -> Trajectories:\n",
        "    pass\n",
        "\n",
        "  def last(\n",
        "    self,\n",
        "    batch_size: int,\n",
        "    flatten_env: bool = False, # whether to include an env axis in sampled trajectories\n",
        "    dtype: Union[str, TrajectoryType] = \"numpy\", # return type of trajectories, numpy or torch\n",
        "  ) -> Trajectories:\n",
        "    pass\n",
        "\n",
        "  def all(\n",
        "    self,\n",
        "    flatten_env: bool = False, # whether to include an env axis in sampled trajectories\n",
        "    dtype: Union[str, TrajectoryType] = \"numpy\" # return type of trajectories, numpy or torch\n",
        "  ) -> Trajectories:\n",
        "    pass\n",
        "\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "buffer = YourBuffer(env=env, buffer_size=10)\n",
        "\n",
        "print(f\"The environment has action space {env.action_space} and Box observation space shape {env.observation_space.shape}\\n\")\n",
        "print(f\"Buffer observation array shape = {buffer.observations.shape}\")\n",
        "print(f\"Buffer action array shape = {buffer.actions.shape}\")\n",
        "print(f\"Buffer reward array shape = {buffer.rewards.shape}\")\n",
        "print(f\"Buffer done array shape = {buffer.dones.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jPfyINFFpts",
        "outputId": "d6c5f331-2f18-4583-b0a0-c483f4066208"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The environment has action space Discrete(2) and Box observation space shape (4,)\n",
            "\n",
            "Buffer observation array shape = (10, 4)\n",
            "Buffer action array shape = (10, 1)\n",
            "Buffer reward array shape = (10, 1)\n",
            "Buffer done array shape = (10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replay Buffer\n",
        "\n",
        "The `ReplayBuffer` class handles sample collection and processing for generally off-policy algorithms. A key feature is the use of a single array to handle observations and next observations in trajectories rather than using two different arrays. This has the advantage of saving memory space but assumes observations are stored sequentially and that observations can only be 'blended' at the end of the trajectory and start of the next."
      ],
      "metadata": {
        "id": "y76b4grREOTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.buffers import ReplayBuffer\n",
        "from pearll.common.type_aliases import Trajectories\n",
        "import gym\n",
        "\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "buffer = ReplayBuffer(env=env, buffer_size=5)\n",
        "\n",
        "print(\"Environment trajectories in order:\")\n",
        "obs = env.reset()\n",
        "for i in range(5):\n",
        "  action = env.action_space.sample()\n",
        "  next_obs, reward, done, _ = env.step(action)\n",
        "  buffer.add_trajectory(obs, action, reward, next_obs, done)\n",
        "  print(f\"{i+1}. {Trajectories(obs, action, reward, next_obs, done)}\")\n",
        "  obs = next_obs\n",
        "\n",
        "trajectories = buffer.sample(batch_size=5)\n",
        "print(f\"\\nSampled trajectories are randomly indexed:\")\n",
        "for i in range(5):\n",
        "  print(f\"{i+1}. {Trajectories(trajectories.observations[i], trajectories.actions[i], trajectories.rewards[i], trajectories.next_observations[i], trajectories.dones[i])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_swBPujDHQa",
        "outputId": "9dc785f4-e535-4e22-d697-a576f3e37c7f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment trajectories in order:\n",
            "1. Trajectories(observations=array([-0.00788589,  0.03270863,  0.03365615, -0.02002516]), actions=0, rewards=1.0, next_observations=array([-0.00723172, -0.16287942,  0.03325565,  0.2830838 ]), dones=False)\n",
            "2. Trajectories(observations=array([-0.00723172, -0.16287942,  0.03325565,  0.2830838 ]), actions=1, rewards=1.0, next_observations=array([-0.01048931,  0.03175281,  0.03891732,  0.00107225]), dones=False)\n",
            "3. Trajectories(observations=array([-0.01048931,  0.03175281,  0.03891732,  0.00107225]), actions=1, rewards=1.0, next_observations=array([-0.00985425,  0.22629564,  0.03893877, -0.27908224]), dones=False)\n",
            "4. Trajectories(observations=array([-0.00985425,  0.22629564,  0.03893877, -0.27908224]), actions=1, rewards=1.0, next_observations=array([-0.00532834,  0.4208411 ,  0.03335712, -0.55923413]), dones=False)\n",
            "5. Trajectories(observations=array([-0.00532834,  0.4208411 ,  0.03335712, -0.55923413]), actions=1, rewards=1.0, next_observations=array([ 0.00308848,  0.61547936,  0.02217244, -0.84122392]), dones=False)\n",
            "\n",
            "Sampled trajectories are randomly indexed:\n",
            "1. Trajectories(observations=array([-0.00532834,  0.4208411 ,  0.03335712, -0.55923414], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([ 0.00308848,  0.61547935,  0.02217244, -0.8412239 ], dtype=float32), dones=array([0.], dtype=float32))\n",
            "2. Trajectories(observations=array([-0.01048931,  0.03175281,  0.03891732,  0.00107225], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([-0.00985425,  0.22629564,  0.03893877, -0.27908224], dtype=float32), dones=array([0.], dtype=float32))\n",
            "3. Trajectories(observations=array([-0.00985425,  0.22629564,  0.03893877, -0.27908224], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([-0.00532834,  0.4208411 ,  0.03335712, -0.55923414], dtype=float32), dones=array([0.], dtype=float32))\n",
            "4. Trajectories(observations=array([-0.00532834,  0.4208411 ,  0.03335712, -0.55923414], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([ 0.00308848,  0.61547935,  0.02217244, -0.8412239 ], dtype=float32), dones=array([0.], dtype=float32))\n",
            "5. Trajectories(observations=array([-0.00985425,  0.22629564,  0.03893877, -0.27908224], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([-0.00532834,  0.4208411 ,  0.03335712, -0.55923414], dtype=float32), dones=array([0.], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rollout Buffer\n",
        "\n",
        "The `RolloutBuffer` class handles sample collection and processing for generally on-policy algorithms. This uses separate memory for observations and next observations unlike the `ReplayBuffer`."
      ],
      "metadata": {
        "id": "O5H_yZl9e_J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.buffers import RolloutBuffer\n",
        "from pearll.common.type_aliases import Trajectories\n",
        "import gym\n",
        "\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "buffer = RolloutBuffer(env=env, buffer_size=5)\n",
        "\n",
        "print(\"Environment trajectories in order:\")\n",
        "obs = env.reset()\n",
        "for i in range(5):\n",
        "  action = env.action_space.sample()\n",
        "  next_obs, reward, done, _ = env.step(action)\n",
        "  buffer.add_trajectory(obs, action, reward, next_obs, done)\n",
        "  print(f\"{i+1}. {Trajectories(obs, action, reward, next_obs, done)}\")\n",
        "  obs = next_obs\n",
        "\n",
        "# In this case, the sample method is similar to the last method.\n",
        "trajectories = buffer.sample(batch_size=5)\n",
        "print(f\"\\nSampled trajectories are also in order:\")\n",
        "for i in range(5):\n",
        "  print(f\"{i+1}. {Trajectories(trajectories.observations[i], trajectories.actions[i], trajectories.rewards[i], trajectories.next_observations[i], trajectories.dones[i])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KnwCWLNFN-H",
        "outputId": "fa9c1301-8957-4f47-dfef-5ee704b87f2f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment trajectories in order:\n",
            "1. Trajectories(observations=array([ 0.01372224,  0.01503374,  0.00939766, -0.01590847]), actions=1, rewards=1.0, next_observations=array([ 0.01402292,  0.21001967,  0.00907949, -0.30561157]), dones=False)\n",
            "2. Trajectories(observations=array([ 0.01402292,  0.21001967,  0.00907949, -0.30561157]), actions=0, rewards=1.0, next_observations=array([ 0.01822331,  0.01476951,  0.00296726, -0.01007908]), dones=False)\n",
            "3. Trajectories(observations=array([ 0.01822331,  0.01476951,  0.00296726, -0.01007908]), actions=0, rewards=1.0, next_observations=array([ 0.0185187 , -0.18039487,  0.00276568,  0.28353858]), dones=False)\n",
            "4. Trajectories(observations=array([ 0.0185187 , -0.18039487,  0.00276568,  0.28353858]), actions=0, rewards=1.0, next_observations=array([ 0.01491081, -0.37555615,  0.00843645,  0.5770925 ]), dones=False)\n",
            "5. Trajectories(observations=array([ 0.01491081, -0.37555615,  0.00843645,  0.5770925 ]), actions=1, rewards=1.0, next_observations=array([ 0.00739968, -0.18055346,  0.0199783 ,  0.28707916]), dones=False)\n",
            "\n",
            "Sampled trajectories are also in order:\n",
            "1. Trajectories(observations=array([ 0.01372224,  0.01503374,  0.00939766, -0.01590847], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([ 0.01402292,  0.21001966,  0.00907949, -0.30561158], dtype=float32), dones=array([0.], dtype=float32))\n",
            "2. Trajectories(observations=array([ 0.01402292,  0.21001966,  0.00907949, -0.30561158], dtype=float32), actions=array([0]), rewards=array([1.], dtype=float32), next_observations=array([ 0.01822331,  0.01476951,  0.00296726, -0.01007908], dtype=float32), dones=array([0.], dtype=float32))\n",
            "3. Trajectories(observations=array([ 0.01822331,  0.01476951,  0.00296726, -0.01007908], dtype=float32), actions=array([0]), rewards=array([1.], dtype=float32), next_observations=array([ 0.0185187 , -0.18039487,  0.00276568,  0.28353858], dtype=float32), dones=array([0.], dtype=float32))\n",
            "4. Trajectories(observations=array([ 0.0185187 , -0.18039487,  0.00276568,  0.28353858], dtype=float32), actions=array([0]), rewards=array([1.], dtype=float32), next_observations=array([ 0.01491081, -0.37555614,  0.00843645,  0.57709247], dtype=float32), dones=array([0.], dtype=float32))\n",
            "5. Trajectories(observations=array([ 0.01491081, -0.37555614,  0.00843645,  0.57709247], dtype=float32), actions=array([1]), rewards=array([1.], dtype=float32), next_observations=array([ 0.00739968, -0.18055347,  0.0199783 ,  0.28707916], dtype=float32), dones=array([0.], dtype=float32))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HER Buffer\n",
        "\n",
        "The `HERBuffer` is an implementation of the Hindsight Experience Replay buffer. An explanation of the code is pinned by Towards Data Science under \"Tips and Tricks\": https://towardsdatascience.com/hindsight-experience-replay-her-implementation-92eebab6f653\n",
        "\n",
        "Unfortunately, this only supports single agent environments right now (not compatible with `gym.vector.VectorEnv`)"
      ],
      "metadata": {
        "id": "c_gAbXiuguYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dxmpMm2xgRhd"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}