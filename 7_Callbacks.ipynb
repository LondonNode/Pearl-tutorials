{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7. Callbacks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfkc7t9OgEX6giaHV3lTgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LondonNode/Pearl-tutorials/blob/main/7_Callbacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2ZrH6l0NnEj"
      },
      "outputs": [],
      "source": [
        "!pip install pearll"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a tutorial for the `callbacks` module within Pearl. This allows the user to inject unique logic into the training flow to be called every time the agent steps in the environment via the `step_env()` method in the `BaseAgent`."
      ],
      "metadata": {
        "id": "0fgCZGlgNrH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Callback\n",
        "\n",
        "The `BaseCallback` is the base class for other callbacks. This contains the abtract method `_on_step()` to be implemented. This should return a boolean, which if `False` will abort the training early."
      ],
      "metadata": {
        "id": "DVr2gj-xOFZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.callbacks import BaseCallback\n",
        "from pearll.common.logging_ import Logger\n",
        "from pearll.models.actor_critics import ActorCritic, Dummy\n",
        "from pearll.settings import LoggerSettings\n",
        "\n",
        "from gym.spaces import Box\n",
        "\n",
        "\n",
        "logger_settings = LoggerSettings()\n",
        "logger = Logger(\n",
        "  tensorboard_log_path=logger_settings.tensorboard_log_path,\n",
        "  file_handler_level=logger_settings.file_handler_level,\n",
        "  stream_handler_level=logger_settings.stream_handler_level,\n",
        "  verbose=logger_settings.verbose,\n",
        "  num_envs=1,\n",
        ")\n",
        "\n",
        "space = Box(-100, 100, shape=(1,))\n",
        "actor = Dummy(space=space)\n",
        "critic = Dummy(space=space)\n",
        "model = ActorCritic(actor, critic)\n",
        "\n",
        "class YourCallback(BaseCallback):\n",
        "  def __init__(self, logger: Logger, model: ActorCritic):\n",
        "    super().__init__(logger, model)\n",
        "\n",
        "  def _on_step(self):\n",
        "    return True\n",
        "\n",
        "callback = YourCallback(logger, model)\n",
        "print(f\"n_calls records the number of times the main on_step method of the callback is called\\nat initialization n_calls = {callback.n_calls}\\n\")\n",
        "print(f\"step records how many steps the agent has taken in the environment\\nat initialization step = {callback.step}\\n\")\n",
        "\n",
        "keep_training = callback.on_step(step=5)\n",
        "print(f\"After on_step is called\\nn_calls = {callback.n_calls}\\nstep = {callback.step}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ4Wd219NqrN",
        "outputId": "d5444335-b8b4-4e00-83f5-356c8ad7d7d5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_calls records the number of times the main on_step method of the callback is called\n",
            "at initialization n_calls = 0\n",
            "\n",
            "step records how many steps the agent has taken in the environment\n",
            "at initialization step = 0\n",
            "\n",
            "After on_step is called\n",
            "n_calls = 1\n",
            "step = 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint Callback\n",
        "\n",
        "For now, only the `CheckpointCallback` is implemented. This saves the model as the agent trains. Let's demonstrate with a simple DQN on CartPole."
      ],
      "metadata": {
        "id": "SzxlQhh1RJTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.agents import DQN\n",
        "from pearll.models import ActorCritic, EpsilonGreedyActor, Critic\n",
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import CategoricalHead\n",
        "from pearll.settings import ExplorerSettings, Settings\n",
        "from pearll.callbacks import CheckpointCallback\n",
        "\n",
        "import gym\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class CheckpointSettings(Settings):\n",
        "  save_freq: int = 10\n",
        "  save_path: str = os.path.join(os.getcwd(), \"checkpoints\")\n",
        "  name_prefix: str = \"agent\"\n",
        "\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "agent = DQN(\n",
        "  env=env,\n",
        "  model=None,\n",
        "  # NOTE: multiple callbacks can be used, just add them to the list!\n",
        "  callbacks=[CheckpointCallback],\n",
        "  # NOTE: each callback needs its own settings object.\n",
        "  callback_settings=[CheckpointSettings()],\n",
        "  explorer_settings=ExplorerSettings(start_steps=0),\n",
        ")\n",
        "agent.fit(\n",
        "  num_steps=100, batch_size=32, critic_epochs=16, train_frequency=(\"episode\", 1)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_lf6vCLPvwf",
        "outputId": "790ebb85-f81e-4e53-982e-b62924e827bd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device cpu\n",
            "Saving weights to /content/checkpoints/agent_9_steps.pt\n",
            "16: Log(reward=17.0, actor_loss=None, critic_loss=None, divergence=None, entropy=None)\n",
            "Saving weights to /content/checkpoints/agent_19_steps.pt\n",
            "29: Log(reward=13.0, actor_loss=None, critic_loss=None, divergence=None, entropy=None)\n",
            "Saving weights to /content/checkpoints/agent_29_steps.pt\n",
            "Saving weights to /content/checkpoints/agent_39_steps.pt\n",
            "44: Log(reward=15.0, actor_loss=None, critic_loss=0.8602405413985252, divergence=None, entropy=None)\n",
            "Saving weights to /content/checkpoints/agent_49_steps.pt\n",
            "Saving weights to /content/checkpoints/agent_59_steps.pt\n",
            "66: Log(reward=22.0, actor_loss=None, critic_loss=0.8353521041572094, divergence=None, entropy=None)\n",
            "Saving weights to /content/checkpoints/agent_69_steps.pt\n",
            "Saving weights to /content/checkpoints/agent_79_steps.pt\n",
            "Saving weights to /content/checkpoints/agent_89_steps.pt\n",
            "93: Log(reward=27.0, actor_loss=None, critic_loss=0.7781975753605366, divergence=None, entropy=None)\n",
            "Saving weights to /content/checkpoints/agent_99_steps.pt\n",
            "Saving weights to /content/checkpoints/agent_109_steps.pt\n",
            "115: Log(reward=22.0, actor_loss=None, critic_loss=0.7575780637562275, divergence=None, entropy=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Fd9Qr8R-SDol"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}