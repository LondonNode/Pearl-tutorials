{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMJMQdVZZhs1AGT4WioqEnN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LondonNode/Pearl-tutorials/blob/main/1_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kchoXUjC5aFI"
      },
      "outputs": [],
      "source": [
        "!pip install pearll"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook is a tutorial for the `models` module within Pearl. This module represents the neural network structures for approximating a policy or value function to be optimized. All deep structures are built using the *PyTorch* framework.\n",
        "\n",
        "The key features implemented are as follows:\n",
        "\n",
        "| Features                 | Pearl   | \n",
        "|-------------------       |---------|\n",
        "| Modular Components       | ✅      |\n",
        "| Target networks          | ✅      |\n",
        "| Vector representation    | ✅      |\n",
        "| Network Population       | ✅      |\n",
        "| Dummy Models             | ✅      |\n",
        "| Shared architectures     | ✅      |\n",
        "| Separate architectures   | ✅      |"
      ],
      "metadata": {
        "id": "FUMyimaf5jJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# An example of a model setup\n",
        "\n",
        "import torch as T\n",
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import ValueHead, DeterministicHead\n",
        "from pearll.models import Actor, Critic, ActorCritic\n",
        "from pearll.settings import PopulationSettings\n",
        "\n",
        "# Shared encoder\n",
        "encoder = IdentityEncoder()\n",
        "# Separate torsos\n",
        "actor_torso = MLP(layer_sizes=[5, 10, 10], activation_fn=T.nn.ReLU)\n",
        "critic_torso = MLP(layer_sizes=[5, 5, 5], activation_fn=T.nn.ReLU)\n",
        "# Separate heads\n",
        "actor_head = DeterministicHead(input_shape=10, action_shape=2)\n",
        "critic_head = ValueHead(input_shape=5)\n",
        "\n",
        "# No target network for actor\n",
        "actor = Actor(encoder=encoder, torso=actor_torso, head=actor_head)\n",
        "# Add target network for critic\n",
        "critic = Critic(encoder=encoder, torso=critic_torso, head=critic_head, create_target=True)\n",
        "\n",
        "# Population settings to create 10 actors and 10 critics with normally\n",
        "# distributed parameters.\n",
        "population_settings = PopulationSettings(\n",
        "    actor_population_size=10,\n",
        "    critic_population_size=10,\n",
        "    actor_distribution=\"normal\",\n",
        "    critic_distribution=\"normal\",\n",
        ")\n",
        "model = ActorCritic(actor=actor, critic=critic, population_settings=population_settings)\n",
        "\n",
        "input = T.ones((10, 5))\n",
        "\n",
        "# forward = run on population networks\n",
        "# predict = run on single global network\n",
        "population_actors_output = model(input)\n",
        "population_critics_output = model.forward_critics(input)\n",
        "target_critics_output = model.forward_target_critics(input)\n",
        "global_actor_output = model.predict(input)\n",
        "global_critic_output = model.predict_critic(input)"
      ],
      "metadata": {
        "id": "IQOSohplY1vI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Structure\n",
        "\n",
        "The models within Pearl are broken down into three components for easy configuration and modularity: the **encoder**, the **torso** and the **head**."
      ],
      "metadata": {
        "id": "1YGWg_v57ruW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoders\n",
        "\n",
        "The encoder processes the input; for example, by concatenating the state observation and action for the continuous Q function network. "
      ],
      "metadata": {
        "id": "HqfzJefn95hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models import encoders as enc\n",
        "import torch as T\n",
        "\n",
        "# It is assumed that the models can have two input options:\n",
        "# 1. An observation\n",
        "# 2. An observation AND action"
      ],
      "metadata": {
        "id": "-LkuCV8NBeeX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The IdentityEncoder passes the input through unchanged\n",
        "\n",
        "observation = T.ones(2)\n",
        "action = T.ones(2)\n",
        "\n",
        "encoder = enc.IdentityEncoder()\n",
        "print(f\"IdentityEncoder output: {encoder(observation)}\")\n",
        "print(f\"IdentityEncoder output with action: {encoder(observation, action)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDrkdNSj-0XO",
        "outputId": "58b0731e-6904-4fba-c3c2-c316e8244d9e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IdentityEncoder output: tensor([1., 1.])\n",
            "IdentityEncoder output with action: tensor([1., 1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The FlattenEncoder flattens the input\n",
        "\n",
        "observation = T.ones(2, 2)\n",
        "\n",
        "encoder = enc.FlattenEncoder()\n",
        "print(f\"FlattenEncoder output: {encoder(observation)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sebhaQMBguT",
        "outputId": "f6ed089e-29ba-4932-c49b-66b201e6f465"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlattenEncoder output: tensor([[1., 1.],\n",
            "        [1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The MLPEncoder acts as a single layer MLP\n",
        "\n",
        "observation = T.ones(2)\n",
        "\n",
        "encoder = enc.MLPEncoder(input_size=2, output_size=1)\n",
        "print(f\"MLPEncoder output: \\n {encoder(observation)}\\n\")\n",
        "print(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LKsLFwpEYex",
        "outputId": "e48baea0-5f5c-4d0b-f0b4-0c56f3556591"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPEncoder output: \n",
            " tensor([-0.0740], grad_fn=<AddBackward0>)\n",
            "\n",
            "MLPEncoder(\n",
            "  (model): Linear(in_features=2, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The CNNEncoder is the CNN from the DQN Nature paper: \n",
        "# Mnih, Volodymyr, et al.\n",
        "# \"Human-level control through deep reinforcement learning.\"\n",
        "# Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "from gym.spaces import Box\n",
        "\n",
        "observation = T.normal(0, 1, (1, 1, 64, 64))\n",
        "\n",
        "encoder = enc.CNNEncoder(observation_space=Box(low=0, high=255, shape=(1, 64, 64)))\n",
        "print(f\"CNNEncoder output shape: \\n {encoder(observation).shape}\\n\")\n",
        "print(encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO9xf38vE4yv",
        "outputId": "28be7e22-3ec3-4fa3-e066-41a6e1173581"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNNEncoder output shape: \n",
            " torch.Size([1, 512])\n",
            "\n",
            "CNNEncoder(\n",
            "  (cnn): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (linear): Sequential(\n",
            "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The DictEncoder allows for dictionary observations e.g. from GoalEnv\n",
        "# This allows us to specify the labels of the input and to pass it to another\n",
        "# encoder module for processing, but defaults to the IdentityEncoder.\n",
        "\n",
        "observation = {\"label\": T.ones(2), \"other_label\": T.zeros(100)}\n",
        "\n",
        "encoder1 = enc.DictEncoder(labels=[\"label\"], encoder=enc.IdentityEncoder())\n",
        "print(f\"DictEncoder output: \\n {encoder1(observation)}\\n\")\n",
        "\n",
        "post_processing = enc.MLPEncoder(input_size=2, output_size=1)\n",
        "encoder2 = enc.DictEncoder(labels=[\"label\"], encoder=post_processing)\n",
        "print(f\"DictEncoder output with post-processing MLPEncoder: \\n {encoder2(observation)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rQQukCMGC2W",
        "outputId": "dc996958-5bac-48a3-81ee-2c5b47ff327e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DictEncoder output: \n",
            " tensor([1., 1.])\n",
            "\n",
            "DictEncoder output with post-processing MLPEncoder: \n",
            " tensor([0.0874], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Torsos\n",
        "\n",
        "The torso embodies the deep layers; for now, only a *multilayer  perceptron* is supported."
      ],
      "metadata": {
        "id": "DMrGa5eA-pQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models.torsos import MLP\n",
        "import torch as T\n",
        "\n",
        "input = T.ones(2)\n",
        "\n",
        "torso = MLP(layer_sizes=[2, 3, 5, 1])\n",
        "print(f\"Torso network: \\n {torso}\")\n",
        "print(f\"Torso output: {torso(input)}\\n\")\n",
        "\n",
        "# An activation function can be defined between layers\n",
        "torso_with_activations = MLP(layer_sizes=[2, 3, 5, 1], activation_fn=T.nn.ReLU)\n",
        "print(f\"Torso network with ReLU activations: \\n {torso_with_activations}\")\n",
        "print(f\"Torso output with ReLU activations: {torso_with_activations(input)}\")"
      ],
      "metadata": {
        "id": "er23lF435f1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d6bdd0-20dc-4b44-ef7c-00607a0e1f61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torso network: \n",
            " MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "    (1): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (2): Linear(in_features=5, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Torso output: tensor([0.2159], grad_fn=<AddBackward0>)\n",
            "\n",
            "Torso network with ReLU activations: \n",
            " MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=2, out_features=3, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=3, out_features=5, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=5, out_features=1, bias=True)\n",
            "    (5): ReLU()\n",
            "  )\n",
            ")\n",
            "Torso output with ReLU activations: tensor([0.2642], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Heads\n",
        "\n",
        "The head dictates the output; for example, a categorical distribution for an actor. There are two types of head, one for an 'actor' and one for a 'critic'. The critic heads are used for prediction, where the goal is to get a good approximation for a value function or Q function. The actor heads are used for control, where the goal is to optimize a policy to maximize reward."
      ],
      "metadata": {
        "id": "dxypmx54-1LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models import heads\n",
        "import torch as T"
      ],
      "metadata": {
        "id": "ziTM0MptOI7g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Critics"
      ],
      "metadata": {
        "id": "FPkhTouSROmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The value function represents the expected sum of rewards from a state so\n",
        "# should have a single number output\n",
        "\n",
        "input = T.ones(5)\n",
        "\n",
        "head = heads.ValueHead(input_shape=5, activation_fn=T.nn.Tanh)\n",
        "print(f\"ValueHead output: {head(input)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew8_vK9N-5wO",
        "outputId": "973d6823-688e-45e6-ce46-2b7bb0f978cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ValueHead output: tensor([-0.1909], grad_fn=<TanhBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Q value represents the expected sum of rewards from a state given an\n",
        "# action, but in the discrete action case we can have a single observation \n",
        "# input and Q value outputs for each action.\n",
        "\n",
        "input = T.ones(5)\n",
        "\n",
        "# The ContinuousQHead is actually the same as the ValueHead\n",
        "head = heads.ContinuousQHead(input_shape=5, activation_fn=T.nn.Tanh)\n",
        "print(f\"ContinuousQHead output: {head(input)}\")\n",
        "\n",
        "head = heads.DiscreteQHead(input_shape=5, output_shape=2, activation_fn=T.nn.Tanh)\n",
        "print(f\"DiscreteQHead output: {head(input)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S5_XwkYPfKK",
        "outputId": "6a1de5fb-a5c4-42e1-9ffd-ac647602878e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ContinuousQHead output: tensor([0.1124], grad_fn=<TanhBackward0>)\n",
            "DiscreteQHead output: tensor([-0.0922, -0.1400], grad_fn=<TanhBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actors"
      ],
      "metadata": {
        "id": "FYm6oEVARSYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The DummyHead is just a dummy for when you don't want a nerual network\n",
        "# e.g. black box static function optimization using evolutionary strategies\n",
        "\n",
        "head = heads.DummyHead()\n",
        "# Output shows there is nothing in this head! It's here to allow compatibility\n",
        "# in upstream models :)\n",
        "print(head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwtSpsuGQo8W",
        "outputId": "6ce7c944-7def-4536-fad3-157caae8f7d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DummyHead()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The DeterministicHead is used if a deterministic policy is used. That is,\n",
        "# the network doesn't output a distribution to be sampled from, but rather\n",
        "# the action itself.\n",
        "\n",
        "input = T.ones(5)\n",
        "\n",
        "head = heads.DeterministicHead(input_shape=5, action_shape=2, activation_fn=T.nn.ReLU)\n",
        "print(f\"DeterministicHead output: {head(input)}\\n\")\n",
        "print(head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llW1rJ8IRr3e",
        "outputId": "62c25e98-b48b-443d-f7ea-4c2fd4a72034"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeterministicHead output: tensor([0.0000, 0.1568], grad_fn=<ReluBackward0>)\n",
            "\n",
            "DeterministicHead(\n",
            "  (model): MLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=5, out_features=2, bias=True)\n",
            "      (1): ReLU()\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The other heads output probability distributions that are sampled to get\n",
        "# the action. Let's take the case where the action is normally distributed.\n",
        "# A diagonal gaussian distribution is supported -> diagonal covariance matrix!\n",
        "# We can specify how the standard deviation of the distribution is learned. If\n",
        "# 'mlp' is used, the standard deviation output is determined by a normal linear\n",
        "# layer. By default it's set to 'parameter' so that the output isn't updated\n",
        "# every step.\n",
        "head1 = heads.DiagGaussianHead(input_shape=5, action_size=1)\n",
        "head2 = heads.DiagGaussianHead(input_shape=5, action_size=1, log_std_network_type=\"mlp\")\n",
        "print(f\"T.nn.Parameter std: \\n {head1}\\n\")\n",
        "print(f\"MLP std: \\n {head2}\\n\")\n",
        "\n",
        "input = T.ones(5)\n",
        "print(f\"Distribution output: {head1.action_distribution(input)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cP2yUmQQSWRP",
        "outputId": "9fbfede5-b2fd-4bc3-d7f5-589ac8daf6e3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T.nn.Parameter std: \n",
            " DiagGaussianHead(\n",
            "  (mean_network): MLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=5, out_features=1, bias=True)\n",
            "      (1): Tanh()\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "MLP std: \n",
            " DiagGaussianHead(\n",
            "  (mean_network): MLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=5, out_features=1, bias=True)\n",
            "      (1): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (log_std_network): MLP(\n",
            "    (model): Sequential(\n",
            "      (0): Linear(in_features=5, out_features=1, bias=True)\n",
            "      (1): Softplus(beta=1, threshold=20)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Distribution output: Normal(loc: tensor([-0.5080], grad_fn=<TanhBackward0>), scale: tensor([1.], grad_fn=<ExpBackward0>))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actor Critic\n",
        "\n",
        "The `ActorCritic` is the main network interface compatible with the other high level modules in Pearl. This in turn is made up of an `Actor` and a `Critic`, and consists of many useful methods to easily and quickly process inputs and manipulate the networks themselves."
      ],
      "metadata": {
        "id": "EaWI2fvGWMOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models.encoders import IdentityEncoder\n",
        "from pearll.models.torsos import MLP\n",
        "from pearll.models.heads import ValueHead, CategoricalHead\n",
        "\n",
        "encoder = IdentityEncoder()\n",
        "torso = MLP(layer_sizes=[5, 10, 5])\n",
        "critic_head = ValueHead(input_shape=5)\n",
        "actor_head = CategoricalHead(input_shape=5, action_size=2)"
      ],
      "metadata": {
        "id": "7krQSD-Urf-6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pearll.models import Critic\n",
        "import numpy as np\n",
        "\n",
        "# Setting the create_target flag to True creates a target network.\n",
        "critic = Critic(\n",
        "  encoder=encoder,\n",
        "  torso=torso,\n",
        "  head=critic_head,\n",
        "  create_target=True,\n",
        "  polyak_coeff=0.995,\n",
        ")\n",
        "\n",
        "input = T.ones(5)\n",
        "\n",
        "# run the online critic network\n",
        "output = critic(input)\n",
        "# run the target critic network\n",
        "target_output = critic.forward_target(input)\n",
        "# update the target network via polyak averaging: \n",
        "# target_params = polyak_coeff * target_params + (1 - polyak_coeff) * online_params\n",
        "critic.update_targets()\n",
        "# update the target network by directly copying from the online network:\n",
        "# target_params = online_params\n",
        "critic.assign_targets()\n",
        "\n",
        "\n",
        "print(f\"This network state can be represented as a vector with shape: {critic.numpy().shape}\")\n",
        "# You can also access the vector state parameter directly:\n",
        "state = critic.state\n",
        "# Update network state\n",
        "critic.set_state(np.ones(121))\n",
        "print(f\"After update, network state vector is np.ones(121): {all(np.equal(np.ones(121), critic.numpy()))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqFmB0HAWSUu",
        "outputId": "5826a953-1243-4bfa-cb3c-5d3800cc79e0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This network state can be represented as a vector with shape: (121,)\n",
            "After update, network state vector is np.ones(121): True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Actor is the same as the Critic with the addition of getting a\n",
        "# probability distribution output\n",
        "\n",
        "from pearll.models import Actor\n",
        "\n",
        "actor = Actor(\n",
        "    encoder=encoder,\n",
        "    torso=torso,\n",
        "    head=actor_head,\n",
        ")\n",
        "\n",
        "input = T.ones(5)\n",
        "\n",
        "# Get distribution output\n",
        "dist = actor.action_distribution(input)\n",
        "print(f\"Distribution: {dist}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQGdek8DsJts",
        "outputId": "1add725a-dfaa-421a-cdb7-eda13bbc85f9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribution: Categorical(logits: torch.Size([2]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The ActorCritic combines and Actor and Critic template to generate a\n",
        "# population of networks that can be manipulated\n",
        "\n",
        "from pearll.models import ActorCritic\n",
        "from pearll.settings import PopulationSettings\n",
        "import numpy as np\n",
        "\n",
        "# A settings object can be used to configure the network populations\n",
        "settings = PopulationSettings(\n",
        "    actor_population_size=2,\n",
        "    critic_population_size=2,\n",
        "    actor_distribution=\"uniform\",\n",
        "    critic_distribution=\"uniform\",\n",
        "    actor_std=None,\n",
        "    critic_std=None,\n",
        ")\n",
        "\n",
        "model = ActorCritic(actor=actor, critic=critic, population_settings=settings)\n",
        "print(f\"Now each actor/critic network population state can be represented as a matrix with shape (population_size, single_network_params): {model.numpy_actors().shape}\")\n",
        "# Can update the population networks in the same way\n",
        "model.set_critics_state(np.ones((2, 127)))\n",
        "# Update the single global network as the average of the population networks\n",
        "model.update_global()\n",
        "# Update any target networks via Polyak averaging\n",
        "model.update_targets()\n",
        "# Update any target networks by assignment\n",
        "model.assign_targets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeBUVJbAvO_j",
        "outputId": "cede71a8-9afb-4bf4-c2d8-ebcaf102500b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now each actor/critic network population state can be represented as a matrix with shape (population_size, single_network_params): (2, 127)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A dummy model is useful in cases where you don't need a neural network\n",
        "# structure for your algorithm. Now such use cases are compatible with\n",
        "# Pearl as well (e.g. optimizing spherical function with evolutionary\n",
        "# strategy)!\n",
        "\n",
        "from pearll.models import Dummy\n",
        "from gym.spaces import Box\n",
        "from pearll.models import ActorCritic\n",
        "from pearll.settings import PopulationSettings\n",
        "import numpy as np\n",
        "\n",
        "actor = Dummy(space=Box(-100, 100, shape=(1,)))\n",
        "critic = Dummy(space=Box(-100, 100, shape=(1,)))\n",
        "\n",
        "# Default PopulationSettings defines a single network for each actor/critic\n",
        "model = ActorCritic(actor=actor, critic=critic)\n",
        "\n",
        "# Input doesn't matter anymore, the output is just the state from the space \n",
        "# provided at initialization\n",
        "state = model(np.ones(1))\n",
        "print(f\"State of network from actor space using forward(): {state}\")\n",
        "# The state can be set the same way as a standard ActorCritic\n",
        "model.set_critics_state(np.array([100]))\n",
        "# The state can be taken from the numpy() method again\n",
        "state = model.numpy_actors()\n",
        "print(f\"State of network from actor space using numpy(): {state}\")\n",
        "print(f\"State of network from critic space using numpy(): {model.numpy_critics()}\")\n",
        "print(\"NOTE: numpy() returns numpy representation, forward() returns torch representation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI6tghIz0MZp",
        "outputId": "c7102493-3d6c-46bb-fb8b-81dfc626c6c2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State of network from actor space using forward(): tensor([-90.1194])\n",
            "State of network from actor space using numpy(): [[-90.11937]]\n",
            "State of network from critic space using numpy(): [[100]]\n",
            "NOTE: numpy() returns numpy representation, forward() returns torch representation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RgJNmlGL5LB5"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}